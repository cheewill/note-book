{"./":{"url":"./","title":"介绍","keywords":"","body":" 亿级流量电商详情页系统实战（第二版）：缓存架构+高可用服务架构+微服务架构 第一版 第二版 亿级流量电商详情页系统实战（第二版）：缓存架构+高可用服务架构+微服务架构 cache-pdp：Cache architecture product details page 本章是第一版与第二版全内容目录导航 第一版 第 001 ~ 123 章是第一版，主要对缓存架构、高可用中用到的技术做了深入讲解 第一版笔记配套项目 001. 课程介绍以及高并发高可用复杂系统中的缓存架构有哪些东西 002. 基于大型电商网站中的商品详情页系统贯穿的授课思路介绍 003. 小型电商网站的商品详情页的页面静态化架构以及其缺陷 004. 大型电商网站的异步多级缓存构建 + nginx 数据本地化动态渲染的架构 005. 能够支撑高并发+高可用+海量数据+备份恢复的 redis 的重要性 006. 从零开始在虚拟机中一步一步搭建一个 4 个节点的 CentOS 集群 007. 单机版 redis 的安装以及 redis 生产环境启动方案 008. redis 持久化机对于生产环境中的灾难恢复的意义 009. 图解分析 redis 的 RDB 和 AOF 两种持久化机制的工作原理 010. redis 的 RDB 和 AOF 两种持久化机制的优劣势对比 011. redis 的 RDB 持久化配置以及数据恢复实验 012. redis 的 AOF 持久化深入讲解各种操作和相关实验 013. 在项目中部署 redis 企业级数据备份方案以及各种踩坑的数据恢复容灾演练 014. redis 如何通过读写分离来承载读请求 QPS 超过 10 万 +？ 015. redis replication 以及 master 持久化对主从架构的安全意义 016. redis 主从复制原理、断点续传、无磁盘化复制、过期 key 处理 017. redis replication 的完整流运行程和原理的再次深入剖析 018. 在项目中部署 redis 的读写分离架构（包含节点间认证口令） 019. 对项目的主从 redis 架构进行 QPS 压测以及水平扩容支撑更高 QPS 020. redis 主从架构下如何才能做到 99.99% 的高可用性？ 021. redis 哨兵架构的相关基础知识的讲解 022. redis 哨兵主备切换的数据丢失问题：异步复制、集群脑裂 023. redis 哨兵的多个核心底层原理的深入解析（包含 slave 选举算法） 024. 在项目中以经典的 3 节点方式部署哨兵集群 025. 对项目中的哨兵节点进行管理以及高可用 redis 集群的容灾演练 026. redis 如何在保持读写分离+高可用的架构下，还能横向扩容支撑 1T + 海量数据 027. 数据分布算法：hash+ 一致性 hash + redis cluster 的 hash slot 028. 在项目中重新搭建一套读写分离+高可用+多 master 的 redis cluster 集群 029. 对项目的 redis cluster 实验多 master 写入、读写分离、高可用性 030. redis cluster 通过 master 水平扩容来支撑更高的读写吞吐 + 海量数据 031. redis cluster 的自动化 slave 迁移实现更强的高可用架构的部署方案 032. redis cluster 的核心原理分析：gossip 通信、jedis smart 定位、主备切换 033. redis 在实践中的一些常见问题以及优化思路（包含 linux 内核参数优化） 034. redis 阶段性总结：1T 以上海量数据+10 万以上 QPS 高并发+ 99.99% 高可用 035. 亿级流量商品详情页的多级缓存架构以及架构中每一层的意义 036. Cache Aside Pattern 缓存+数据库读写模式的分析 037. 高并发场景下的缓存 + 数据库双写不一致问题分析与解决方案设计 038. 在 linux 虚拟机中安装部署 MySQL 数据库 039. 库存服务的开发框架整合与搭建：spring boot + mybatis + jedis 040. 在库存服务中实现缓存与数据库双写一致性保障方案（一、二、三、四） 044. 库存服务代码调试以及打印日志观察服务的运行流程是否正确 045. 商品详情页结构分析、缓存全量更新问题以及缓存维度化解决方案 046. 缓存数据生产服务的工作流程分析以及工程环境搭建 047. 完成 spring boot 整合 ehcache 的搭建以支持服务本地堆缓存 048. redis 的 LRU 缓存清除算法讲解以及相关配置使用 049. zookeeper + kafka 集群的安装部署以及如何简单使用的介绍 050. 基于 kafka + ehcache + redis 完成缓存数据生产服务的开发与测试 051. 基于“分发层 + 应用层”双层 nginx 架构提升缓存命中率方案分析 052. 基于 OpenResty 部署应用层 nginx 以及 nginx + lua 开发 hello world 053. 部署分发层 nginx 以及基于 lua 完成基于商品 id 的定向流量分发策略 054. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（一） 055. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（二） 056. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（三） 057. 分布式缓存重建并发冲突问题以及 zookeeper 分布式锁解决方案 058. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（一） 059. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（二） 060. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（三） 061. Java 程序员、缓存架构以及 Storm 大数据实时计算之间的关系 062. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：大白话介绍 063. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：大白话讲集群架构与核心概念 064. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：大白话讲并行度和流分组 065. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：纯手敲 WordCount 程序 066. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：纯手工集群部署 067. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：基于集群运行计算拓扑 068. 缓存冷启动问题：新系统上线 redis 彻底崩溃导致数据无法恢复 069. 缓存预热解决方案：基于 storm 实时热点统计的分布式并行缓存预热 070. 基于 nginx+lua 完成商品详情页访问流量实时上报 kafka 的开发 071. 基于 storm+kafka 完成商品访问次数实时统计拓扑的开发 072. 基于 storm 完成 LRUMap 中 topn 热门商品列表的算法讲解与编写 073. 基于 storm+zookeeper 完成热门商品列表的分段存储 074. 基于双重 zookeeper 分布式锁完成分布式并行缓存预热的代码开发 075. 将缓存预热解决方案的代码运行后观察效果以及调试和修复所有的 bug 076. 热点缓存问题：促销抢购时的超级热门商品可能导致系统全盘崩溃的场景 077. 基于 nginx+lua+storm 的热点缓存的流量分发策略自动降级解决方案 078. 在 storm 拓扑中加入热点缓存实时自动识别和感知的代码逻辑 079. 在 storm 拓扑中加入 nginx 反向推送缓存热点与缓存数据的代码逻辑 080. 在流量分发+后端应用双层 nginx 中加入接收热点缓存数据的接口 081. 在 nginx+lua 中实现热点缓存自动降级为负载均衡流量分发策略的逻辑 082. 在 storm 拓扑中加入热点缓存消失的实时自动识别和感知的代码逻辑 083. 将热点缓存自动降级解决方案的代码运行后观察效果以及调试和修复 bug 084. hystrix 与高可用系统架构：资源隔离+限流+熔断+降级+运维监控 085. hystrix 要解决的分布式系统可用性问题以及其设计原则 086. 电商网站的商品详情页缓存服务业务背景以及框架结构说明 087. 基于 spring boot 快速构建缓存服务以及商品服务 088. 快速完成缓存服务接收数据变更消息以及调用商品服务接口的代码编写 089. 商品服务接口故障导致的高并发访问耗尽缓存服务资源的场景分析 090. 基于 hystrix 的线程池隔离技术进行商品服务接口的资源隔离 091. 基于 hystrix 的信号量技术对地理位置获取逻辑进行资源隔离与限流 092. hystrix 的线程池+服务+接口划分以及资源池的容量大小控制 093. 深入分析 hystrix 执行时的 8 大流程步骤以及内部原理 094. 基于 request cache 请求缓存技术优化批量商品数据查询接口 095. 开发品牌名称获取接口的基于本地缓存的 fallback 降级机制 096. 深入理解 hystrix 的短路器执行原理以及模拟接口异常时的短路实验 097. 深入理解线程池隔离技术的设计原则以及动手实战接口限流实验 098. 基于 timeout 机制来为商品服务接口的调用超时提供安全保护 099. 基于 hystrix 的高可用分布式系统架构项目实战课程的总结 100. 基于 request collapser 请求合并技术进一步优化批量查询 101. hystirx 的 fail-fast 与 fail-silient 两种最基础的容错模式 102. 为商品服务接口调用增加 stubbed fallback 降级机制 103. 基于双层嵌套 command 开发商品服务接口的多级降级机制 104. 基于 facade command 开发商品服务接口的手动降级机制 105. 生产环境中的线程池大小以及 timeout 超时时长优化经验总结 106. 生产环境中的线程池自动扩容与缩容的动态资源分配经验 107. hystrix 的 metric 统计相关的各种高阶配置讲解 108. hystrix dashboard 可视化分布式系统监控环境部署 109. 生产环境中的hystrix分布式系统的工程运维经验总结 110. 高并发场景下恐怖的缓存雪崩现象以及导致系统全盘崩溃的后果 111. 缓存雪崩的基于事前+事中+事后三个层次的完美解决方案 112. 基于 hystrix 完成对 redis 访问的资源隔离以避免缓存服务被拖垮 113. 为 redis 集群崩溃时的访问失败增加 fail silent 容错机制 114. 为 redis 集群崩溃时的场景部署定制化的熔断策略 115. 基于 hystrix 限流完成源服务的过载保护以避免流量洪峰打死 MySQL 116. 为源头服务的限流场景增加 stubbed fallback 降级机制 117. 高并发场景下的缓存穿透导致 MySQL 压力倍增问题以及其解决方案 118. 在缓存服务中开发缓存穿透的保护性机制 119. 高并发场景下的 nginx 缓存失效导致 redis 压力倍增问题以及解决方案 120. 在 nginx lua 脚本中开发缓存失效的保护性机制 121. 支撑高并发与高可用的大型电商详情页系统的缓存架构课程总结 122. 如何将课程中的东西学以致用在自己目前的项目中去应用？ 123. 如何带着课程中讲解的东西化为自己的技术并找一份更好的工作？ 第二版 从 123 章起是第二版，主要是实战，重点是商品详情页架构实战 第二版笔记配套项目 124. 大型电商网站的商品详情页的深入分析 125. 大型电商网站的商品详情页系统架构是如何一步一步演进的 126. 亿级流量大型电商网站的商品详情页系统架构的整体设计 127. 商品详情页动态渲染系统：架构整体设计 128. 商品详情页动态渲染系统：大型网站的多机房 4级 缓存架构设计 129. 商品详情页动态渲染系统：复杂的消息队列架构设计 130. 商品详情页动态渲染系统：使用多线程并发提升系统吞吐量的设计 131. 商品详情页动态渲染系统：redis 批量查询性能优化设计 132. 商品详情页动态渲染系统：全链路高可用架构设计 133. 商品详情页动态渲染系统：微服务架构设计 134. 商品详情页动态渲染系统：机房与机器的规划 135. 商品详情页动态渲染系统：部署 CentOS 虚拟机集群 136. 商品详情页动态渲染系统：双机房部署接入层与应用层 Nginx+Lua 137. 商品详情页动态渲染系统：为什么是 twemproxy+redis 而不是 redis cluster？ 138. 商品详情页动态渲染系统：redis 复习以及 twemproxy 基础知识讲解 139. 商品详情页动态渲染系统：部署双机房一主三从架构的 redis 主集群 140. 商品详情页动态渲染系统：给每个机房部署一个 redis 从集群 141. 商品详情页动态渲染系统：为 redis 主集群部署 twemproxy 中间件 142. 商品详情页动态渲染系统：为每个机房的 redis 从集群部署 twemproxy 中间件 143. 商品详情页动态渲染系统：部署 RabbitMQ 消息中间件 144. 商品详情页动态渲染系统：部署 MySQL 数据库 145. 商品详情页动态渲染系统：声音小问题&课程代码二次开发&商品服务需求 146. 商品详情页动态渲染系统：工程师的 why-how-what 思考方法&价格服务说明 147. 商品详情页动态渲染系统：库存服务的场景介绍以及课程需求说明 148. 商品详情页动态渲染系统：微服务与 Spring Cloud 基本介绍 149. 商品详情页动态渲染系统：Spring Boot 与微服务的关系以及开发回顾 150. 商品详情页动态渲染系统：Spring Cloud 之 Eureka 注册中心 151. 商品详情页动态渲染系统：Spring Cloud 之 Ribbon+Rest 调用负载均衡 152. 商品详情页动态渲染系统：Spring Cloud 之 Fegion 声明式服务调用 153. 商品详情页动态渲染系统：Spring Cloud 之 Hystrix 熔断降级 154. 商品详情页动态渲染系统：Spring Cloud 之 Zuul 网关路由 155. 商品详情页动态渲染系统：Spring Cloud 之 Config 统一配置中心 156. 商品详情页动态渲染系统：Spring Cloud 之 Sleuth 调用链路追踪 157. 商品详情页动态渲染系统：Spring Cloud 之 Eureka Server 安全认证 158. 商品详情页动态渲染系统：完成 Spring Boot+Spring Cloud+MyBatis 整合 159. 商品详情页动态渲染系统：基于 Spring Cloud 开发商品服务（一） 160. 商品详情页动态渲染系统：基于 Spring Cloud 开发商品服务（二） 161. 商品详情页动态渲染系统：基于 Spring Cloud 开发价格服务 162. 商品详情页动态渲染系统：基于 Spring Cloud 开发库存服务 163. 商品详情页动态渲染系统：windows 部署 rabbitmq 作为开发测试环境 164. 商品详情页动态渲染系统：windows 部署 redis 作为开发测试环境 165. 商品详情页动态渲染系统：依赖服务将数据变更消息写入 rabbitmq 或双写 redis 166. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据同步服务 167. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据聚合服务 168. 商品详情页动态渲染系统：完成数据同步服务与数据聚合服务的测试 169. 商品详情页动态渲染系统：消息队列架构升级之去重队列 170. 商品详情页动态渲染系统：消息队列架构升级之刷数据与高优先级队列 171. 商品详情页动态渲染系统：吞吐量优化之批量调用依赖服务接口 172. 商品详情页动态渲染系统：吞吐量优化之 redis mget 批量查询数据 173. 商品详情页动态渲染系统：在分发层 nginx 部署流量分发的 lua 脚本 174. 商品详情页动态渲染系统：完成应用层 nginx 的 lua 脚本的编写与部署 175. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据直连服务 176. 商品详情页动态渲染系统：完成多级缓存全链路的测试多个 bug 修复 177. 商品详情页动态渲染系统：商品介绍分段存储以及分段加载的介绍 178. 商品详情页动态渲染系统：高可用架构优化之读链路多级降级思路介绍 179. 商品详情页动态渲染系统：高可用架构优化之 hystrix 隔离与降级 180. 商品详情页动态渲染系统：部署 jenkins 持续集成服务器 181. 商品详情页动态渲染系统：在 CentOS 6 安装和部署 Docker 182. 商品详情页动态渲染系统：在 CentOS 6 安装 maven、git 以及推送 github 183. 商品详情页动态渲染系统：通过 jenkins+docker 部署 eureka 服务 184. 商品详情页动态渲染系统：twemproxy hash tag+mget 优化思路介绍 185. 商品详情页动态渲染系统：所有服务最终修改以及 jenkins+docker 部署 186. 商品详情页 OneService 系统：整体架构设计 187. 商品详情页 OneService 系统：基于 Spring Cloud 构建 OneService 服务 188. 商品详情页 OneService 系统：库存服务与价格服务的代理接口开发 189. 商品详情页 OneService 系统：请求预处理功能设计介绍 190. 商品详情页 OneService 系统：多服务接口合并设计介绍 191. 商品详情页 OneService 系统：基于 hystrix 进行接口统一降级 192. 商品详情页 OneService 系统：基于 hystrix dashboard 进行统一监控 193. 商品详情页 OneService 系统：基于 jenkins+docker 部署 OneService 服务 194. 商品详情页 OneService 系统：基于 jenkins+docker 部署 hystrix terbine 服务 195. 商品详情页前端介绍&课程总结& Java 架构师展望 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"001-introduce.html":{"url":"001-introduce.html","title":"001. 课程介绍以及高并发高可用复杂系统中的缓存架构有哪些东西","keywords":"","body":" 001. 课程介绍以及高并发高可用复杂系统中的缓存架构有哪些东西 现在常见的 java 工程师/架构师对缓存技术的了解和掌握程度 缓存架构/技术掌握的不够，对你的发展带来了哪些阻碍？ 课程的一个简单的介绍 真正能支撑高并发以及高可用的复杂系统中的缓存架构有哪些东西？ 学会了这套课程，能给你带来些什么？工作中以及职业发展中？ 001. 课程介绍以及高并发高可用复杂系统中的缓存架构有哪些东西 现在常见的 java 工程师/架构师对缓存技术的了解和掌握程度 我常年在一些国内最大的那些互联网公司里负责招人，java 这块我们也会招，各种各样的人我都见过 比如大型的互联网公司的人，传统行业的一些人，初级的人，高阶的架构师，高级工程师，技术经理，技术总监，带几十个人 缓存技术，并不是使用 redis 简简单单的存进去取出来，在复杂的场景的时候，就会出现复杂的缓存架构 工作中都会用到一些缓存技术，redis/memcached 基础使用，初步的集群知识 我面试过的人里，能掌握到很少的缓存架构的人，屈指可数（个位数），而且都是在大公司有过类似的大型复杂系统架构经验的人 缓存架构/技术掌握的不够，对你的发展带来了哪些阻碍？ 工作中 如果你这块技术掌握不够，然后你的公司的项目遇到了一些相关的难题，高并发+高性能的场景，hold 不住类似的这种高并发的系统 因为缓存架构做得不好，不到位，实际在公司的项目里，出了一些大 case，导致系统崩溃，造成巨大的经济损失 职业发展中 我面试过的人中有在建立上写了 redis、memcached、activemq、zookeeper、kafka、lucene、activiti、爬虫等等，各种技术写了都几十种技术 没有一样是精通的，redis 就会简单的操作，memcached、activemq、zookeeper、爬虫等全都是简单的操作 叫你说一下架构设计思路，有没有一些考量的点，高并发的中场景，高可用的场景，说不出来 如果是这样的话，那么你不太可能做到更高级的一个职位了，因为很多公司的人也不傻，技术一看就平平庸庸，怎么给你一个很好的职位呢？职业发展怎么做上去呢？ 那么就需要你具备亮点：技术亮点，高人一筹 当你去面试 java 高工、java 资深工、java 架构的时候，有技术亮点，在某些方面有一些造诣，如果你的技术很牛，各种技术都有深度，架构面临过一些复杂的场景，别人搞不定的高并发高可用的系统架构，你都能搞定，职业发展就会做的很好 课程的一个简单的介绍 亿级流量电商网站的商品详情页系统，项目实战、业务背景，当然不可能是一个完完全全系统，会简化，把要讲的东西贯穿起来，学习到亿级流量的电商网站商品详情页的整体架构设计 其中 复杂的缓存架构：才是我们最真实要讲解的东西，支撑高并发、高可用 缓存架构过程中，我们会讲解各种高并发场景下的各种难题，怎么去解决这些难题、缓存架构的过程、各种技术和解决方案、高可用性 「亿级流量电商网站的商品详情页系统」的架构讲解会作为项目背景贯穿，且项目实战； 缓存架构，支撑高并发，高可用的系统架构； 在缓存架构的过程中，高并发以及高可用相关的各种技术点和知识点、解决方案串在一起讲解了 这套课程，下面的 12 个问题学完以后，会学到很多的全网独家的技术 大型电商网站的商品详情页系统的架构 复杂的缓存架构 如何用复杂的缓存架构去支撑高并发 在自己项目中如果有可能将缓存架构做成高可用机会，那么可以学到高可用系统架构构建的技术，做到真正把这课程知识点融会贯通 真正能支撑高并发以及高可用的复杂系统中的缓存架构有哪些东西？ 如何让 redis 集群支撑几十万 QPS 高并发 + 99.99% 高可用 + TB 级海量数据 + 企业级数据备份与恢复？ redis 企业级集群架构 如何支撑高性能以及高并发到极致？同时给缓存架构最后的安全保护层？ (nginx + lua) + redis + ehcache 的三级缓存架构 高并发场景下，如何解决数据库与缓存双写的时候数据不一致的情况？ 企业级的完美的数据库 + 缓存双写一致性解决方案 如何解决大 value 缓存的全量更新效率低下问题？ 缓存维度化拆分解决方案 如何将缓存命中率提升到极致？： 双层 nginx 部署架构，以及 lua 脚本实现的一致性 hash 流量分发策略 如何解决高并发场景下，缓存重建时的分布式并发重建的冲突问题？ 基于 zookeeper 分布式锁的缓存并发重建解决方案 如何解决高并发场景下，缓存冷启动 MySQL 瞬间被打死的问题？ 基于 storm 实时统计热数据的分布式快速缓存预热解决方案 如何解决热点缓存导致单机器负载瞬间超高？ 基于 storm 的实时热点发现，以及毫秒级的实时热点缓存负载均衡降级 如何解决分布式系统中的服务高可用问题？避免多层服务依赖因为少量故障导致系统崩溃？ 基于 hystrix 的高可用缓存服务，资源隔离 + 限流 + 降级 + 熔断 + 超时控制 如何应用分布式系统中的高可用服务的高阶技术？ 基于 hystrix 的容错 + 多级降级 + 手动降级 + 生产环境参数优化经验 + 可视化运维与监控 如何解决恐怖的缓存雪崩问题？避免给公司带来巨大的经济损失？ 独家的事前 + 事中 + 事后三层次完美解决方案 如何解决高并发场景下的缓存穿透问题？避免给 MySQL 带来过大的压力？ 缓存穿透解决方案 如何解决高并发场景下的缓存失效问题？避免给 redis 集群带来过大的压力？ 缓存失效解决方案 学会了这套课程，能给你带来些什么？工作中以及职业发展中？ 工作中 如果你遇到了类似的缓存架构的一些问题，你可以立刻将学到的东西结合你的项目业务融入到架构中去；系统架构重构，抵抗各种更加复杂的场景的架构 职业发展中 缓存、redis、复杂的缓存架构，解决的复杂场景，技术亮点，青睐，拿到更好的职位 学完这套课程能去应聘 java 架构师吗？ 绝对不行，缓存架构 只是 java 架构师必备的一项架构技能，还包含其他的技能，比如高并发（缓存架构、异步队列架构、复杂的分库分表）、高可用架构（hystrix 分布式系统服务的高可用），微服务的架构等 本课程只是让你积累了成长为 java 架构师过程中，必备的一项缓存架构的技能 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"002.html":{"url":"002.html","title":"002. 基于大型电商网站中的商品详情页系统贯穿的授课思路介绍","keywords":"","body":"002. 基于大型电商网站中的商品详情页系统贯穿的授课思路介绍 之前的两个课程「es 的快速入门 + es 高手进阶」是纯粹的技术讲解，大部分反馈说学完也比较迷茫，没有一个场景来支撑。 所以决定拿一个从真实的系统中抽离出来的，简化过后的一个项目，去贯穿整个课程来讲解，提供了连续而且仿真的一个业务场景 各种各样的业务场景，以及在业务场景中面临的难题和问题，去学习一个又一个的技术或者解决方案，或者架构设计思想，这样的授课思路可能是更好的方案 亿级流量电商网站的商品详情页系统，最最核心的架构就是缓存架构，商品详情页系统整体有自己整体的架构 一步一步的去实现商品详情页系统中的一些核心的部分，涉及到最最主要的就是缓存架构，高并发 缓存架构，一步一步讲解各种各样支撑高并发场景的缓存技术，解决方案，架构设计，如何将缓存架构本身做成高可用的架构，缓存架构本身面临的可用性的问题 基于 hystrix 去讲解，缓存架构本身做成高可用的，通过这些的学习你就能掌握到高可用架构的设计以及相关的技术 整体就是在讲解这些东西： 商品详情页系统架构 -> 缓存架构 -> 高并发技术 + 解决方案 + 架构 -> 高可用技术 + 解决方案 + 架构 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"003.html":{"url":"003.html","title":"003. 小型电商网站的商品详情页的页面静态化架构以及其缺陷","keywords":"","body":"003. 小型电商网站的商品详情页的页面静态化架构以及其缺陷 本课程的案例背景是商品详情页的系统架构，以此基础讲解 -> 缓存架构 -> 高并发 -> 高可用 电商网站里，大概可以说分成两种， 小型电商，简单的一种架构方案，页面静态化的方案； 大型电商，复杂的一套架构，大电商，国内排名前几的电商，用得应该咱们这里讲解的这套大型的详情页架构（核心思想） 为了讲解大型电商的详情页架构，这里先把小型的讲解下 部分页面静态化或全量的页面静态化 先来看看页面静态化带来的问题，假设有一个商品详情页的模板如下 商品名称：#{productName} 商品价格：#{productPrice} 商品描述：#{productDesc} 里面的占位语法需要数据来填充，比如保存在 mysql 中的，此时数据变化了或者模板变化了，都需要重新渲染成 html 页面，放在 nginx 上，供用户访问。如果只是某一个商品数据变化了只影响了一个页面那还好说，直接重新渲染这一个即可，但是有商品推荐的，可能其他商品详情页面里面也会出现。这个时候就比较难判定了，可能只会全量渲染了 那么问题来了，对于小网站，页面很少，很实用，非常简单，使用的模板引擎有 velocity、freemarker 等，页面数据管理的 cms 系统，内容管理系统等做一个一键全量渲染功能即可 对于大型网站来说，比如淘宝，他们的商品数据太多了，根本就没有这么多的时间去重新渲染，上亿的商品等，可能需要好几天 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"004.html":{"url":"004.html","title":"004. 大型电商网站的异步多级缓存构建 + nginx 数据本地化动态渲染的架构","keywords":"","body":"004. 大型电商网站的异步多级缓存构建 + nginx 数据本地化动态渲染的架构 大型电商网站的详情页架构一般是这样的核心思路，如上图 两个关键点： 缓存数据生产服务 nginx 上的 html 模板 + 本地缓存数据 来捋一捋流程： 用户访问 nginx 会先从 nginx 的本地缓存获取数据渲染后返回，这个速度很快，因为全是内存操作。 本地缓存数据是有时间的，比如 10 分钟 假如 nginx 本地缓存失效 会从 redis 中获取数据回来并缓存上 假如 redis 中的数据失效 会从缓存数据生产服务中获取数据并缓存上 缓存数据生产服务 本地也有一个缓存，比如用的是 ehcache 他们通过队列监听商品修改等事件，让自己的缓存数据及时更新 其他服务 商品、店铺等服务能获取到商品的修改事件等，及时往 mq 中发出商品的修改事件， 并提供商品原始数据的查询。这里可能是直接从 mysql 库中查询的 这样一来，在缓存上其实就挡掉了很多数据，一层一层的挡并发 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"005.html":{"url":"005.html","title":"005. 能够支撑高并发+高可用+海量数据+备份恢复的 redis 的重要性","keywords":"","body":"005. 能够支撑高并发 + 高可用 + 海量数据 + 备份恢复的 redis 的重要性 一块儿一块儿的去讲解，商品详情页的架构实现 缓存架构的第一块儿，要掌握的很好的，就是 redis 架构 高并发、高可用、海量数据、备份、随时可以恢复，缓存架构如果要支撑这些要点，首先 redis 就得支撑 redis 架构它自身就支持：每秒钟几十万的访问量 QPS、99.99% 的高可用性，TB 级的海量的数据、备份和恢复，有了它缓存架构就成功了一半了 最最简单的模式，无非就是存取 redis，存数据，取数据，支撑你的缓存架构，最基础的就是 redis 架构 解决各种各样高并发场景下的缓存面临的难题，缓存架构中不断的引入各种解决方案和技术，解决高并发的问题 解决各种各样缓存架构本身面临的高可用的问题，缓存架构中引入各种解决方案和技术，解决高可用的问题 总之：redis 很重要 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"006.html":{"url":"006.html","title":"006. 从零开始在虚拟机中一步一步搭建一个 4 个节点的 CentOS 集群","keywords":"","body":" 006. 从零开始在虚拟机中一步一步搭建一个 4 个节点的 CentOS 集群 在虚拟机中安装 CentOS 网络配置注意的地方 桥接模式配置截图 在每个 CentOS 中都安装 Java 和 Perl lrzsz 安装 JAVA 安装 rpm 自动安装后的环境变量问题 安装 Perl 在 4 个虚拟机中安装 CentOS 集群 配置 4 台 CentOS 为 ssh 免密码互相通信 为什么带领大家从 0 开始部署环境 006. 从零开始在虚拟机中一步一步搭建一个 4 个节点的 CentOS 集群 从零开始，纯手工，一步一步搭建出一个 4 个节点的 CentOS 集群， 为我们后面的课程做准备，后面会讲解大型的分布式的 redis 集群架构， 一步一步纯手工搭建 redis 集群，集群部署，主从架构，分布式集群架构 我们后面的课程，会讲解一些实时计算技术的应用，包括 storm，讲解一下 storm 的基础知识，对于 java 工程师来说，会用就可以了，用一些 storm 最基本的分布式实时计算的 feature 就 ok 了，搭建一个 storm 的集群 部署我们整套的系统、nginx、tomcat+java web 应用、mysql 等 尽量以真实的网络拓扑的环境，去给大家演示一下整套系统的部署，不要所有东西，全部放在一个节点上玩儿，也可以去试一试，但是作为课程来说，效果不是太理想 如果要说非常真实的网络拓扑环境来说的话： redis 集群，独立的一套机器 storm 集群，独立的一套机器 nginx 独立部署 tomcat + java web 应用，独立部署 mysql 独立部署 至少十台机器，去部署整套系统，我在自己的笔记本电脑上来讲课的，这么玩儿撑不住的， 电脑本身就 6G 内存的话，学习这种大型的系统架构的课程，是有点吃力，给大家建议，至少到 8G 以上，16G 凑合 我们会纯手工，从零开始，因为很多视频课程，里面讲师都是现成的虚拟机，自己都装好了，包括各种必要的软件，讲课的时候直接基于自己的虚拟机就开始讲解了 很多同学就会发现，想要做到跟讲师一样的环境都很难，自己可能照着样子装了个环境，但是发现，各种问题，各种报错，环境起不来，学习课程的过程很艰难 学视频课程，肯定是要跟着视频的所有的东西自己去做一做，练一练，结果你却因为环境问题，做不了，连不了，那就太惨了 我们的课程从 centos 的镜像文件，到所有的需要使用的软件，全都给你，在自己电脑上，下载一个虚拟机管理软件 virtual box，就可以跟着玩儿了 如果你一步一步跟着视频做，搭建起整个环境，应该问题不大，环境问题，给大家弄成傻瓜式的 在虚拟机中安装 CentOS 启动一个 virtual box 虚拟机管理软件（vmware，我早些年，发现不太稳定，主要是当时搭建一个 hadoop 大数据的集群，发现每次休眠以后再重启，集群就挂掉了） virtual box，发现很稳定，集群从来不会随便乱挂，所以就一直用 virtual box 了 virtual box 官网下载最新 即可（本次笔记使用的是 VirtualBox-6.0.4-128413-Win.exe），安装完成之后，需要先配置下虚拟电脑的默认位置：管理 -> 全局设定 -> 虚拟电脑位置，本次笔记是改动到了 E:\\VirtualBoxVMs 使用课程提供的 CentOS 6.5 镜像即可，CentOS-6.5-i386-minimal.iso 32 位的系统，由于讲师内存不是很大，32 位的能节省不少内存占用 创建虚拟机，打开 Virtual Box，点击“新建”按钮 虚拟机名称：eshop-cache01 类型：Linux 版本：Red Hat（32-bit） 内存大小：1024MB 硬盘创建：选项默认（位置也默认，因为修改了全局的虚拟电脑位置，会默认在该位置下） 设置虚拟机网卡： 选择创建好的虚拟机，点击“设置”按钮，在网络一栏中，连接方式中，选择“Bridged Adapter” 桥接网卡。 安装虚拟机中的 CentOS 6.5 操作系统 选择创建好的虚拟机，点击启动 选择启动盘：选择安装介质（即本地的 CentOS-6.5-i386-minimal.iso 文件）后启动 选择第一项：Install or upgrade an existing system Disc Found 弹窗中选择 Skip；（在虚拟机中鼠标怎么回到自己电脑？键盘右侧的 Ctrl 键，建议修改为 Ctrl + Alt 键，使用起来更方便 ） 图形界面选择 Next 语言选择：默认语言 English -> U.S.English 选择 Baisc Storage Devices -> Yes,discard any data 主机名：eshop-cache01 选择时区：Asia/Shanghai 亚洲/上海 设置初始密码（root 账户的）：hadoop Replace Existing Linux System -> Write changes to disk CentOS 6.5 自己开始安装了，这次选择的是 mini 的安装，所以很快，大概 5 分钟左右 安装完以后，CentOS 会提醒你要重启一下，选择 reboot 重启。 配置网络 重启完成之后，使用 root/hadoop 登录系统， 由于只安装终端模式，没有 ui 模式，登录系统后看到的就和终端登录的类似界面 检测外网：ping www.baidu.com 检测宿主机：ping 192.168.99.111 ip 是你笔记本的局域网 ip 会发现都 ping 不通，配置网络为静态地址 vi /etc/sysconfig/network-scripts/ifcfg-eth0 文件内容，需要删除几个默认的项目，最后保持如下内容 DEVICE=eth0 TYPE=Ethernet ONBOOT=yes BOOTPROTO=dhcp 删除技巧：进入 vi 后，先不要按「i」键进入编辑模式，上下键移动到要删除的行，连续按两下「d」即可删除这一行文本 使用 ifconfig 命令会发现没有 eth0 的信息 使用 service network restart 重启网络，有可能会失败，如果显示 ok 的话，就是默认按照你的宿主机的相关网关信息分配了 ip，但是不要慌 使用配置文件固定 ip 信息 vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改为如下内容 DEVICE=eth0 TYPE=Ethernet ONBOOT=yes BOOTPROTO=static IPADDR=192.168.99.170 NATMASK=255.255.255.0 GATEWAY=192.168.99.1 对于桥接网络模式来说：就是配置成和你宿主机同一个局域网即可 再次 service network restart 重启网络服务和 ifconfig 查看 ip 信息 配置 hosts vi /etc/hosts 增加一行，可以通过 eshop-cache01 访问本机 192.168.99.170 eshop-cache01 配置 SecureCRT 此时就可以使用 SecureCRT 从本机连接到虚拟机进行操作了 一般来说，虚拟机管理软件，virtual box，可以用来创建和管理虚拟机，但是一般不会直接在 virtualbox 里面去操作，因为比较麻烦，没有办法复制粘贴 比如后面我们要安装很多其他的一些东西，perl、java、redis、storm，复制一些命令直接去执行 在这里本人喜欢使用 xshell，就以 xshell 开始了 关闭防火墙 原因是为了方便，因为一些软件如集群之间需要打开固定的端口，防止连接不上，先关闭防火墙 service iptables stop service ip6tables stop chkconfig iptables off chkconfig ip6tables off 还有一个策略需要关闭下（视频中没有解说是为什么这个是什么） vi /etc/selinux/config 内容修改为 SELINUX=disabled windows 上的防火墙也需要关闭，后面要搭建集群，有的大数据技术的集群之间，在本地你给了防火墙的话，可能会没有办法互相连接，会导致搭建失败 配置 yum 工具 yum clean all # 生成缓存，安装的时候从缓存中查询相关数据，提高安装速度 yum makecache 安装 wget，可以通过该工具下载软件 yum install wget 网络配置注意的地方 按照视频和百度来配置了好几个小时，笔记本用的无线网卡，同样无法连接网络， 可以 ping 通宿主机就是不能连接网络。 ::: tip 最后发现问题了，无论是 nat 还是是桥接模式。我在配置中把 GATEWAY 错写尘 GETEWAY 了 ::: 最后选择 nat 勾选接入网线， 全局配置 nat 网络 如上配置说明： 网段：192.168.50.[2 ~ 254] 子网掩码：255.255.255.0 网关：192.168.50.1 vi /etc/sysconfig/network-scripts/ifcfg-eth0 中设置 BOOTPROTO=dhcp 重启网络 service network restart 发现 ok 了。 通过 ifconfig 查看到的分配地址为 检测网络情况，均发现可以 ping 通 检测外网：ping www.baidu.com 检测宿主机：ping 192.168.99.111 ip 是你笔记本的局域网 ip 那么静态化配置如下： DEVICE=eth0 TYPE=Ethernet ONBOOT=yes BOOTPROTO=static IPADDR=192.168.50.10 NETMASK=255.255.255.0 GATEWAY=192.168.50.1 桥接模式配置截图 一句话桥接模式重点：选择和物理机相同的网卡，且网段和物理机在同一网段即可 优点：配置简单，虚拟机在网络删就如同你物理机一 界面名称选择你物理机删连网用网卡； 在每个 CentOS 中都安装 Java 和 Perl 后面的很多软件都会依赖这两个软件 lrzsz 安装 lrzsz 工具可以上传下载文件操作。所以比较方便 yum -y install lrzsz rz # 弹出上传文件的选择框，选择你要上传的文件 sz 文件名 # 弹出下载保存文件框，把 虚拟机上的文件下载到本机 后面我们的软件都装在 /usr/local 中，使用 rm -rf ./* 清空该文件夹下的内容， 我看过该文件夹下的其他目录基本上都是空目录，所以放心删除 JAVA 安装 jdk-7u65-linux-i586.rpm 本次使用这个 java 版本，把包上传到 /usr/local 中 # 安装 java # 后补：官网下载的 jdk8 rpm 包，执行 -ivh 之后就已经安装好了 # 通过 find / -name java 找到安装到了 /usr/java 目录下，环境变量暂时不知道在哪里配置的 rpm -ivh jdk-7u65-linux-i586.rpm # 删除安装包 rm -rf jdk-7u65-linux-i586.rpm 配置环境变量 vi ~/.bashrc # 增加 java 环境变量 export JAVA_HOME=/usr/java/latest export PATH=$PATH:$JAVA_HOME/bin # 刷新环境变量 source ~/.bashrc # 检查是否安装成功 java -version rpm 自动安装后的环境变量问题 通过 rpm 安装之后，可以通过以下方式查找生效的命令路径，通过以下命令查找之后，发现最终指向了 jre/bin/java； 也就是说不是通过环境变量，而是直接是软连方式 [root@eshop-detail01 local]# which java /usr/bin/java [root@eshop-detail01 local]# ls -l /usr/bin/java lrwxrwxrwx 1 root root 22 Jul 15 23:09 /usr/bin/java -> /etc/alternatives/java [root@eshop-detail01 local]# ll /etc/alternatives/java lrwxrwxrwx 1 root root 40 Jul 15 23:09 /etc/alternatives/java -> /usr/java/jdk1.8.0_202-i586/jre/bin/java 安装 Perl perl：是一个基础的编程语言的安装，如同 java 一样 为什么要装 perl？对于大型电商网站的详情页系统来说是很复杂的。nginx+lua 的时候就需要依赖 perl # 先安装 gcc yum install -y gcc # 下载 per wget http://www.cpan.org/src/5.0/perl-5.16.1.tar.gz # 解压 tar -xzf perl-5.16.1.tar.gz # 记得，我们都在 /usr/local 中操作 cd perl-5.16.1 ./Configure -des -Dprefix=/usr/local/perl # 该命令耗时很长，只能等着 make && make test && make install # 检查是否安装成功 perl -v 很多讲师，拿着自己之前花了很多时间调试好的虚拟机环境，去讲课，这个很不负责任， 要全新安装一个环境其实要做的事情还是很多的，有时候自己都有可能忘记了 在 4 个虚拟机中安装 CentOS 集群 现在只装好了一台虚拟机，还需要装三台。笨的方法就是手工一台一台按照上面的步骤去安装； 机器分布 hostName ip eshop-cache01 192.168.99.170 eshop-cache02 192.168.99.171 eshop-cache03 192.168.99.172 eshop-cache04 192.168.99.173 这里选择使用 VirtualBox 的复制功能来完成其他 3 台机器的安装。（复制需要先关闭被复制的机器） 复制 修改每台机器的静态 ip 修改每台机器的 hostName # 手动修改该文件中的主机名 vi /etc/sysconfig/network HOSTNAME=XXXX # 修改 hosts 中的 hostname # 对于 hosts 中的修改，在本次的笔记实践过程中没有发现生成主机名的域名映射，所以可以忽略 vi /etc/hosts 127.0.0.1 localhost # 这个是默认的 127.0.0.1 对应主机名 # 我们主要是修改这个和主机名一致即可 修改完所有的机器后，建议都重启，使用新 xshell 连接复制修改下面的域名地址映射 每台机器的 hosts 配置域名地址映射； vi /etc/hosts 192.168.99.170 eshop-cache01 192.168.99.171 eshop-cache02 192.168.99.172 eshop-cache03 192.168.99.173 eshop-cache04 配置完成之后可以使用 ping 检查是否已经配置好了域名映射，比如在 04 上运行 ping eshop-cache01 对于复制的机器网络服务启动不起来，也就是使用 service network restart 命令失败出现该错误 device eth0 does not seem to be present,delaying initializationvim 解决方案： 如图，修改 vi /etc/udev/rules.d/70-persistent-net.rules 中的内容与桥接网卡的 mac 地址一致即可 配置 4 台 CentOS 为 ssh 免密码互相通信 # 生成秘钥，命令中一直回车即可 ssh-keygen -t rsa # 进入秘钥文件目录 cd /root/.ssh/ # 让自己本机使用 ssh 免密，把 pub 中的内容复制到 authorized_keys 文件中 cp id_rsa.pub authorized_keys # 可使用 ssh 命令连接本机 hostname，提示输入 yes 即可登录 ssh eshop-cache01 # 退出 ssh 登录的终端 exit 把自己的公钥 copy 到要指定机器的 authorized_keys 文件中， 也就是说，你想要免密登录那一台机器就把自己的公钥写入 authorized_keys 文件夹中 # 命令语法如下，在执行该命令的时候需要输入指定机器的密码 ssh-copy-id -i hostname # 比如 ssh-copy-id -i eshop-cache01 ssh-copy-id -i eshop-cache02 ssh-copy-id -i eshop-cache03 ssh-copy-id -i eshop-cache04 一个小技巧，其他三台机器都使用 ssh-copy-id -i eshop-cache01 把公钥集中到一台机器上，这样 authorized_keys 中的文件内容就包含了 4 台机器的公钥，这个时候只需要把该文件内容 copy 到其他 3 台机器覆盖即可，可以使用如下命令来复制 在 eshop-cache01 上 cd /root/.ssh scp authorized_keys eshop-cache02:/root/.ssh scp authorized_keys eshop-cache03:/root/.ssh scp authorized_keys eshop-cache04:/root/.ssh 为什么带领大家从 0 开始部署环境 做 java 在公司里做项目，有几个人是自己去维护 linux 集群的？几乎没有，或者很少很少，类似这一讲要做的事情，其实都是 SRE（运维的同学）去做的 但是对于课程来说，我们只能自己一步一步做，才有环境去学习啊！ 基于虚拟机的 linux 集群环境，都准备好了，手上有 4 台机器，后面玩儿各种 redis、kafka、storm、tomcat、nginx，都有机器了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/007.html":{"url":"redis/007.html","title":"007. 单机版 redis 的安装以及 redis 生产环境启动方案","keywords":"","body":" 007. 单机版 redis 的安装以及 redis 生产环境启动方案 安装单机版 redis redis 的生产环境启动方案 redis cli的使用 小结 007. 单机版 redis 的安装以及 redis 生产环境启动方案 ::: tip 都在 /usr/local 目录中安装和下载软件。有的会自动安装到目录。 有一部分就是我们手工安装到该目录下的 ::: 安装单机版 redis 大家可以自己去官网下载，当然也可以用课程提供的压缩包 安装依赖 tcl，如果先安装 redis 的话，会报错，所以需要安装一下 tcl wget http://downloads.sourceforge.net/tcl/tcl8.6.1-src.tar.gz tar -xzvf tcl8.6.1-src.tar.gz cd /usr/local/tcl8.6.1/unix/ ./configure make && make install 安装 redis 使用 redis-3.2.8.tar.gz（截止 2017 年 4 月的最新稳定版） tar -zxvf redis-3.2.8.tar.gz cd redis-3.2.8 make && make test && make install 在等待很长时间后，最后有一个错误信息 !!! WARNING The following tests failed: *** [err]: Server is able to generate a stack trace on selected systems in tests/integration/logging.tcl expected stack trace not found into log file *** [err]: Test replication partial resync: no backlog (diskless: yes, reconnect: 1) in tests/integration/replication-psync.tcl Expected condition '[s -1 sync_partial_err] > 0' to be true ([s -1 sync_partial_err] > 0) Cleanup: may take some time... OK make[1]: *** [test] Error 1 make[1]: Leaving directory `/usr/local/redis-3.2.8/src' make: *** [test] Error 2 该错误。暂时不知道是否影响安装，但是这里的命令是 && 连接的，所以后面的 make install 没有被执行。 这里单独执行 make install。完成安装试试 [root@eshop-cache01 redis-3.2.8]# make install cd src && make install make[1]: Entering directory `/usr/local/redis-3.2.8/src' Hint: It's a good idea to run 'make test' ;) INSTALL install INSTALL install INSTALL install INSTALL install INSTALL install make[1]: Leaving directory /usr/local/redis-3.2.8/src 安装还是会出现提示说 test 没有通过，但是这里是一个 Hint（提示），经过后面启动方案测试后，可以正常启动 redis 的生产环境启动方案 如果一般的学习课程，你就随便用 redis-server 启动一下 redis，做一些实验，这样的话没什么意义 在生产环境是要把 redis 作为一个系统的 daemon 进程去运行的，每次系统启动 redis 进程一起启动 redis/utils 目录下，有个 redis_init_script 脚本 将 redis_init_script 脚本拷贝到 /etc/init.d 目录中 cp redis_init_script /etc/init.d/ # 将文件修改为 redis_6379，6379 是 redis 的默认端口号 cd /etc/init.d/ mv redis_init_script redis_6379 端口号的配置也在该脚本中配置的 REDISPORT=6379 创建两个目录： /etc/redis（存放 redis 的配置文件） /var/redis/6379（存放 redis 的持久化文件） mkdir /etc/redis mkdir /var/redis/ mkdir /var/redis/6379 修改 redis 配置文件 redis.conf 该文件默认在 redis 安装目录下，拷贝到 /etc/redis 目录中，修改名称为 6379.conf cp /usr/local/redis-3.2.8/redis.conf /etc/redis/ cd /etc/redis/ mv redis.conf 6379.conf 这里为什么要这样修改呢？是因为 redis_init_script 脚本中的 conf 配置指定了该目录下的 端口号.conf 文件 PIDFILE=/var/run/redis_${REDISPORT}.pid CONF=\"/etc/redis/${REDISPORT}.conf\" 修改 redis.conf（6379.conf） 中的部分配置为生产环境 daemonize yes // 让redis以daemon进程运行 pidfile /var/run/redis_6379.pid // 设置redis的pid文件位置 port 6379 // 设置 redis的监听端口号 dir /var/redis/6379 //设置持久化文件的存储位置 建议在 windows 下使用文本编辑器搜索修改后在上传覆盖 启动 redis # 执行 redis_6379 脚本 cd /etc/init.d # 如果没有执行权限的话，修改执行权限 ，可以使用 chmod u+x redis_6379 # chmod 777 redis_6379 ./redis_6379 start 确认 redis 进程是否启动，ps -ef | grep redis 让 redis 跟随系统启动自动启动 使用 chkconfig 命令开启该文件的系统服务, 可以在 redis_6379 配置文件中上面添加 chkconfig 的注释信息 如下，不要在 #!/bin/sh 上面添加 #!/bin/sh # # Simple Redis init.d script conceived to work on Linux systems # as it does use of the /proc filesystem. # chkconfig: 2345 90 10 # description: Redis is a persistent key-value database 添加完成之后，使用以下命令开启随系统启动 chkconfig redis_6379 on 有关 chkconfig 命令的更多信息 参考百度 redis cli的使用 # 停止本机的 6379 端口的 redis 进程 redis-cli shutdown # 关闭指定机器的 redis，不加 shutdown 命令的话就是登陆到 cli redis-cli -h 127.0.0.1 -p 6379 shutdown # ping redis 的端口，看是否正常 redis-cli PING # 默认连接本机 6379 的redis redis-cli # 在 cli 中可以使用 redis 的命令，下面使用最贱的 set 和 get 命令测试 SET k1 v1 GET k1 小结 redis 的技术包括 4 块： 各种数据结构和命令的使用，包括 java api 的使用 一些特殊的解决方案的使用，pub/sub 消息系统、分布式锁、输入的自动完成，等等 日常的管理相关的命令 企业级的集群部署和架构 我们这套课程，实际上是针对企业级的大型缓存架构，用的项目是真实的大型电商网站的详情页系统（缓存） 我们首先讲解的第一块，其实就是企业级的大型缓存架构中的 redis 集群架构（海量数据、高并发、高可用），最最流行，最最常用的分布式缓存系统 所以前面三块的知识不在本课程范围内，当然后面我们做商品详情页系统的业务开发的时候，当然也会去用 redis 的一些命令。 redis 基础知识可以通过教程、书籍、视频去学习 本课程会重点讲解 redis 持久化、主从架构、复制原理、集群架构、数据分布式存储原理、哨兵原理、高可用架构 本课程与网上一些 redis 的教程有什么不同呢？持久化、集群、哨兵这些都是泛泛而讲，简单带你搭建一下，而我会深入集群架构的底层原理，哨兵的底层原理，用一线的经验告诉你 redis 的大规模的架构师如何去支撑海量数据、高并发、高可用的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/008.html":{"url":"redis/008.html","title":"008. redis 持久化机对于生产环境中的灾难恢复的意义","keywords":"","body":"008. redis 持久化机对于生产环境中的灾难恢复的意义 有的可能自己看过一些 redis 的教程，所有的资料其实都会讲解 redis 持久化，但是有个问题，我到目前为止，没有看到有人很仔细的去讲解，redis 的持久化意义 比如 redis 的持久化，RDB、AOF 区别，各自的特点是什么，适合什么场景？redis 的企业级的持久化方案是什么，是用来跟哪些企业级的场景结合起来使用的？ 基于这个原因本课程会着重讲解 redis 持久化的意义 redis 持久化的意义，在于故障恢复 比如你部署了一个 redis，作为 cache 缓存，当然也可以保存一些较为重要的数据，如果没有持久化的话，redis 遇到灾难性故障的时候，就会丢失所有的数据 如果通过持久化将数据存在磁盘上，然后可以定期同步和备份这些文件到一些云存储服务上去，那么就可以保证数据不丢失全部，还是可以恢复一部分数据回来的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/009.html":{"url":"redis/009.html","title":"009. 图解分析 redis 的 RDB 和 AOF 两种持久化机制的工作原理","keywords":"","body":" 009. 图解分析 redis 的 RDB 和 AOF 两种持久化机制的工作原理 RDB AOF 小结 009. 图解分析 redis 的 RDB 和 AOF 两种持久化机制的工作原理 我们已经知道对于一个企业级的 redis 架构来说，持久化是不可减少的 ::: tip 牢记企业级 redis 集群架构是用来支撑海量数据、高并发、高可用 持久化主要是做灾难恢复、数据恢复，也可以归类到高可用的一个环节里面去 ::: 比如你 redis 整个挂了，redis 就不可用了，你要做的事情是让 redis 变得可用，尽快变得可用你会怎么做？ 你会重启 redis，尽快让它对外提供服务，但是就像上一讲说，如果你没做数据备份，这个时候 redis 就算启动了，也不可用，数据没有了，如果这个时候大量的请求过来，缓存全部无法命中，在 redis 里根本找不到数据，这个时候就死定了，缓存雪崩（后面会讲解）问题，所有请求没有在 redis 命中，就会去 mysql 数据库这种数据源头中去找，一下子 mysql 承接高并发，然后就挂了 mysql 挂掉，你都没法去找数据恢复到 redis 里面去，redis 的数据从哪儿来？从 mysql 来，所以这个事情在大型互联网项目中是恐怖的 具体的完整的缓存雪崩的场景，还有企业级的解决方案，到后面讲 如果你把 redis 的持久化做好，备份和恢复方案做到企业级的程度，那么即使你的 redis 故障了，也可以通过备份数据，快速恢复，一旦恢复立即对外提供服务 redis 的持久化跟高可用是有关系的，放在企业级 redis 架构中去讲解 redis 持久化的两种机制：RDB，AOF RDB 对 redis 中的数据执行周期性的持久化，如下图 AOF 每条写入命令作为日志，写入 aof 文件中 为了保证性能，会先写入 os cache 中，然后定期强制执行 fsync 操作将数据刷入磁盘 它的原理： 因为每台单机 redis 的数据量是受内存限制的，所以 aof 文件不会无限增长 且当数据超过内存限制的时候，会自动使用 LRU 算法将一部分数据淘汰掉 AOF 存放的是每条写入命令，所以会不断膨胀，当达到一定时候，会做 rewrite 操作 rewrite 操作：基于当时 redis 内存中的数据，重新构造一个更小的 aof 文件，然后删除旧的 aof 文件 如上图，总结一下： aof 不断被追加，内存中数据有最大限制会自动淘汰，当 aof 中的数据大于内存中数据时，就会执行 rewrite 操作，生成新的 aof 文件 AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集 小结 通过 RDB 或 AOF，都可以将 redis 内存中的数据给持久化到磁盘上面来，然后可以将这些数据备份到别的地方去，比如说阿里云，云服务 如果 redis 挂了，服务器上的内存和磁盘上的数据都丢了，可以从云服务上拷贝回来之前的数据，放到指定的目录中，然后重新启动 redis，redis 就会自动根据持久化数据文件中的数据，去恢复内存中的数据，继续对外提供服务 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/010.html":{"url":"redis/010.html","title":"010. redis 的 RDB 和 AOF 两种持久化机制的优劣势对比","keywords":"","body":" 010. redis 的 RDB 和 AOF 两种持久化机制的优劣势对比 RDB 持久化机制的优点 RDB 持久化机制的缺点 AOF 持久化机制的优点 AOF 持久化机制的缺点 RDB 和 AOF 到底该如何选择 010. redis 的 RDB 和 AOF 两种持久化机制的优劣势对比 RDB 持久化机制的优点 适合做冷备 RDB 会生成多个数据文件，每个数据文件都代表了某一个时刻中 redis 的数据，这种多个数据文件的方式，非常适合做冷备，可以将这种完整的数据文件发送到一些远程的安全存储上去，如云上，以预定好的备份策略来定期备份 redis 中的数据 性能影响小 能让 redis 对外提供的读写服务不受影响，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可 数据恢复快 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。 因为 AOF，存放的指令日志，做数据恢复的时候，其实是要回放和执行所有的指令日志，来恢复出来内存中的所有数据的 RDB 就是一份数据文件，恢复的时候，直接加载到内存中即可 RBD 做冷备的优点？ RDB 生成多个文件，每个文件都代表了某一个时刻的完整的数据快照 AOF 只有一个文件，但是你可以，每隔一定时间，去 copy 一份这个文件出来 那么 RDB 做冷备，优势在哪儿呢？由 redis 去控制固定时长生成快照文件的事情，比较方便; 而 AOF，还需要自己写一些脚本去做这个事情，且在最坏的情况下，提供数据恢复的时候，速度比 AOF 快，所以 RDB 特别适合做冷备份，冷备 RDB 持久化机制的缺点 在故障时，数据丢得多 一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据（因为在内存中还未来得及导出到磁盘） 这个问题也是 rdb 最大的缺点，就是不适合做第一优先的恢复方案，如果你依赖 RDB 做第一优先恢复方案，会导致数据丢失的比较多 性能影响？ 什么鬼？上面说优点的时候说是性能影响小，这里缺点又提到了？ 以下一段话根本理解不了， RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。一般不要让 RDB 的间隔太长，否则每次生成的 RDB 文件太大了，对 redis 本身的性能可能会有影响的 AOF 持久化机制的优点 在故障时，数据丢得少 一般 AOF 会每隔 1 秒，通过一个后台线程执行一次 fsync 操作，保证 os cache 中的数据写入磁盘中，最多丢失 1 秒钟的数据 AOF 文件写入性能高 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复(官方提供了一个修复工具) rewrite 操作对 redis 主线程影响较小 AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在 rewrite log 的时候，会对其中的数据进行压缩，创建出一份需要恢复数据的最小日志出来。再创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 关于这里我实在是没有想到要怎么去做，因为写入的是指令，那么知道当前内存中的存的是哪些指定的数据呢？ AOF 文件内容比较容易阅读 这个特性非常适合做灾难性的误删除的紧急恢复。比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据 AOF 持久化机制的缺点 日志文件稍大 对于同一份数据来说，AOF 日志文件通常比 RDB 数据快照文件更大 性能稍低 AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低，因为 AOF 一般会配置成每秒 fsync 一次日志文件，当然，每秒一次 fsync，性能也还是很高的 发生过 BUG 以前 AOF 发生过 bug，就是通过 AOF 记录的日志，进行数据恢复的时候，没有恢复一模一样的数据出来。所以说，类似 AOF 这种较为复杂的基于命令日志 /merge/ 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。不过 AOF 就是为了避免 rewrite 过程导致的 bug，因此每次 rewrite 并不是基于旧的指令日志进行 merge 的，而是基于当时内存中的数据进行指令的重新构建，这样健壮性会好很多。 说 rewrite 非常复杂，因为是基于当时内存中已有数据进行构建指令达到压缩日志文件的目的，反正我是想不出来怎么实现的 数据恢复较慢 前面都说了，不适合做冷备，数据恢复基于指令稍慢 RDB 和 AOF 到底该如何选择 不要仅仅使用 RDB，因为那样会导致你丢失很多数据 也不要仅仅使用 AOF，因为那样有两个问题 第一，你通过 AOF 做冷备，没有 RDB 做冷备，来的恢复速度更快; 第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug 综合使用 AOF 和 RDB 两种持久化机制 用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复 结论就是：都用，AOF 作为第一恢复方式，RDB 后补 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/011.html":{"url":"redis/011.html","title":"011. redis 的 RDB 持久化配置以及数据恢复实验","keywords":"","body":" 011. redis 的 RDB 持久化配置以及数据恢复实验 如何配置 RDB 持久化机制 RDB 持久化机制的工作流程 基于 RDB 持久化机制的数据恢复实验 011. redis 的 RDB 持久化配置以及数据恢复实验 如何配置 RDB 持久化机制 /etc/redis/6379.conf save 900 1 save 300 10 save 60 10000 以上内容是原始默认的配置，该功能叫做 SNAPSHOTTING（快照） save ：当 n 秒后有 n 个 key 发生改变，就做一次快照备份 可以设置多个检查点，默认设置了 3 个检查点 RDB 持久化机制的工作流程 redis 根据配置自己尝试去生成 rdb 快照文件 fork 一个子进程出来 子进程尝试将数据 dump 到临时的 rdb 快照文件中 完成 rdb 快照文件的生成之后，就替换之前的旧的快照文件 每次生成一个新的快照，都会覆盖之前的老快照，所以只会有一个 dump.rdb 基于 RDB 持久化机制的数据恢复实验 思路： 保存几条数据 关闭 redis 重启 redis 检查数据是否还在 下面使用命令来实验 redis-cli set k1 11 set k2 22 set k3 33 exit redis-cli shutdown # 启动 redis cd /etc/init.d/ ./redis_6379 start # 进入 cli 查看数据是否还存在 redis-cli get k1 get k2 实验证明数据还是存在的。 这里有一个争议点，因为是 redis 自带的停止工具，是一种安全的退出模式，会将内存中的数据立即生成一份 rdb 快照文件，该文件存储在 /var/redis/6379/dump.rdb 中 下面再来测试 2 种非安全的退出模式： 第一种 写入几条数据，然后直接 kill 掉 redis 进程，启动后会发现数据丢失了 第二种 手动配置一个 save 5 1 写入几条数据，等待 5 秒钟，会发现自动进行了一次 dump rdb 快照 可通过查看 dump.rdb 文件更新时间确定 kill -9 redis 进程 启动 redis 查看数据 因为有 save 生效，所以数据都在； ::: tip 在非正常退出 redis 的时候，再次启动会报错 [root@eshop-cache01 init.d]# ./redis_6379 start /var/run/redis_6379.pid exists, process is already running or crashed 由此可以看出来，当 redis 启动的时候回生成一个 pid 文件，如果该文件存在则不能再次启动 这里只能先删除该 pid 文件后，才能启动 redis 了 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/012.html":{"url":"redis/012.html","title":"012. redis 的 AOF 持久化深入讲解各种操作和相关实验","keywords":"","body":" 012. redis 的 AOF 持久化深入讲解各种操作和相关实验 AOF 持久化的配置 AOF 持久化的数据恢复实验 AOF rewrite AOF 破损文件的修复 AOF 和 RDB 同时工作 最后一个小实验 012. redis 的 AOF 持久化深入讲解各种操作和相关实验 AOF 持久化的配置 AOF 持久化，默认是关闭的，RDB 是默认开启的 打开 AOF 持久化机制之后，redis 每次接收到一条写命令，就会写入日志文件中，当然是先写入 os cache 的，然后每隔一定时间再 fsync 一下 /etc/redis/6379.conf 中的 APPEND ONLY MODE 配置区。 appendonly yes 启用后 appendfsync 属性开始生效，有三个策略可选 no：不主动执行fsync 仅仅 redis 负责将数据写入 os cache 就撒手不管了，然后后面 os 自己会时不时有自己的策略将数据刷入磁盘，不可控了 always：每次写入一条数据就执行一次 fsync 每次写入一条数据，立即将这个数据对应的写日志 fsync 到磁盘上去，性能非常非常差，吞吐量很低; 确保说 redis 里的数据一条都不丢，那就只能这样了 everysec：每隔一秒执行一次 fsync 每秒将 os cache 中的数据 fsync 到磁盘，这个最常用的，生产环境一般都这么配置，性能很高，QPS 还是可以上万的 QPS 指每秒钟的请求数量。大概的举个例子 : mysql -> 是基于大量磁盘，1~2K QPS redis -> 基于内存，磁盘做持久化，单机 QPS 一般来说上万没有问题 AOF 持久化的数据恢复实验 先仅仅打开 RDB，写入一些数据，然后 kill -9 杀掉 redis 进程，接着重启 redis，发现数据没了，因为 RDB 快照还没生成（上一节一节做过该实验） 打开AOF的开关，启用 AOF 持久化 写入一些数据，观察 AOF 文件中的日志内容 kill -9 杀掉 redis 进程，重新启动 redis 进程，发现数据被恢复回来了，就是从 AOF 文件中恢复回来的 redis 进程启动的时候，直接就会从 appendonly.aof 中加载所有的日志，把内存中的数据恢复回来 /var/redis/6379 该路径下，是之前配置的路径 [root@eshop-cache01 6379]# cat appendonly.aof *2 $6 SELECT $1 0 *3 $3 set $6 mykey1 $4 123k 上面的内容是我写入了一条 set mykey1 123k 命令之后的。但是不太能看懂是什么 它们先被写入 os cache 的，1 秒后才 fsync 到磁盘中的 AOF rewrite redis 中的数据其实有限的，很多数据可能会自动过期，可能会被用户删除，可能会被 redis 用缓存清除的算法清理掉，总之 redis 中的数据会不断淘汰掉旧的，就一部分常用的数据会被自动保留在 redis 内存中 所以可能很多之前的已经被清理掉的数据，对应的写日志还停留在 AOF 中，AOF 日志文件就一个，会不断的膨胀，到很大很大 所以 AOF 会自动在后台每隔一定时间做 rewrite 操作，比如日志里已经存放了针对 100w 数据的写日志了; redis 内存中只剩下 10 万; 基于内存中当前的 10 万数据构建一套最新的日志，到 AOF 中; 覆盖之前的老日志; 确保 AOF 日志文件不会过大，保持跟 redis 内存数据量一致 redis 2.4 之前，还需要手动，开发一些脚本 crontab 定时通过 BGREWRITEAOF 命令去执行 AOF rewrite，但是 redis 2.4 之后，会自动进行 rewrite 操作 aof rewrite 有两个重要的配置参数 /etc/redis/6379.conf auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 上面的配置意思是： 当 aof 日志超过 64 m 且，上一次 aof 之后的文件大小，比如是 60 m，那么当文件增长到 120 m 的时候，就会触发 rewrite 操作 auto-aof-rewrite-percentage: 增长百分比，比上一次增长多少内容的时候就会触发 rewrite 操作 auto-aof-rewrite-min-size：rewrite 操作的最小文件大小，超过该大小才会执行 rewrite 操作 rewrite 流程 redis fork 一个子进程 子进程基于当前内存中的数据，构建日志，开始往一个新的临时的 AOF 文件中写入日志 redis 主进程，接收到 client 新的写操作之后，在内存中写入日志，同时新的日志也继续写入旧的 AOF 文件 子进程写完新的日志文件之后，redis 主进程将内存中的新日志再次追加到新的 AOF 文件中 用新的日志文件替换掉旧的日志文件 下图对上面文字描述的演示 AOF 破损文件的修复 如果 redis 在 append 数据到 AOF 文件时，机器宕机了，可能会导致 AOF 文件破损 可以用 redis-check-aof --fix 命令来修复破损的 AOF 文件（该命令在 redis 安装目录下） redis-check-aof --fix xxx.aof 可以手动以破坏，然后执行修复： 将 aof 文件删除后两行数据 然后使用 redis-check-aof 修复 查看被修复的文件 修复的原理貌似就是删除掉破损的数据。因为 aof 的内容变少了。 AOF 和 RDB 同时工作 他们的自动执行是互斥的 如果 RDB 在执行 snapshotting，此时用户手动执行 BGREWRITEAOF 命令，那么等 RDB 快照生成之后，才会去执行 AOF rewrite 同时有 RDB snapshot 文件和 AOF 日志文件，那么 redis 重启的时候，会优先使用 AOF 进行数据恢复，因为其中的日志更完整 最后一个小实验 在有 rdb 的 dump 和 aof 的 appendonly 的同时，rdb 里也有部分数据，aof 里也有部分数据，这个时候其实会发现，rdb 的数据不会恢复到内存中 设置 rdb 5 秒保存一次，写入两条数据，等待 rdb 数据持久化后停止 redis 进程 这个时候 rdb 和 aof 中的数据都是完整的 我们模拟让 aof 破损，然后 fix，有一条数据会被 fix 删除 再次用 fix 的 aof 文件去重启 redis，发现数据只剩下一条了 同时存在的时候，会优先使用 aof 文件恢复数据。 数据恢复完全是依赖于底层的磁盘的持久化的，如果 rdb 和 aof 上都没有数据，那就没了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/013.html":{"url":"redis/013.html","title":"013. 在项目中部署 redis 企业级数据备份方案以及各种踩坑的数据恢复容灾演练","keywords":"","body":" 013. 在项目中部署 redis 企业级数据备份方案以及各种踩坑的数据恢复容灾演练 企业级的持久化的配置策略 企业级的数据备份方案 数据恢复方案 013. 在项目中部署 redis 企业级数据备份方案以及各种踩坑的数据恢复容灾演练 到这里为止，其实还是停留在简单学习知识的程度，学会了 redis 的持久化的原理和操作，但是在企业中，持久化到底是怎么去用得呢？ 企业级的数据备份和各种灾难下的数据恢复，是怎么做得呢？ 企业级的持久化的配置策略 在企业中，RDB 的生成策略，用默认的也差不多，如果有可能改动的地方，可能是如下两个配置： save 60 10000：如果你希望尽可能确保说，RDB 最多丢 1 分钟的数据，那么尽量就是每隔 1 分钟都生成一个快照，低峰期，数据量很少，也没必要 AOF 一定要打开，fsync，everysec auto-aof-rewrite-percentage 100: 就是当前 AOF 大小膨胀到超过上次 100%，上次的两倍 auto-aof-rewrite-min-size 64mb: 根据你的数据量来定，16mb，32mb 企业级的数据备份方案 RDB 非常适合做冷备，每次生成之后，就不会再有修改了 数据备份方案：写 crontab 定时调度脚本去做数据备份 小时级：每小时都 copy 一份 rdb 的备份，到一个目录中去，仅仅保留最近 48 小时的备份 日级：每天都保留一份当日的 rdb 的备份，到一个目录中去，仅仅保留最近 1 个月的备份 每天晚上将当前服务器上所有的数据备份，发送一份到远程的云服务上去 每次 copy 备份的时候，都把太旧的备份给删了 这里只能演示前两条，使用脚本来完成 这里在 /usr/local/redis 目录下完成这个备份实验 按小时级备份 copy/redis_rdb_copy_hourly.sh #!/bin/sh # 生成文件夹名称 2019032023 cur_date=`date +%Y%m%d%k` # 以防万一，先删除，再创建目录 rm -rf /usr/local/redis/snapshotting/$cur_date # -p 可以创建多级目录 mkdir -p /usr/local/redis/snapshotting/$cur_date cp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date # 生成 48 小时前的目录 2019031823 del_date=`date -d -48hour +%Y%m%d%k` rm -rf /usr/local/redis/snapshotting/$del_date # 该命令打开的是一个列表，有多条调度任务就一行一个 crontab -e # 每/周日时分 0 秒，也就是每小时执行一次该脚本 0 * * * * sh /usr/local/redis/copy/redis_rdb_copy_hourly.sh 按天备份，与前面的脚本一样，只是时间表达式不一样 copy/redis_rdb_copy_daily.sh #!/bin/sh # 生成文件夹名称 20190320 cur_date=`date +%Y%m%d` # 以防万一，先删除，再创建目录 rm -rf /usr/local/redis/snapshotting/$cur_date # -p 可以创建多级目录 mkdir -p /usr/local/redis/snapshotting/$cur_date cp /var/redis/6379/dump.rdb /usr/local/redis/snapshotting/$cur_date # 生成 一个月前的文件夹 del_date=`date -d -1month +%Y%m%d` rm -rf /usr/local/redis/snapshotting/$del_date # 该命令打开的是一个列表，有多条调度任务就一行一个 crontab -e # 每/周日时分 0 秒，也就是每小时执行一次该脚本 0 * * * * sh /usr/local/redis/copy/redis_rdb_copy_hourly.sh 0 0 * * * sh /usr/local/redis/copy/redis_rdb_copy_daily.sh 数据恢复方案 这里讲解 5 个场景下的数据恢复方案 如果是 redis 进程挂掉 那么重启 redis 进程即可，直接基于 AOF 日志文件恢复数据 如果是 redis 进程所在机器挂掉 那么重启机器后，尝试重启 redis 进程，尝试直接基于 AOF 日志文件进行数据恢复 AOF没有破损，也是可以直接基于 AOF 恢复的 AOF append-only，顺序写入，如果 AOF 文件破损，那么用 redis-check-aof fix 如果 redis 当前最新的 AOF 和 RDB 文件出现了丢失/损坏 那么可以尝试基于该机器上当前的某个最新的 RDB 数据副本进行数据恢复 当前最新的 AOF 和 RDB 文件都出现了丢失/损坏到无法恢复，一般不是机器的故障，是人为 模拟数据恢复-错误的做法：停止 redis之后，先删除 appendonly.aof，然后将我们的 dump.rdb 拷贝过去，然后再重启 redis，这个时候其实不会恢复 dump.rdb 的数据，因为我们开启了 aof，当 aof 不存在的时候，也不会主动去用 dump.rdb 去恢复数据 正确的做法：停止 redis，关闭 aof，拷贝 rdb 备份，重启 redis，确认数据恢复， 直接在命令行热修改 redis 配置，打开 aof，这个 redis 就会将内存中的数据对应的日志，写入 aof 文件中 热修改命令：redis config set appendonly yes 切记不要停止 redis ，修改配置文件为 yes ，再启动 redis。因为这个时候 aof 文件没有生成的话，数据就又会没有的 如果当前机器上的所有 RDB 文件全部损坏 那么从远程的云服务上拉取最新的 RDB 快照回来恢复数据 如果是发现有重大的数据错误，比如某个小时上线的程序一下子将数据全部污染了，数据全错了 那么可以选择某个更早的时间点，对数据进行恢复 举个例子，12 点上线了代码，发现代码有 bug，导致代码生成的所有的缓存数据，写入 redis，全部错了，那么你应该找到一份 11 点的 rdb 的冷备，然后按照上面的步骤，去恢复到 11 点的数据，就可以了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/014.html":{"url":"redis/014.html","title":"014. redis 如何通过读写分离来承载读请求 QPS 超过 10 万 +？","keywords":"","body":" 014. redis 如何通过读写分离来承载读请求 QPS 超过 10 万 +？ redis 高并发跟整个系统的高并发之间的关系 redis 不能支撑高并发的瓶颈在哪里？ 如果 redis 要支撑超过 10万+ 的并发，那应该怎么做？ 014. redis 如何通过读写分离来承载读请求 QPS 超过 10 万 +？ redis 高并发跟整个系统的高并发之间的关系 搞高并发的话，不可避免的要把底层的缓存搞得很好，这里就是 redis 使用 mysql 来支撑高并发的话，就算做到了，那么也是通过一系列复杂的分库分表方案。订单系统中是有事务要求的，QPS 到几万，就已经比较高了，很难提升上去了 要做一些电商的商品详情页，真正的超高并发，QPS 上十万，甚至是百万，一秒钟百万的请求量 光是 redis 是不够的，但是 redis 是整个大型的缓存架构中，支撑高并发的架构里面，非常重要的一个环节 首先，你的底层的缓存中间件，缓存系统，必须能够支撑的起我们说的那种高并发，其次，再经过良好的整体的缓存架构的设计（多级缓存架构、热点缓存），支撑真正的上十万，甚至上百万的高并发 redis 不能支撑高并发的瓶颈在哪里？ 就是 单机 单机 redis 一般情况下能够承载的 QPS 上万到几万不等，根据你的业务操作的复杂性， redis 提供很多复杂的操作，如 lua 脚本等复杂的操作，那么可能会更低。 比如就简单的 kv 查询来说还是比较容易达到上万的。 假设有上千万、上亿的用户来访问，直接就能把你的单机 redis 干死 如果 redis 要支撑超过 10万+ 的并发，那应该怎么做？ 单机的 redis 几乎不太可能 QPS 超过 10万+，除非一些特殊情况，比如你的机器性能特别好，配置特别高，真实物理机，维护做的特别好，而且你的整体的操作不是太复杂 单机在一般就几万。要提高并发，一般的方案是 读写分离，一般来说，对缓存，一般都是用来支撑读高并发的，写的请求是比较少的，可能写请求也就一秒钟几千，一两千，大量的请求都是读，一秒钟二十万次读 也就是 读多写少 的情况下才能用缓存。写多读少可以选择使用异步写，本课程主要讲解缓存 如上图，一主多从，主负责写，并且将数据同步复制到其他 slave 节点，从节点负责读，还可水平扩展 slave 节点以支撑更多的 QPS 主从架构 -> 读写分离 -> 支撑 10万+ 读 QPS 的架构 接下来要讲解的就是怎么实现 redis 的主从架构。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/015.html":{"url":"redis/015.html","title":"015. redis replication 以及 master 持久化对主从架构的安全意义","keywords":"","body":" 015. redis replication 以及 master 持久化对主从架构的安全意义 图解 redis replication 基本原理 redis replication 的核心机制 master 持久化对于主从架构的安全保障的意义 015. redis replication 以及 master 持久化对主从架构的安全意义 redis 高并发的思路：redis replication -> 主从架构 -> 读写分离 -> 水平扩容支撑高并发 本章节主要讲解 redis replication 的最最基本的原理，作为铺垫 图解 redis replication 基本原理 在前一章节基本上已经讲过了，如上图差不多。写操作成功之后，会异步的把数据复制到 slave 上 redis replication 的核心机制 redis 采用异步方式复制数据到 slave 节点 不过 redis 2.8 开始，slave node 会周期性地确认自己每次复制的数据量 一个 master node 是可以配置多个 slave node 的 slave node 也可以连接其他的 slave node slave node 做复制的时候，是不会 block master node 的正常工作的 slave node 在做复制的时候，也不会 block 对自己的查询操作 它会用旧的数据集来提供服务; 但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了 slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量 实现高可用性 slave 有很大的关系，后面会讲解 master 持久化对于主从架构的安全保障的意义 如果采用了主从架构，那么建议必须开启 master node 的持久化！ 很简单的道理，master 提供写，它自己的数据是最完整的，所以需要它自己来做持久化。 如果不使用 master 做持久化的冷备，而采用 slave 来做冷备的话，当 master 死机再重启，因为自己本地没有数据，会将空的数据同步到所有的 slave 上去。 其次 master 的各种备份方案，要不要做，万一说本地的所有文件丢失了; 从备份中挑选一份 rdb 去恢复 master; 这样才能确保 master 启动的时候，是有数据的 后面会讲解哨兵（sentinal）高可用机制，即使采用了该高可用机制，slave node 可以自动接管 master node，但是也可能 sentinal 还没有检测到 master failure，master node 就自动重启了，还是可能导致上面的所有 slave node 数据清空故障 总结： master 持久化开启 冷备方案一定要做（之前讲解的定时备份的方案） 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/016.html":{"url":"redis/016.html","title":"016. redis 主从复制原理、断点续传、无磁盘化复制、过期 key 处理","keywords":"","body":" 016. redis 主从复制原理、断点续传、无磁盘化复制、过期 key 处理 主从架构的核心原理 主从复制的断点续传 无磁盘化复制 过期 key 处理 016. redis 主从复制原理、断点续传、无磁盘化复制、过期 key 处理 主从架构的核心原理 当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node， 如果是重新连接：master node 仅仅会复制给 slave 部分缺少的数据; 如果是首次连接：会触发一次 full resynchronization（全量同步） 开始 full resynchronization 的时候，master 会启动一个后台线程，开始生成一份 RDB 快照文件，同时还会将从客户端收到的所有写命令缓存在内存中。RDB 文件生成完毕之后，master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中。然后 master 会将内存中缓存的写命令发送给 slave，slave 也会同步这些数据。 slave node 如果跟 master node 有网络故障，断开了连接，会自动重连。master如果发现有多个 slave node 都来重新连接，仅仅会启动一个 rdb save 操作，用一份数据服务所有 slave node。 在正常情况下异步复制会很简单，来一条，异步复制一条 主从复制的断点续传 从 redis 2.8 开始，就支持主从复制的断点续传，如果主从复制过程中，网络连接断掉了，那么可以接着上次复制的地方，继续复制下去，而不是从头开始复制一份 master node 会在内存中创建一个 backlog，master 和 slave 都会保存一个 replica offset 和 master id，offset 就是保存在 backlog 中的。如果 master 和 slave 网络连接断掉了，slave 会让 master 从上次的 replica offset 开始继续复制，但是如果没有找到对应的 offset，那么就会执行一次 resynchronization 无磁盘化复制 master 在内存中直接创建 rdb，然后通过网络发送给 slave，不会在自己本地落地磁盘了 该功能是通过配置文件配置的，主要涉及到以下两个参数： repl-diskless-sync：无磁盘同步 默认为 no（关闭状态） repl-diskless-sync-delay：等待一定时长再开始复制，因为要等更多 slave 重新连接过来 过期 key 处理 slave 不会过期 key 只会等待 master 过期 key。 如果 master 过期了一个 key，或者通过 LRU 淘汰了一个 key，那么会模拟一条 del 命令发送给 slave。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/017.html":{"url":"redis/017.html","title":"017. redis replication 的完整流运行程和原理的再次深入剖析","keywords":"","body":" 017. redis replication 的完整流运行程和原理的再次深入剖析 复制的完整流程 数据同步相关的核心机制 全量复制 增量复制 heartbeat 异步复制 017. redis replication 的完整流运行程和原理的再次深入剖析 复制的完整流程 slave node 启动，仅仅保存 master node 的信息 但是复制流程没开始，master 信息包括 host 和 ip ， 那么是从哪儿来的？是在配置文件 redis.conf 里面的 slaveof 配置的 slave node 定时检查是否需要与 master 连接 内部有个定时任务，每秒检查是否有新的 master node 要连接和复制，如果发现，就跟 master node 建立 socket 网络连接 slave node 发送 ping 命令给 master node 口令认证 如果 master 设置了 requirepass，那么 salve node 必须发送 masterauth的口令过去进行认证 master node 第一次执行全量复制，将所有数据发给 slave node master node 后续持续将写命令，异步复制给 slave node 数据同步相关的核心机制 指的就是第一次 slave 连接 msater 的时候，执行的全量复制，该过程里面的一些细节的机制 master 和 slave 都会维护一个 offset master 会在自身不断累加 offset，slave 也会在自身不断累加 offset slave 每秒都会上报自己的 offset 给 master，同时 master 也会保存每个 slave的 offset 这个倒不是说特定就用在全量复制的，主要是 master 和 slave 都要知道各自的数据的 offset，才能知道互相之间的数据不一致的情况 backlog master node 有一个 backlog，默认是 1MB 大小 master node 给 slave node 复制数据时，也会将数据在 backlog 中同步写一份 backlog 主要是用来做全量复制中断候的增量复制的 疑问：那么这个 backlog 里面是存储 offset 的吗？ master run id 通过 info server 命令可以看到 master run id [root@eshop-cache01 ~]# redis-cli 127.0.0.1:6379> info server # Server redis_version:3.2.8 redis_git_sha1:00000000 redis_git_dirty:0 redis_build_id:6daa1ff954b79779 redis_mode:standalone os:Linux 2.6.32-431.el6.i686 i686 arch_bits:32 multiplexing_api:epoll gcc_version:4.4.7 process_id:1045 run_id:9b3e4cb502e78b0b5664f66eeac6eceb36bc8e28 # 这里 tcp_port:6379 uptime_in_seconds:169663 uptime_in_days:1 hz:10 lru_clock:9598685 executable:/usr/local/bin/redis-server config_file:/etc/redis/6379.conf 如果根据 host+ip 定位 master node，是不靠谱的，如果 master node 重启或者数据出现了变化，那么 slave node 应该根据不同的 run id 区分，run id 不同就做全量复制 如上图，解释了为什么要通过 run id 来定位 master node 如果需要不更改 run id 重启 redis，可以使用 redis-cli debug reload 命令 psync 从节点使用 psync 从 master node 进行复制，psync runid offset master node 会根据自身的情况返回响应信息，可能是 FULLRESYNC runid offset 触发全量复制，可能是 CONTINUE 触发增量复制 解释下：假如 runid 与自身不符，那么久可以全量更新数据 全量复制 master 执行 bgsave，在本地生成一份 rdb 快照文件 master node 将 rdb 快照文件发送给 salve node 如果 rdb 复制时间超过 60 秒（可通过 repl-timeout 属性配置），那么 slave node 就会认为复制失败，可以适当调节大这个参数 对于千兆网卡的机器，一般每秒传输 100MB，6G 文件，很可能超过 60s master node 在生成 rdb 时，会将所有新的写命令缓存在内存中，在 salve node 保存了 rdb 之后，再将新的写命令复制给 salve node，保证主从数据一致 client-output-buffer-limit slave 256MB 64MB 60 如果在复制期间，内存缓冲区持续消耗超过 64MB，或者一次性超过 256MB，那么停止复制，复制失败 什么意思呢？比如在等待 slave 同步 rdb 文件的时候，master 接收写的命令在缓冲区超过了 64m 的数据，那么此次复制失败 slave node 接收到 rdb 之后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，同时基于旧的数据版本对外提供服务 如果 slave node 开启了 AOF，那么会立即执行 BGREWRITEAOF，重写 AOF 总的来说，还是比较耗时的，rdb 生成、rdb 通过网络拷贝、slave 旧数据的清理、slave aof rewrite，很耗费时间 如果复制的数据量在 4G~6G 之间，那么很可能全量复制时间消耗到 1 分半到 2 分钟 增量复制 如果全量复制过程中，master-slave 网络连接断掉，那么 salve 重新连接 master 时，会触发增量复制 master 直接从自己的 backlog 中获取部分丢失的数据，发送给 slave node，默认 backlog 就是 1MB msater 就是根据 slave 发送的 psync 中的 offset 来从 backlog 中获取数据的 heartbeat 主从节点互相都会发送 heartbeat 信息 master 默认每隔 10 秒发送一次 heartbeat salve node 每隔 1 秒发送一个 heartbeat 异步复制 master 每次接收到写命令之后，先在内部写入数据，然后异步发送给 slave node 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/018.html":{"url":"redis/018.html","title":"018. 在项目中部署 redis 的读写分离架构（包含节点间认证口令）","keywords":"","body":" 018. 在项目中部署 redis 的读写分离架构（包含节点间认证口令） 启用复制，部署 slave node 强制读写分离 集群安全认证 读写分离架构的测试 在 master 上写数据 在 slave 上获取数据 018. 在项目中部署 redis 的读写分离架构（包含节点间认证口令） 之前几讲都是在铺垫各种 redis replication 的原理和知识，那么关键是怎么搭建呢？ 本章讲解 一主一从，往主节点去写，在从节点去读，可以读到，主从架构就搭建成功了 启用复制，部署 slave node 在 eshop-cache02 上安装 redis，可参考之前的章节 中的「安装单机版 redis」与 「redis 的生产环境启动方案」 安装好之后，开启 slaveof 属性，把该机器变成 slave node /etc/redis/6379.conf slaveof eshop-cache01 6379 强制读写分离 基于主从复制架构，实现读写分离 /etc/redis/6379.conf # 该属性已经默认开启， slave-read-only yes 开启了只读的 redis slave node，会拒绝所有的写操作，这样可以强制搭建成读写分离的架构 集群安全认证 master 上启用安全认证：requirepass eshop-cache01/etc/redis/6379.conf requirepass redis-pass slave 上使用连接口令：masterauth eshop-cache02/etc/redis/6379.conf masterauth redis-pass 也就是 master 启用密码，slave 要持有相同的密码才能连接上 配置完成后，记得重启 cd /etc/init.d/ redis-cli shutdown ./redis_6379 start ::: warning 由于配置名称都一样，不要上传错了。 ::: 读写分离架构的测试 先启动主节点，eshop-cache01 上的 redis实例 再启动从节点，eshop-cache02 上的 redis实例 在 eshop-cache01 上尝试获取数据 [root@eshop-cache01 init.d]# redis-cli 127.0.0.1:6379> get k1 (error) NOAUTH Authentication required. 会发现报错了，原因是之前我们开启了密码，这个时候要怎么连接 redis-cli 呢？ [root@eshop-cache01 init.d]# redis-cli -h redis-cli 3.2.8 Usage: redis-cli [OPTIONS] [cmd [arg [arg ...]]] -h Server hostname (default: 127.0.0.1). -p Server port (default: 6379). -s Server socket (overrides hostname and port). -a Password to use when connecting to the server. 可以看到帮助命令需要使用 -a 来指定密码 [root@eshop-cache01 init.d]# redis-cli -a redis-pass 127.0.0.1:6379> get k1 (nil) # 注意，在关闭 redis 的时候同样也需要使用密码 redis-cli -a redis-pass shutdown 在 master 上写数据 [root@eshop-cache01 init.d]# redis-cli -a redis-pass 127.0.0.1:6379> set k1 123456 OK 在 slave 上获取数据 [root@eshop-cache02 init.d]# redis-cli 127.0.0.1:6379> get k1 (nil) 发现没有获取到数据，这是怎么回事呢？那么一般说明我们的 slave 可能配置有问题。 这个时候要是能看到日志就好了，在 eshop-cache02/etc/redis/6379.conf中， 配置上 logfile /etc/redis/log.log 重启后可以看到日志中出现不能连接到 master [root@eshop-cache02 redis]# ll total 52 -rw-r--r-- 1 root root 46774 Mar 23 2019 6379.conf -rw-r--r-- 1 root root 2719 Mar 19 05:14 log.log [root@eshop-cache02 redis]# tail -f log.log 24489:S 19 Mar 05:14:28.768 # Error condition on socket for SYNC: Connection refused 24489:S 19 Mar 05:14:29.789 * Connecting to MASTER eshop-cache01:6379 原因是：/etc/redis/6379.conf 中的 bind 属性配置没有放开 默认是绑定的 127.0.0.1，只能本机访问 redis。改成本机的内外 ip 地址就可以对外提供服务了，这里由于之前配置了 hosts 映射，使用 hostname # bind 127.0.0.1 bind 127.0.0.1 eshop-cache01 ::: tip bind 可以写多条，如果没有 127 的ip，使用 redis-cli 会默认连接 127 的 ip，这样你自己也不能使用这个本机简便的登录方式了 如果已经使用 redis-cli 连接不上怎么办？需要自己带上 ip 地址访问，如下： redis-cli -h eshop-cache02 shutdown ::: 记得是每个 redis 节点上都要修改成绑定自己机器的 hontname 对外开放访问后（bind 127.0.0.1 eshop-cache01） 终于连接上了 [root@eshop-cache02 init.d]# ./redis_6379 start Starting Redis server... [root@eshop-cache02 init.d]# redis-cli 127.0.0.1:6379> get k1 # 能获取到 master 上的数据 \"123456\" 127.0.0.1:6379> info replication # 查看信息 # Replication role:slave master_host:eshop-cache01 # 可以看到 master 的信息 master_port:6379 master_link_status:up master_last_io_seconds_ago:1 master_sync_in_progress:0 slave_repl_offset:253 slave_priority:100 slave_read_only:1 connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/019.html":{"url":"redis/019.html","title":"019. 对项目的主从 redis 架构进行 QPS 压测以及水平扩容支撑更高 QPS","keywords":"","body":" 019. 对项目的主从 redis 架构进行 QPS 压测以及水平扩容支撑更高 QPS eshop-cace01 压测数据 eshop-cace02 压测数据 redis 支持的 QPS 数据说明 水平扩容 redis 读节点，提升吞吐量 019. 对项目的主从 redis 架构进行 QPS 压测以及水平扩容支撑更高 QPS 你如果要对自己刚刚搭建好的 redis 做一个基准的压测，测一下你的 redis 的性能和 QPS（query per second 每秒查询次数） redis 自己提供的 redis-benchmark 压测工具，是最快捷最方便的，用一些简单的操作和场景去压测（主要是简单） 工具路径：/usr/local/redis-3.2.8/src/redis-benchmark 语法如下，除了下面自带的 三个控制并发，次数，大小等，其他的自带的链接参数也是需要的，如你配置了密码。需要带上 -a redis-pass密码 ./redis-benchmark -c Number of parallel connections (default 50) -n Total number of requests (default 100000) -d Data size of SET/GET value in bytes (default 2) 比如：你的高峰期的访问量，在高峰期，瞬时最大用户量会达到 10 万+，供访问 1000万 次，每次 50 byte 数据 -c 100000，-n 10000000，-d 50 下面是对我的刚才部署好的 redis 测试的数据，这个测试还是需要几分钟时间 eshop-cace01 压测数据 1 核 1G，虚拟机， /usr/local/redis-3.2.8/src [root@eshop-cache01 src]# ./redis-benchmark -a redis-pass 以下数据是粘贴的老师的，数据太多，分屏复制都要复制好多次 ====== PING_INLINE ====== 100000 requests completed in 1.28 seconds 50 parallel clients 3 bytes payload keep alive: 1 99.78% eshop-cace02 压测数据 由于数据太长就不粘贴了 redis 支持的 QPS 数据说明 这个很难给出一个数据，刚才也看到了，这个与机器配置，场景（复制操作？简单操作？数据量大小）都有关系。 如搭建一些集群，专门为某个项目，搭建的专用集群，4 核 4G 内存，比较复杂的操作，数据比较大，几万的 QPS 单机做到，差不多了 一般来说 redis 提供的高并发，至少上万，没问题 看看和那些有关系？ 机器配置 操作的复杂度 数据量的大小 网络带宽/网络开销 所以具体是多少 QPS 需要自己去测试，而且与生产环境可能还不一致， 因为有大量的网络请求的调用。网络开销等 后面讲到的商品详情页的 cache，可能是需要把大串数据拼接在一起，作为一个 json 串，大小可能都几 KB 了，所以根据环境的不同，QPS 也会不一样 水平扩容 redis 读节点，提升吞吐量 就按照上一节课讲解的，再在其他服务器上搭建 redis 从节点，假设单个从节点读请 QPS 在 5 万左右，两个 redis 从节点，所有的读请求打到两台机器上去，承载整个集群读 QPS 在 10万+ 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/020.html":{"url":"redis/020.html","title":"020. redis 主从架构下如何才能做到 99.99% 的高可用性？","keywords":"","body":" 020. redis 主从架构下如何才能做到 99.99% 的高可用性？ 什么是 99.99% 高可用？ redis 不可用是什么？ redis 怎么才能做到高可用？ 020. redis 主从架构下如何才能做到 99.99% 的高可用性？ 什么是 99.99% 高可用？ 不可用：系统挂掉，很难恢复起来，短时间内都不行，这就不可用 高可用：全年 99.99/99.9/99 % 的时间都能正常提供服务就是高可用 这里有一个比较学术点的解释：可用性的高低 是使用 不可用时间 占 总时间 的比例来衡量。不可用时间是从故障发生到故障恢复的时间。比如，可用性 4 个 9 的系统（99.99%），它一年宕机时间不能超过53分钟（=3652460(1-0.9999)）。做到高可用系统，需要尽可能的 降低故障发生的次数 和 *减少故障持续的时间。 redis 不可用是什么？ 单实例不可用？主从架构不可用？不可用的后果是什么？ 如上图，简单说就是 master 不可用的时候，后果就很严重 redis 怎么才能做到高可用？ 通过主备切换，在很短的时间内恢复可用状态。redis 哨兵（sentinal node）功能提供了这种支持。 接下来的几个章节都是讲解哨兵相关知识 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/021.html":{"url":"redis/021.html","title":"021. redis 哨兵架构的相关基础知识的讲解","keywords":"","body":" 021. redis 哨兵架构的相关基础知识的讲解 哨兵的介绍 哨兵的核心知识 为什么 redis 哨兵集群只有 2 个节点无法正常工作？ 4、经典的 3 节点哨兵集群 021. redis 哨兵架构的相关基础知识的讲解 哨兵的介绍 sentinal 中文名是哨兵 哨兵是 redis 集群架构中非常重要的一个组件，主要功能如下 集群监控：负责监控 redis master 和 slave 进程是否正常工作 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址 哨兵本身也是分布式的，作为一个哨兵集群去运行，互相协同工作 故障转移时，判断一个 master node 是宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了 目前采用的是 sentinal 2 版本，相对于 sentinal 1 来说，重写了很多代码，主要是让故障转移的机制和算法变得更加健壮和简单 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性 哨兵 + redis 主从的部署架构，是不会保证数据零丢失的，只能保证 redis 集群的高可用性 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练 需要进行充足的测试，可用把生产数据拿过来进行容灾演练 为什么 redis 哨兵集群只有 2 个节点无法正常工作？ 哨兵集群必须部署 2 个以上节点，如果哨兵集群仅仅部署了个 2 个哨兵实例，且 Configuration: quorum = 1 （只有一个节点投票通过就算选举成功） +----+ +----+ | M1 |---------| R1 | | S1 | | S2 | +----+ +----+ master 宕机，s1 和 s2 中只要有 1 个哨兵认为 master 宕机就可以切换，同时 s1 和 s2 中会选举出一个哨兵来执行故障转移 这个时候，需要 majority，也就是大多数哨兵都是运行的，2 个哨兵的 majority 就是 2（因为至少是 2 个节点以上，2 的 majority=2，3的 majority=2，5 的 majority=3，4 的 majority=2），2 个哨兵都运行着，就可以允许执行故障转移 但是如果整个 M1 和 S1 运行的机器宕机了，那么哨兵只有 1 个了，此时就没有 majority 来允许执行故障转移，虽然另外一台机器还有一个 R1，但是故障转移不会执行 # 4、经典的 3 节点哨兵集群 +----+ | M1 | | S1 | +----+ | +----+ | +----+ | R2 |----+----| R3 | | S2 | | S3 | +----+ +----+ Configuration: quorum = 2，majority 如果 M1 所在机器宕机了，那么三个哨兵还剩下 2 个，S2 和 S3 可以一致认为 master 宕机，然后选举出一个来执行故障转移 同时 3 个哨兵的 majority 是 2，所以还剩下的 2 个哨兵运行着，就可以允许执行故障转移 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/022.html":{"url":"redis/022.html","title":"022. redis 哨兵主备切换的数据丢失问题：异步复制、集群脑裂","keywords":"","body":" 022. redis 哨兵主备切换的数据丢失问题：异步复制、集群脑裂 两种数据丢失的情况 异步复制导致的数据丢失 脑裂导致的数据丢失 解决异步复制和脑裂导致的数据丢失 022. redis 哨兵主备切换的数据丢失问题：异步复制、集群脑裂 两种数据丢失的情况 异步复制导致的数据丢失 因为 master -> slave 的复制是异步的，所以可能有部分数据还没复制到 slave，master 就宕机了，此时这些部分数据就丢失了 脑裂导致的数据丢失 何为脑裂？如上图由于一个集群中的 master 恰好网络故障，导致与 sentinal 联系不上了，sentinal 把另一个 slave 提升为了 master。此时就存在两个 master了。 当我们发现的时候，停止掉其中的一个 master，手动切换成 slave，当它连接到提升后的 master 的时候，会开始同步数据，那么自己脑裂期间接收的写数据就被丢失了 解决异步复制和脑裂导致的数据丢失 主要通过两个配置参数来解决 min-slaves-to-write 1 min-slaves-max-lag 10 如上两个配置：要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒，如果超过 1 个 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了 此配置保证就算脑裂了，那么最多只能有 10 秒的数据丢失 上图说明了脑裂时，master 拒绝写数据的时候，client 可能额外需要做的事情，client 是说使用方，不是 redis 的东西。 下图也是一样的道理，有如上两个参数配置控制的话，脑裂时会减少数据的丢失 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/023.html":{"url":"redis/023.html","title":"023. redis 哨兵的多个核心底层原理的深入解析（包含 slave 选举算法）","keywords":"","body":" 023. redis 哨兵的多个核心底层原理的深入解析（包含 slave 选举算法） sdown 和 odown 转换机制 哨兵集群的自动发现机制 slave 配置的自动纠正 slave-master 选举算法 quorum 和 majority configuration epoch configuraiton 传播 023. redis 哨兵的多个核心底层原理的深入解析（包含 slave 选举算法） sdown 和 odown 转换机制 sdown 和 odown 是两种失败状态 sdown 是主观宕机 一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机 odown 是客观宕机 如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机 sdown 达成的条件很简单，如果一个哨兵 ping 一个 master，超过了 is-master-down-after-milliseconds（在哨兵配置文件中配置的） 指定的毫秒数之后，就主观认为 master 宕机 sdown 到 odown 转换的条件很简单，如果一个哨兵在指定时间内，收到了 quorum 指定数量的其他哨兵也认为那个 master 是 sdown 了，那么就认为是 odown 了，客观认为 master 宕机 哨兵集群的自动发现机制 哨兵互相之间的发现，是通过 redis 的 pub/sub 系统实现的，每个哨兵都会往 __sentinel__:hello 这个 channel 里发送一个消息，这时候所有其他哨兵都可以消费到这个消息，并感知到其他的哨兵的存在 每隔两秒钟，每个哨兵都会往自己监控的某个 master+slaves 对应的 __sentinel__:hello channel 里发送一个消息，内容是自己的 host、ip和 runid 还有对这个 master 的监控配置 每个哨兵也会去监听自己监控的每个 master+slaves 对应的 __sentinel__:hello channel，然后去感知到同样在监听这个 master+slaves 的其他哨兵的存在 每个哨兵还会跟其他哨兵交换对 master 的监控配置，互相进行监控配置的同步 slave 配置的自动纠正 哨兵会负责自动纠正 slave 的一些配置，比如 slave 如果要成为潜在的 master 候选人，哨兵会确保 slave 在复制现有 master 的数据; 如果 slave 连接到了一个错误的 master 上，比如故障转移之后，那么哨兵会确保它们连接到正确的 master 上 slave->master 选举算法 如果一个 master 被认为 odown 了，而且 majority 哨兵都允许了主备切换，那么某个哨兵就会执行主备切换操作，此时首先要选举一个 slave 来 会考虑 slave 的一些信息 跟 master 断开连接的时长 slave 优先级 复制 offset run id 如果一个 slave 跟 master 断开连接已经超过了 down-after-milliseconds的 10 倍，外加 master 宕机的时长，那么 slave 就被认为不适合选举为 master，公式如下 (down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state 接下来会对 slave 进行排序 按照 slave 优先级进行排序，slave priority（redis 配置文件中的属性配置，默认为 100） 越低，优先级就越高 如果 slave priority 相同，那么看 replica offset，哪个 slave 复制了越多的数据，offset 越靠后，优先级就越高 如果上面两个条件都相同，那么选择一个 run id 比较小的那个 slave quorum 和 majority 每次一个哨兵要做主备切换，首先需要 quorum 数量的哨兵认为 odown，然后选举出一个哨兵来做切换，这个哨兵还得得到 majority 哨兵的授权，才能正式执行切换 如果 quorum 但是如果 quorum >= majority，那么必须 quorum 数量的哨兵都授权；比如 5个哨兵，quorum=5，那么必须 5 个哨兵都同意授权，才能执行切换 configuration epoch 哨兵会对一套 redis master+slave 进行监控，有相应的监控的配置 执行切换的那个哨兵，会从要切换到的新 master（salve->master）那里得到一个 configuration epoch，这就是一个 version 号，每次切换的 version 号都必须是唯一的 如果第一个选举出的哨兵切换失败了，那么其他哨兵，会等待 failover-timeout 时间，然后接替继续执行切换，此时会重新获取一个新的 configuration epoch，作为新的 version 号 configuraiton 传播 哨兵完成切换之后，会在自己本地更新生成最新的 master 配置，然后同步给其他的哨兵，就是通过之前说的 pub/sub 消息机制 这里之前的 version 号就很重要了，因为各种消息都是通过一个 channel 去发布和监听的，所以一个哨兵完成一次新的切换之后，新的 master 配置是跟着新的 version 号的 其他的哨兵都是根据版本号的大小来更新自己的 master 配置的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/024.html":{"url":"redis/024.html","title":"024. 在项目中以经典的 3 节点方式部署哨兵集群","keywords":"","body":" 024. 在项目中以经典的 3 节点方式部署哨兵集群 哨兵的配置文件 在 eshop-cache03 上再部署一个 redis sentinel 配置 同步 linux 集群时间 启动哨兵进程 检查哨兵状态 024. 在项目中以经典的 3 节点方式部署哨兵集群 动手实操，练习如何操作部署哨兵集群，如何基于哨兵进行故障转移，还有一些企业级的配置方案 在这之前我一直以为 redis sentinel 是另外的一个项目，其实不是，还是 同一个 redis ，不过是通过 redis-sentinel + sentinel 配置文件 启动的另一个程序 哨兵的配置文件 /usr/local/redis-3.2.8/sentinel.conf 是一个模板配置文件，主要的配置项目如下 sentinel monitor mymaster 127.0.0.1 6379 2 sentinel down-after-milliseconds mymaster 60000 sentinel failover-timeout mymaster 180000 sentinel parallel-syncs mymaster 1 sentinel monitor resque 192.168.1.3 6380 4 sentinel down-after-milliseconds resque 10000 sentinel failover-timeout resque 180000 sentinel parallel-syncs resque 5 可以看到如上的配置前缀一样，后面开始不一样了，这个是为什么呢？ 是因为一套哨兵集群可以为多个 maser-slaves 的 redis 集群服务。 mymaster、resque 是每个 redis 主从集群分配一个逻辑的名称 上面的配置是一个最小的哨兵配置，就监控了两个 master node，如果发生了 master-slave 故障转移，或者新的哨兵进程加入哨兵集群，那么哨兵会自动更新自己的配置文件 sentinel monitor mymaster 127.0.0.1 6379 它的含义：sentinel monitor master-group-name hostname port quorum 配置了对哪一个 master 进行监控，并配置了 quorum 参数 quorum 的解释如下： 至少多少个哨兵要一致同意，master 进程挂掉了，或者 slave 进程挂掉了，或者要启动一个故障转移操作 quorum 是用来识别故障的，真正执行故障转移的时候，还是要在哨兵集群执行选举，选举一个哨兵进程出来执行故障转移操作 假设有 5 个哨兵，quorum=2 那么如果 5 个哨兵中的 2 个都认为 master 挂掉了; 2 个哨兵中的一个就会做一个选举，选举一个哨兵出来，执行故障转移；如果 5 个哨兵中有 3 个哨兵都是运行的，那么故障转移就会被允许执行 sentinel down-after-milliseconds mymaster 60000 超过多少毫秒跟一个 redis 实例断了连接，哨兵就可能认为这个 redis 实例挂了 sentinel parallel-syncs mymaster 1 新的 master 切换之后，同时有多少个 slave 被切换到去连接新 master，重新做同步，数字越低，花费的时间越多 哨兵将 slave 升级为 master 后，一次操作能将几个 slave 切换到新的 master 上去。 假设你的 redis 是 1 个 master，4 个 slave： master 宕机了，4 个 slave 中有 1 个切换成了 master，剩下 3 个 slave 就要挂到新的 master 上面去 这个时候，如果 parallel-syncs=1，那么 3 个 slave，一个一个地挂接到新的 master 上面去，1 个挂接完，而且从新的 master sync 完数据之后，再挂接下一个 如果 parallel-syncs 是 3，那么一次性就会把所有 slave 挂接到新的 master 上去 sentinel failover-timeout mymaster 180000 执行故障转移的 timeout 超时时长 在 eshop-cache03 上再部署一个 redis 可参考之 018. 在项目中部署 redis 的读写分离架构（包含节点间认证口令） 中的部署方式。 ::: tip 在这次安装 eshop-cache03 机器上的 redis slave node，会把口令认证给去掉，我们自己学习就不要加口令了，写起命令来也比较麻烦 ::: sentinel 配置 使用 /usr/local/redis-3.2.8/sentinel.conf 作为目标，在 windows 上配置好三个机器的配置，再统一上传 这里记录下主要的配置，源文件说明太多了，这里不贴 以下文件除了 bind 的 ip 不一致外，其他的都是相同的 mkdir /etc/sentinal mkdir -p /var/sentinal/5000 /etc/sentinel/5000.conf ------------------- eshop-cache01 port 5000 bind eshop-cache01 dir /var/sentinal/5000 sentinel monitor mymaster eshop-cache01 6379 2 sentinel down-after-milliseconds mymaster 30000 sentinel failover-timeout mymaster 60000 sentinel parallel-syncs mymaster 1 ------------------- eshop-cache02 port 5000 bind eshop-cache02 dir /var/sentinal/5000 sentinel monitor mymaster eshop-cache01 6379 2 sentinel down-after-milliseconds mymaster 30000 sentinel failover-timeout mymaster 60000 sentinel parallel-syncs mymaster 1 ------------------- eshop-cache03 port 5000 bind eshop-cache03 dir /var/sentinal/5000 sentinel monitor mymaster eshop-cache01 6379 2 sentinel down-after-milliseconds mymaster 30000 sentinel failover-timeout mymaster 60000 sentinel parallel-syncs mymaster 1 同步 linux 集群时间 yum install ntpdate ntpdate -u ntp.api.bz ntpdate ntp1.aliyun.com 启动哨兵进程 redis-sentinel /etc/sentinal/5000.conf 这种启动方式是通过前台启动的，所以可以能看到日志，一台一台启动就好 ------------------- eshop-cache01 1581:X 24 Mar 00:26:46.650 # Sentinel ID is 552d04d3c53079053d30942e339b1270615c1139 1581:X 24 Mar 00:26:46.650 # +monitor master mymaster 192.168.99.170 6379 quorum 2 1581:X 24 Mar 00:27:16.689 # +sdown slave 192.168.99.171:6379 192.168.99.171 6379 @ mymaster 192.168.99.170 6379 ------------------- eshop-cache02 24665:X 24 Mar 00:25:10.597 # Sentinel ID is df47be63833763baccf4e42bbb94b6cf3bae7970 24665:X 24 Mar 00:25:10.597 # +monitor master mymaster 192.168.99.170 6379 quorum 2 24665:X 24 Mar 00:25:40.637 # +sdown master mymaster 192.168.99.170 6379 ------------------- eshop-cache03 24426:X 24 Mar 00:25:18.997 # Sentinel ID is 86a7fdb94cfa1a138e002d876bbf3a7466fe7570 24426:X 24 Mar 00:25:18.997 # +monitor master mymaster 192.168.99.170 6379 quorum 2 24426:X 24 Mar 00:25:49.069 # +sdown master mymaster 192.168.99.170 6379 哨兵之间，互相会自动进行发现，用的就是之前说的 pub/sub，消息发布和订阅 channel 消息系统和机制 我这里启动不知道为什么不像视频中一样，只有 eshop-cache01 有打印日志； 找到问题了： /etc/sentinal/5000.conf 中 bind 127.0.0.1 eshop-cache02 这样写上 127.0.0.1 不能进行通信 正常的日志如下 1052:X 24 Mar 00:44:22.609 # Sentinel ID is 507b3bc6e379011b990bda44b73221b8f7b305c1 1052:X 24 Mar 00:44:22.609 # +monitor master mymaster 192.168.99.170 6379 quorum 2 1052:X 24 Mar 00:44:33.454 * +sentinel sentinel a1cd62295346683bcd8f8b388ac64e83897a13dd 192.168.99.172 5000 @ mymaster 192.168.99.170 6379 1052:X 24 Mar 00:45:27.155 * +sentinel sentinel d7a9812a3f905d07df46986f1b21388a16df39b4 192.168.99.171 5000 @ mymaster 192.168.99.170 6379 检查哨兵状态 # 连接指定哨兵 redis-cli -h eshop-cache01 -p 5000 # 查看监控的 redis 集群配置 sentinel master mymaster # 查看集群下的 slave node 信息 sentinel slaves mymaster # 查看监控 mymaster 集群的所有哨兵 sentinel sentinels mymaster # 查看 mymaster 集群的 master 信息 sentinel get-master-addr-by-name mymaster # 如下的 master 是 170.其他命令数据都很多，只有这个只有2行信息 eshop-cache03:5000> sentinel get-master-addr-by-name mymaster 1) \"192.168.99.170\" 2) \"6379\" 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/025.html":{"url":"redis/025.html","title":"025. 对项目中的哨兵节点进行管理以及高可用 redis 集群的容灾演练","keywords":"","body":" 025. 对项目中的哨兵节点进行管理以及高可用 redis 集群的容灾演练 哨兵节点的增加和删除 redis slave 的永久下线 slave 切换为 Master 的优先级 基于哨兵集群架构下的安全认证 容灾演练 哨兵的生产环境部署 025. 对项目中的哨兵节点进行管理以及高可用 redis 集群的容灾演练 哨兵节点的增加和删除 首先增加 sentinal 有自动发现功能，发现之后就把该哨兵信息添加到自己信息中了，这个可以通过一个实验来证明： 上一章启动了 3 台 sentinal， 在机器重启之后，只启动其中的一台机器 [root@eshop-cache03 ~]# redis-sentinel /etc/sentinal/5000.conf 连接上查看 sentinals 信息 redis-cli -p 5000 -h eshop-cache03 eshop-cache03:5000> sentinel sentinels mymaster 会发现出现了 01 和 02 的信息，但是此时 01 和 02 并没有启动 执行 SENTINEL RESET * 命令 执行之后，再次查看 sentinel sentinels mymaster 信息，会发现变成空的了，消失了 删除 sentinal 的步骤 停止 sentinal 进程 SENTINEL RESET * （重要） 在所有存活的 sentinal 上执行，清理所有的 master 状态 SENTINEL MASTER mastername 在所有存活的 sentinal 上执行，查看自己对 mastername 的集群信息是否与其他 sentinal 一致 redis slave 的永久下线 让 master 摘除某个已经下线的 slave：SENTINEL RESET mastername，在所有的哨兵上面执行 其实还是重置检查一下的有意思 slave 切换为 Master 的优先级 slave->master 选举优先级：slave-priority，值越小优先级越高 基于哨兵集群架构下的安全认证 每个 slave 都有可能切换成 master，所以每个实例都要配置两个指令 master 上启用安全认证，requirepass master 连接口令，masterauth 在 sentinal 的配置文件里面配置密码： sentinel auth-pass 容灾演练 通过哨兵看一下当前的 master：SENTINEL get-master-addr-by-name mymaster 这里的 master 在 02 上面，直接 kill 掉 master；查看 哨兵的日志变化 [root@eshop-cache02 ~]# ps -ef | grep redis root 960 1 2 01:00 ? 00:00:42 /usr/local/bin/redis-server 127.0.0.1:6379 root 1049 1009 3 01:21 pts/0 00:00:26 redis-sentinel eshop-cache02:5000 [sentinel] root 1068 1054 0 01:33 pts/1 00:00:00 grep redis [root@eshop-cache02 ~]# kill 960 等待一会，该配置时间 sentinel down-after-milliseconds mymaster 30000 30 秒后， 可以看到哨兵日志发生了变化 注意：下面的日志是在切换 master 的哨兵上看到的。非切换的哨兵日志上不会有这么多信息 1037:X 24 Mar 01:55:05.525 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1037:X 24 Mar 01:55:05.525 # Sentinel ID is 507b3bc6e379011b990bda44b73221b8f7b305c1 1037:X 24 Mar 01:55:05.525 # +monitor master mymaster 192.168.99.171 6379 quorum 2 1037:X 24 Mar 01:55:15.551 * +convert-to-slave slave 192.168.99.170:6379 192.168.99.170 6379 @ mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:07.924 # +sdown master mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.013 # +odown master mymaster 192.168.99.171 6379 #quorum 2/2 1037:X 24 Mar 01:57:08.013 # +new-epoch 3 1037:X 24 Mar 01:57:08.013 # +try-failover master mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.027 # +vote-for-leader 507b3bc6e379011b990bda44b73221b8f7b305c1 3 1037:X 24 Mar 01:57:08.138 # a1cd62295346683bcd8f8b388ac64e83897a13dd voted for a1cd62295346683bcd8f8b388ac64e83897a13dd 3 1037:X 24 Mar 01:57:08.152 # d7a9812a3f905d07df46986f1b21388a16df39b4 voted for 507b3bc6e379011b990bda44b73221b8f7b305c1 3 1037:X 24 Mar 01:57:08.207 # +elected-leader master mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.207 # +failover-state-select-slave master mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.291 # +selected-slave slave 192.168.99.170:6379 192.168.99.170 6379 @ mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.291 * +failover-state-send-slaveof-noone slave 192.168.99.170:6379 192.168.99.170 6379 @ mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.351 * +failover-state-wait-promotion slave 192.168.99.170:6379 192.168.99.170 6379 @ mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.947 # +promoted-slave slave 192.168.99.170:6379 192.168.99.170 6379 @ mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.947 # +failover-state-reconf-slaves master mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.979 # +failover-end master mymaster 192.168.99.171 6379 1037:X 24 Mar 01:57:08.982 # +switch-master mymaster 192.168.99.171 6379 192.168.99.170 6379 1037:X 24 Mar 01:57:08.983 * +slave slave 192.168.99.171:6379 192.168.99.171 6379 @ mymaster 192.168.99.170 6379 1037:X 24 Mar 01:57:39.038 # +sdown slave 192.168.99.171:6379 192.168.99.171 6379 @ mymaster 192.168.99.170 6379 三个哨兵进程都认为 master 是 sdown 了 超过 quorum 指定的哨兵进程都认为 sdown 之后，就变为 odown 哨兵 1 是被选举为要执行后续的主备切换的那个哨兵 哨兵 1 去新的 master（slave）获取了一个新的 config version 尝试执行 failover 投票选举出一个 slave 区切换成 master，每个哨兵都会执行一次投票 让 salve，slaveof noone，不让它去做任何节点的 slave 了; 把 slave 提拔成 master; 旧的 master 认为不再是 master 了 哨兵就自动认为之前的 171:6379 变成了 slave 了，190:6379 变成了 master 了 哨兵去探查了一下 171:6379 这个 salve 的状态，认为它 sdown 了 此时再查看 /etc/sentinal/5000.conf 中的配置文件的时候，会发现一些配置跟着改变了，比如 # 最开始我们配置的是 hostname，这里被更新成了 ip sentinel monitor mymaster 192.168.99.170 6379 2 将旧的 master 重新启动，查看变化 [root@eshop-cache02 ~]# /etc/init.d/redis_6379 start /var/run/redis_6379.pid exists, process is already running or crashed [root@eshop-cache02 ~]# rm -rf /var/run/redis_6379.pid [root@eshop-cache02 ~]# /etc/init.d/redis_6379 start Starting Redis server... [root@eshop-cache02 ~]# redis-cli 127.0.0.1:6379> info replication # Replication role:slave // 这里，重启之后变成了 slave master_host:192.168.99.170 master_port:6379 master_link_status:up master_last_io_seconds_ago:0 master_sync_in_progress:0 slave_repl_offset:1160 slave_priority:100 slave_read_only:1 connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 看看之前执行故障转移的哨兵日志 # sdown 状态变成了减号 1037:X 24 Mar 02:09:43.363 # -sdown slave 192.168.99.171:6379 192.168.99.171 6379 @ mymaster 192.168.99.170 6379 # 并且转换成了 slave 1037:X 24 Mar 02:09:53.373 * +convert-to-slave slave 192.168.99.171:6379 192.168.99.171 6379 @ mymaster 192.168.99.170 6379 小结： 手动杀掉master 哨兵能否执行主备切换，将 slave 切换为 master 哨兵完成主备切换后，新的 master 能否使用 故障恢复，将旧的 master 重新启动 哨兵能否自动将旧的 master 变为 slave，挂接到新的 master 上面去，而且也是可以使用的 哨兵的生产环境部署 在 /etc/sentinal/5000.conf 中增加两个配置 # 后台启动 daemonize yes # 把日志打印到指定位置 logfile /var/log/sentinal/5000/sentinal.log 记得要把目录创建好 mkdir -p /var/log/sentinal/5000 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/026.html":{"url":"redis/026.html","title":"026. redis 如何在保持读写分离+高可用的架构下，还能横向扩容支撑 1T + 海量数据","keywords":"","body":" 026. redis 如何在保持读写分离+高可用的架构下，还能横向扩容支撑 1T + 海量数据 课程前说明 单机 redis 在海量数据面前的瓶颈 怎么才能够突破单机瓶颈，让 redis 支撑海量数据？ redis 的集群架构 redis cluster vs replication + sentinal 026. redis 如何在保持读写分离+高可用的架构下，还能横向扩容支撑 1T + 海量数据 课程前说明 后面几个章节会老提到 读写分离和 master。之前的课程讲解中说了读写分离后的水平扩容是通过扩容 slave 来达到的。 但是在实际生产环境中，读写分离支持不是很好，特别的 java 这种客户端，可以做到但是稍微复杂 这里的说明其实我也没有太听明白，记住一条信息：通过 master 去扩容的，至于为什么，后面大概 29 讲的时候会讲解 单机 redis 在海量数据面前的瓶颈 之前讲解的一主多从架构，master 的瓶颈 这种架构的瓶颈只是解决了 QPS，但是没有解决海量数据的问题 单机 32G 内存，假如我们就希望存储 1T 的数据呢？ 怎么才能够突破单机瓶颈，让 redis 支撑海量数据？ 这个没有看明白是怎么怎么实际上能支撑海量数据的，难道是要通过路由均衡负载？ redis 的集群架构 redis cluster 支撑 N 个 redis master node，每个 master node 都可以挂载多个 slave node 简单说：redis cluster = 多 master + 读写分离 + 高可用 我们只要基于 redis cluster 去搭建 redis 集群即可，不需要手工去搭建 replication 复制+主从架构+读写分离+哨兵集群+高可用 redis cluster vs replication + sentinal 如果你的数据量很少，主要是承载高并发高性能的场景，比如你的缓存一般就几个 G，单机足够了 replication 一个 mater，多个 slave，要几个 slave 跟你的要求的读吞吐量有关系，然后自己搭建一个 sentinal 集群，去保证 redis 主从架构的高可用性，就可以了 redis cluster 主要是针对海量数据+高并发+高可用的场景，海量数据，如果你的数据量很大，那么建议就用 redis cluster 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/027.html":{"url":"redis/027.html","title":"027. 数据分布算法：hash+ 一致性 hash + redis cluster 的 hash slot","keywords":"","body":" 027. 数据分布算法：hash+ 一致性 hash + redis cluster 的 hash slot redis cluster 介绍 最老土的 hash 算法和弊端（大量缓存重建） 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） redis cluster 的 hash slot 算法 027. 数据分布算法：hash+ 一致性 hash + redis cluster 的 hash slot 讲解分布式数据存储的核心算法，数据分布的算法 hash 算法 -> 一致性 hash 算法（memcached） -> redis cluster 的 hash slot 算法 用不同的算法，就决定了在多个 master 节点的时候，数据如何分布到这些节点上去，解决这个问题 看到这里的时候，已经明白了，可能是通过 key 去路由到多个 master 上的 redis cluster 介绍 自动将数据进行分片，每个 master 上放一部分数据 提供内置的高可用支持，部分 master 不可用时，还是可以继续工作的 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是加 10000 的端口号，比如 16379 16379 端口号是用来进行节点间通信的，通过 cluster bus（集群总线）。cluster bus 的通信是用来进行故障检测，配置更新，故障转移授权 cluster bus 用了另外一种二进制的协议，主要用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间 最老土的 hash 算法和弊端（大量缓存重建） 的确它的最大弊端就是，增加或者减少节点的时候，基本上所有数据都要重建路由 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） 优点：自动缓存迁移 缺点：缓存热点问题 一致性 hash 的严重问题是缓存热点，关键字是 区间，因为它是一个环，顺时针找可用节点，所以只要节点够多，就能更均匀的均衡负载。 所以出现了虚拟节点，来解决这个缺点 如上图，假设只有 3 个物理节点，但是在这个环上，分布了若干个虚拟节点（最后指向的是物理节点） 对于数据落在 1-3 这个区间 无虚拟节点：顺时针向右，全部导向了节点 3 有虚拟节点：顺时针向右，被多个虚拟节点分割，可能会遇上节点 1、2、3 。这样就负载均衡了 redis cluster 的 hash slot 算法 redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot hash slot 让 node 的增加和移除很简单： 增加一个 master，就将其他 master 的 hash slot 移动部分过去 减少一个 master，就将它的 hash slot 移动到其他 master 上去 移动 hash slot 的成本是非常低的 客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现 如上图，思路与一致性 hash 是一样的。通过更过的 hash slot，将路由分布得更均匀。 当一台机器挂掉之后，会在极短的时间内，将挂掉的 hash slot 分配给其他两个物理节点 可以看成是 -> hash slot -> 机器，hash slot 数量固定，不一一对应机器，动态分配的。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/028.html":{"url":"redis/028.html","title":"028. 在项目中重新搭建一套读写分离+高可用+多 master 的 redis cluster 集群","keywords":"","body":" 028. 在项目中重新搭建一套读写分离+高可用+多 master 的 redis cluster 集群 redis cluster 的重要配置 在三台机器上启动 6 个 redis 实例 创建集群 读写分离+高可用+多 master Ruby 的安装 Ruby 安装 rubygems 安装 028. 在项目中重新搭建一套读写分离+高可用+多 master 的 redis cluster 集群 上一章节对 redis cluster 最最基础的一些知识进行了了解。 redis cluster: master+slave 复制和读写分离/高可用和主备切换 都是自动支持的，支持多个 master 的 hash slot 支持数据分布式存储 所以停止之前所有的实例，包括 redis 主从和哨兵集群 redis cluster 的重要配置 cluster-enabled cluster-config-file 这是指定一个文件，供 cluster 模式下的 redis 实例将集群状态保存在那里，包括集群中其他机器的信息，比如节点的上线和下限，故障转移，不是我们去维护的，给它指定一个文件，让 redis 自己去维护的 cluster-node-timeout 节点存活超时时长，超过一定时长，认为节点宕机，master 宕机的话就会触发主备切换，slave 宕机就不会提供服务 在三台机器上启动 6 个 redis 实例 redis cluster 集群要求至少 3 个 master，去组成一个高可用，健壮的分布式的集群，每个 master 都建议至少给一个 slave，3 个 master，3 个 slave，最少的要求 正式环境下，建议都是说在 6 台机器上去搭建，至少 3 台机器，保证每个 master 都跟自己的 slave 不在同一台机器上，如果是 6 台自然更好 3 台机器去搭建 6 个 redis 实例的 redis cluster hostname 配置文件 eshop-cache01 7001.conf、 7002.conf eshop-cache02 7003.conf、 7004.conf eshop-cache03 7005.conf、 7006.conf ::: tip redis 还是一样的，只是配置文件中的属性不同了，建议使用默认的配置文件重新配置 ::: 配置目录整体概览 |- /etc |- /redis # 存放配置文件，按端口名命名 |- 7001.conf |- /redis-cluster # 存放 redis 集群维护的配置文件目录 |- /init.d |-/redis_7001 # 启动脚本 |- /var |-/log/redis # 存放日志文件 |-/7001.log # 日志文件 |-/redis/7001 # 存放数据文件 配置文件和启动脚本 对于配置文件 主要要修改的在下面了 port 7001 cluster-enabled yes cluster-config-file /etc/redis-cluster/node-7001.conf cluster-node-timeout 15000 daemonize yes pidfile /var/run/redis_7001.pid dir /var/redis/7001 logfile /var/log/redis/7001.log # 如果后续不能正常创建集群，请把这里修改成 ip 的形式 bind eshop-cache01 appendonly yes 为了方便快捷的把配置文件搞好。先执行以下命令 mkdir -p /etc/redis-cluster mkdir -p /var/log/redis mkdir -p /var/redis/7001 # 复制默认配置文件，并修改 cp /usr/local/redis-3.2.8/redis.conf /etc/redis/7001.conf # 复制启动脚本，需要 6 个启动脚本 ，里面的 REDISPORT=7001 需要修改成和文件名对应的端口 cp /usr/local/redis-3.2.8/utils/redis_init_script /etc/init.d/redis_7001 然后下载 7001.conf 到 windows 中，按以上配置文件修改完成后， 复制其余 5 份出来，只要全局把 7001 替换成 其余 5 分节点的端口和 bind 修改好就可以了 启动 6 个 redis 实例 # 通过之前一样的命令启动 6 个实例 ./redis_7001 start # 查看日志是否启动正常，现在配置了日志文件的 cat /var/log/redis/7001.log 创建集群 之前启动的 6 个redis 实例只是单机的，需要使用工具创建集群 通过官网提供的工具 redis-trib.rb 工具创建，该工具依赖了 ruby，先装依赖 # 不要直接使用此命令安装，因为安装的是低版本的。参考本文后面的 Ruby 的安装 yum install -y ruby # 不要使用 yum 安装，在 centos 6 中只会安装 1.8 版本的，并且连 ruby 也一起安装的 yum install -y rubygems # 前面的安装请参考 本文后面的 Ruby 安装 gem install redis ## 让 redis-trib.rb 工具不在具体目录下也可以使用，和 windows path 效果类似 cp /usr/local/redis-3.2.8/src/redis-trib.rb /usr/local/bin # 创建集群，注意这里需要使用 ip 地址，否则不成功 # --replicas: 每个 master 有几个 slave # 6 台机器，3 个 master，3 个 slave，尽量自己让 master 和 slave 不在一台机器上 redis-trib.rb create --replicas 1 192.168.99.170:7001 192.168.99.170:7002 192.168.99.171:7003 192.168.99.171:7004 192.168.99.172:7005 192.168.99.172:7006 安装的话，不需要每台机器都安装，在 01 上安装和创建集群即可 # 在输入 yes 后，会如果报错。 Can I set the above configuration? (type 'yes' to accept): yes /opt/rubies/ruby-2.3.8/lib/ruby/gems/2.3.0/gems/redis-4.1.0/lib/redis/client.rb:124:in `call': ERR Slot 0 is already busy (Redis::CommandError) 原因是 slot 被占用，解决方案如下，登录各个节点执行 cluster reset [root@eshop-cache02 redis]# redis-cli -h 192.168.99.171 -p 7003 192.168.99.171:7003> cluster reset OK 192.168.99.171:7003> exit [root@eshop-cache02 redis]# redis-cli -h 192.168.99.171 -p 7004 192.168.99.171:7004> cluster reset OK 192.168.99.171:7004> exit [root@eshop-cache02 redis]# redis-cli -h 192.168.99.170 -p 7001 192.168.99.170:7001> cluster reset OK 192.168.99.170:7001> exit [root@eshop-cache02 redis]# redis-cli -h 192.168.99.170 -p 7002 192.168.99.170:7002> cluster reset OK 192.168.99.170:7002> exit [root@eshop-cache02 redis]# redis-cli -h 192.168.99.172 -p 7005 192.168.99.172:7005> cluster reset OK 192.168.99.172:7005> exit [root@eshop-cache02 redis]# redis-cli -h 192.168.99.172 -p 7006 192.168.99.172:7006> cluster reset OK 192.168.99.172:7006> exit 再次创建集群后，终于成功了。从安装 ruby 开始到现在已经过去了 4 个小时， 真是太坎坷了 可以看到如下的日志，它选择了 3 个master，3 个 slave 并且都不在一台机器上 [root@eshop-cache01 redis]# redis-trib.rb create --replicas 1 192.168.99.170:7001 192.168.99.170:7002 192.168.99.171:7003 192.168.99.171:7004 192.168.99.172:7005 192.168.99.172:7006 >>> Creating cluster >>> Performing hash slots allocation on 6 nodes... Using 3 masters: 192.168.99.170:7001 192.168.99.171:7003 192.168.99.172:7005 Adding replica 192.168.99.171:7004 to 192.168.99.170:7001 Adding replica 192.168.99.170:7002 to 192.168.99.171:7003 Adding replica 192.168.99.172:7006 to 192.168.99.172:7005 M: 3807711e01cd28509d7ba9839e601058bf2a30cf 192.168.99.170:7001 slots:0-5460 (5461 slots) master S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master S: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 replicates 3807711e01cd28509d7ba9839e601058bf2a30cf M: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots:10923-16383 (5461 slots) master S: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 replicates cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf Can I set the above configuration? (type 'yes' to accept): yes >>> Nodes configuration updated >>> Assign a different config epoch to each node >>> Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join... >>> Performing Cluster Check (using node 192.168.99.170:7001) M: 3807711e01cd28509d7ba9839e601058bf2a30cf 192.168.99.170:7001 slots:0-5460 (5461 slots) master 1 additional replica(s) S: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots: (0 slots) slave replicates cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf M: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots: (0 slots) slave replicates 3807711e01cd28509d7ba9839e601058bf2a30cf M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 可以通过 check 命令检查 [root@eshop-cache01 redis]# redis-trib.rb check eshop-cache01:7001 >>> Performing Cluster Check (using node eshop-cache01:7001) M: 3807711e01cd28509d7ba9839e601058bf2a30cf eshop-cache01:7001 slots:0-5460 (5461 slots) master 1 additional replica(s) S: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots: (0 slots) slave replicates cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf M: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots: (0 slots) slave replicates 3807711e01cd28509d7ba9839e601058bf2a30cf M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 读写分离+高可用+多 master 集群安装成功之后，就具有了如下功能： 读写分离：每个 master 都有一个 slave 高可用：master 宕机，slave 自动被切换过去 多 master：横向扩容支持更大数据量 Ruby 的安装 Ruby 安装 在 ruby 官网找到一个安装任意版本的工具 ruby-install cd /ust/local # 安装 ruby-install wget -O ruby-install-0.7.0.tar.gz https://github.com/postmodern/ruby-install/archive/v0.7.0.tar.gz tar -xzvf ruby-install-0.7.0.tar.gz cd ruby-install-0.7.0/ sudo make install # 安装 2.3 版本 ruby，执行后会看到 update runby.. 应该是吧之前 1.8 版本的更新了 # 要等待好长时间，在安装过程中的时候 32% [305/947] io.c ，947 的进度条。。 ruby-install ruby 2.3 # 如果你之前通过 yum 安装过 低版本的 ruby，那么需要移除掉 yum remove ruby # 移除之后，发现提示有误 [root@eshop-cache01 ruby-2.3.8]# ruby -v -bash: /usr/bin/ruby: No such file or directory # 其实在 ruby-install ruby 2.3 安装完成过程中是有显示安装位置的 # 通过 ls 命令 连接到安装目录的 ruby 文件即可 ln -s /usr/local/src/ruby-2.3.8/ruby /usr/bin/ruby 以下尝试过好长时间都不能安装上，主要是 rvm 安装不了 解决 gem install redis redis requires Ruby version >= 2.2.2.报错问题 或者参考 这篇文章 # 安装 curl sudo yum install curl # 安装 RVM # curl -L get.rvm.io | bash -s stable # curl -L https://get.rvm.io | bash -s stable gpg --keyserver hkp://keys.gnupg.net --recv-keys 409B6B1796C275462A1703113804BB82D39DC0E3 7D2BAF1CF37B13E2069D6956105BD0E739499BDB # 我再执行这步骤的时候老是报错 curl returned status '35'. ，下载不下来 # 按照官网的 https://rvm.io/ 方式也是报错 curl -sSL https://get.rvm.io | bash -s stable source /usr/local/rvm/scripts/rvm # 查看 rvm 库中已知的 ruby 版本 rvm list known # 安装一个 ruby 版本 rvm install 2.3.3 # 使用一个 ruby 版本 rvm use 2.3.3 # 卸载一个已知版本 rvm remove 2.0.0 ruby --version # 再安装redis就可以了 gem install redis rubygems 安装 安装方式请参考官网：https://rubygems.org/pages/download cd /usr/local wget https://rubygems.org/rubygems/rubygems-3.0.3.zip # 如果没有安装 zip，请执行 yum install unzip unzip rubygems-3.0.3.zip cd rubygems-3.0.3 ruby setup.rb ln -s /usr/local/rubygems-3.0.3/bin/gem /usr/bin/gem 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/029.html":{"url":"redis/029.html","title":"029. 对项目的 redis cluster 实验多 master 写入、读写分离、高可用性","keywords":"","body":" 029. 对项目的 redis cluster 实验多 master 写入、读写分离、高可用性 实验多 master 写入 - 海量数据的分布式存储 实验不同 master 各自的 slave 读取 - 读写分离 实验自动故障切换 - 高可用性 029. 对项目的 redis cluster 实验多 master 写入、读写分离、高可用性 上一章节 将 redis cluster 搭建起来了 redis cluster 提供了多个 master，数据可以分布式存储在多个 master 上; 每个 master 都带着 slave，自动就做读写分离; 每个 master 如果故障，那么就会自动将 slave 切换成 master，高可用 redis cluster 的基本功能，来测试一下 实验多 master 写入 -> 海量数据的分布式存储 [root@eshop-cache01 ~]# redis-cli -h eshop-cache01 -p 7001 eshop-cache01:7001> set mykey1 v1 OK eshop-cache01:7001> set mykey2 v2 (error) MOVED 14119 192.168.99.172:7005 eshop-cache01:7001> 先登录其中一台机器，然后写入数据，会发现 mykey2 error 了，这是告诉你需要到 172 上面去执行写入。 [root@eshop-cache02 ~]# redis-cli -h 192.168.99.172 -p 7005 192.168.99.172:7005> set mykey2 v2 OK 192.168.99.172:7005> get mykey2 \"v2\" 192.168.99.172:7005> get mykey1 (error) MOVED 1860 192.168.99.170:7001 192.168.99.172:7005> 在提示的上面执行写入数据，能看到成功了，但是读取 myke1 又提示需要到他对应的 master 上去读取 原因是：每个 master 都会计算这个 key 对应的 CRC16 值，然后对 16384个 hashslot 取模，找到 key 对应的 hashslot，找到 hashslot 对应的 master 如果对应的 master 就在自己本地的话，自己就处理掉了 但是如果计算出来的 hashslot 在其他 master 上，那么就会给客户端返回一个 moved error，告诉你，你得到哪个 master 上去执行这条写入的命令 什么叫做多 master 的写入？就是每条数据只能存在于一个 master 上，不同的 master 负责存储不同的数据，这个就是分布式的数据存储 比如：100w 条数据，5 个 master，每个 master 就负责存储 20w 条数据，分布式数据存储 以下内容是讲师独白 + 扩展知识角度 我除了大型的 java 系统架构，还专注在大数据系统架构、分布式、分布式存储 hadoop hdfs、分布式资源调度 hadoop yarn、分布式计算 hadoop mapreduce/hive、分布式的 nosql 数据库 hbase、分布式的协调 zookeeper、分布式通用计算引擎 spark、分布式的实时计算引擎 storm 这些方面。讲解的角度不一样 如果你要处理海量数据，就涉及到了一个名词，叫做大数据，只要涉及到大数据，那么其实就会涉及到分布式 如 redis cluster 就是分布式的 因为我来讲 java 系统的架构，有时候跟其他人不一样，不是纯搞 java，但是我因为工作时间很长，早期专注做 java 架构好多年，大数据兴起，就一直专注大数据系统架构 大数据相关的系统，也涉及很多的 java 系统架构，高并发、高可用、高性能、可扩展、分布式系统等很复杂的架构 所以会给大家稍微拓展一下知识面，从不同的角度去讲解一块知识 redis 高并发、高性能、每日上亿流量的大型电商网站的商品详情页系统的缓存架构，面对这个需求来讲解的，redis 是作为大规模缓存架构中的底层的核心存储的支持 高并发、高性能、每日上亿流量，对 redis 深入讲解一些原理： redis 持久化 -> 灾难的时候，做数据恢复， 复制 -> 读写分离，扩容 slave，支撑更高的读吞吐，redis 怎么支撑读 QPS 超过 10 万，几十万; 哨兵，在 redis 主从，一主多从，怎么保证 99.99% 可用性; redis cluster，海量数据 java 架构课，架构思路和设计是很重要的，但是另外一点，我希望能够带着大家用真正 java 架构师的角度去看待一些技术，而不是仅仅停留在技术的一些细节的点 给大家从一些大数据的角度，去分析一下我们 java 架构领域中的一些技术 天下武功，都出自一脉，研究过各种大数据的系统，比如 redis cluster 讲解了很多原理，它跟 elasticsearch 很多底层的分布式原理，都是类似的 如：redis AOF，fsync elasticsearch 建立索引的时候，先写内存缓存，每秒钟把数据刷入 os cache，接下来再每隔一定时间 fsync 到磁盘上去 redis cluster，写可以到任意 master，任意 master 计算 key 的 hashslot 以后，告诉 client，重定向，路由到其他 mater 去执行，分布式存储的一个经典的做法 elasticsearch 建立索引的时候，也会根据 doc id/routing value，做路由，路由到某个其他节点，重定向到其他节点去执行 分布式的一些系统，如 hadoop，spark，storm 里面很多核心的思想都是类似的 实验不同 master 各自的 slave 读取 -> 读写分离 [root@eshop-cache01 ~]# redis-cli -h 192.168.99.171 -p 7004 192.168.99.171:7004> get mykey2 (error) MOVED 14119 192.168.99.172:7005 192.168.99.171:7004> get mykey1 (error) MOVED 1860 192.168.99.170:7001 192.168.99.171:7004> readonly OK 192.168.99.171:7004> get mykey2 (error) MOVED 14119 192.168.99.172:7005 192.168.99.171:7004> get mykey1 \"v1\" 可以看到在 salve 节点上登录之后，一开始获取数据会报错，但是带上 readonly 指令后，再次获取只要是对应 master 的 salve 就能获取到数据 注意：check 命令查看到的集群信息，一个 M 一个 S，不要以为这个顺序就是互相对应的。 [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7001 >>> Performing Cluster Check (using node eshop-cache01:7001) M: 3807711e01cd28509d7ba9839e601058bf2a30cf eshop-cache01:7001 slots:0-5460 (5461 slots) master 1 additional replica(s) S: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots: (0 slots) slave replicates cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf M: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots: (0 slots) slave replicates 3807711e01cd28509d7ba9839e601058bf2a30cf M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 可以通过命令 info 命令查看，可以看到 7004 才是 7001 的从节点 192.168.99.171:7004> info .... # Replication role:slave master_host:192.168.99.170 master_port:7001 后面，马上把 redis 架构给讲完之后，就开始讲解业务系统的开发，包括高并发的商品详情页系统的大型的缓存架构，jedis cluster 相关 api 去封装和测试对 redis cluster 的访问 jedis cluster api，就可以自动针对多个 master 进行写入和读取 redis-cli -c 启动，就会自动进行各种底层的重定向的操作 [root@eshop-cache01 ~]# redis-cli -h 192.168.99.171 -p 7004 -c 192.168.99.171:7004> get mykey1 -> Redirected to slot [1860] located at 192.168.99.170:7001 \"v1\" 192.168.99.170:7001> get mykey2 -> Redirected to slot [14119] located at 192.168.99.172:7005 \"v2\" 实验 redis cluster 的读写分离的时候，会发现有一定的限制性，默认情况下 redis cluster 的核心的理念，主要是用 slave 做高可用的，每个 master 挂一两个 slave，主要是做数据的热备，还有 master 故障时的主备切换，实现高可用的 ::: tip redis cluster 默认是不支持 slave 节点读或者写的，跟我们手动基于 replication 搭建的主从架构不一样的 ::: slave node 上 使用 readonly，get，这个时候才能在 slave node 进行读取 redis cluster 主从架构是出来，读写分离，复杂了点也可以做，但是 jedis 客户端，对 redis cluster 的读写分离支持不太好的 默认的话就是读和写都到 master 上去执行的 如果你要让最流行的 jedis 做 redis cluster 的读写分离的访问，那可能还得自己修改一点 jedis 的源码，成本比较高 核心的思路，在 redis cluster 中，就没有所谓的读写分离的概念了 读写分离是为了什么？主要是因为要建立一主多从的架构，才能横向任意扩展 slave node 去支撑更大的读吞吐量 redis cluster 的架构下，实际上本身 master 就是可以任意扩展的，你如果要支撑更大的读吞吐量，或者写吞吐量，或者数据量，都可以直接对 master 进行横向扩展就可以了，也可以实现支撑更高的读吞吐的效果 这个结论不会去跟大家直接讲解的，很多东西都要带着一些疑问，未知，实际经过一些实验和操作之后，让你体会的更加深刻一些 画外音：这个我很赞同，现在我是终于明白了为什么说 redis cluster 读写分离，但是又不是 replication 读写分离，他们的区别很深刻了 redis cluster 主从架构、读写分离，没说错没有撒谎，但是支持不太好，在 server 层面、jedis client 层面也不太好。 所以说扩容 master，跟之前扩容 slave，效果是一样的 实验自动故障切换 -> 高可用性 # 查看哪一个是 master [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7001 >>> Performing Cluster Check (using node eshop-cache01:7001) M: 3807711e01cd28509d7ba9839e601058bf2a30cf eshop-cache01:7001 slots:0-5460 (5461 slots) master 1 additional replica(s) S: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots: (0 slots) slave replicates cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf M: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots: (0 slots) slave replicates 3807711e01cd28509d7ba9839e601058bf2a30cf M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. # 我这里 7001 是 master # kill 掉它 [root@eshop-cache01 ~]# ps -ef | grep redis root 27419 1 0 09:25 ? 00:00:31 /usr/local/bin/redis-server 192.168.99.170:7001 [cluster] root 27424 1 0 09:25 ? 00:00:31 /usr/local/bin/redis-server 192.168.99.170:7002 [cluster] root 27505 27464 0 11:00 pts/2 00:00:00 grep redis [root@eshop-cache01 ~]# kill 27419 再次运行 check 的时候就会发现少了一个节点，然而 master 节点却还是 3 个。 7004 变成 master 了 [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7002 >>> Performing Cluster Check (using node eshop-cache01:7002) S: b0d66be3b15e117696c50a781ff24a842456733d eshop-cache01:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 M: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots:0-5460 (5461 slots) master 0 additional replica(s) M: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s) S: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots: (0 slots) slave replicates cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 这个时候连接到 7004 上去读写都能正常使用，且能获取到 7001 上写入的数据 然后重启 7001，再次查看他的状态，发现 7001 被自动转为 slave 了 [root@eshop-cache01 ~]# /etc/init.d/redis_7001 start Starting Redis server... [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7001 >>> Performing Cluster Check (using node eshop-cache01:7001) S: 3807711e01cd28509d7ba9839e601058bf2a30cf eshop-cache01:7001 slots: (0 slots) slave replicates a174fe6613862db8985f82caac58bde91dfbd664 S: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots: (0 slots) slave replicates cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf M: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots:0-5460 (5461 slots) master 1 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 M: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 通过登录 7004 查看信息也是一样的 [root@eshop-cache01 ~]# redis-cli -h 192.168.99.171 -p 7004 192.168.99.171:7004> info replication # Replication role:master connected_slaves:1 slave0:ip=192.168.99.170,port=7001,state=online,offset=421,lag=1 master_repl_offset:421 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:420 在这里发现，之前使用 info 命令返回了一大堆信息，但是其中就包含了 replication，原来这个 info 命令后面可以跟一个具体的项，就只返回该了 总的感觉上来说，redis cluster 还是很强大。这个思路对于缓存来说比数据库考虑得要少，但是针对缓存场景来说，的确是很赞的一种设计 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/030.html":{"url":"redis/030.html","title":"030. redis cluster 通过 master 水平扩容来支撑更高的读写吞吐 + 海量数据","keywords":"","body":" 030. redis cluster 通过 master 水平扩容来支撑更高的读写吞吐 + 海量数据 加入新的 master reshard 一些数据到新添加的 master 上 添加 node 作为 slave 删除 node 030. redis cluster 通过 master 水平扩容来支撑更高的读写吞吐 + 海量数据 redis cluster 模式下，不建议做物理的读写分离了，我们建议通过 master 的水平扩容，来横向扩展读写吞吐量，还有支撑更多的海量数据 比如 redis 单机假设读吞吐是 5w/s，写吞吐 2w/s，如果有 5 台 master，读吞吐可以达到总量 25w/s QPS，写可以达到 10w/s QPS redis 单机内存不建议过大（建议值 6G、8G），因为 fork 类操作的时候很耗时，会导致请求延时的问题 只要横向扩容更多的 master 就能达到支撑 1TB 数据量。 加入新的 master 暂时不添加机器了（比较耗时，主要是为了演示这个功能），在 eshop-cache03 上增加一个 7007 的 redis 节点，然后把该节点加入到 redis cluster 中 搭建方式和配置文件的修改参考 # 添加节点到集群 [root@eshop-cache01 ~]# redis-trib.rb add-node 192.168.99.172:7007 192.168.99.170:7001 >>> Adding node 192.168.99.172:7007 to cluster 192.168.99.170:7001 >>> Performing Cluster Check (using node 192.168.99.170:7001) S: 3807711e01cd28509d7ba9839e601058bf2a30cf 192.168.99.170:7001 slots: (0 slots) slave replicates a174fe6613862db8985f82caac58bde91dfbd664 M: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots:0-5460 (5461 slots) master 1 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 S: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots: (0 slots) slave replicates 9247dfb394441619da9da5b75b62b034c3f420e5 M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. >>> Send CLUSTER MEET to node 192.168.99.172:7007 to make it join the cluster. # 检查集群信息 [root@eshop-cache01 ~]# redis-trib.rb check 192.168.99.170:7001 >>> Performing Cluster Check (using node 192.168.99.170:7001) S: 3807711e01cd28509d7ba9839e601058bf2a30cf 192.168.99.170:7001 slots: (0 slots) slave replicates a174fe6613862db8985f82caac58bde91dfbd664 M: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots:10923-16383 (5461 slots) master 1 additional replica(s) M: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots:0-5460 (5461 slots) master 1 additional replica(s) M: 5d236f5ff996f0d378914b01159a8b7c5dee383d 192.168.99.172:7007 slots: (0 slots) master 0 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 S: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots: (0 slots) slave replicates 9247dfb394441619da9da5b75b62b034c3f420e5 M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:5461-10922 (5462 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 可以看到 7007 变成了一个 master，但是只有 0 个 slots ，表示此事的 7007 还不能对外服务，需要手动把执行迁移命令 reshard 一些数据到新添加的 master 上 注：由于第一次迁移的时候输入完 done 后，就被刷屏了，记录没有保存下来，我把分配的 4096 个 slots 迁移到了 7006 上，这里再分配一次。记录下命令的反应（集群创建的时候，它会均匀分配 slots 到每个机器上的） # 任意连接一台集群内的集群即可 [root@eshop-cache01 ~]# redis-trib.rb reshard 192.168.99.170:7001 >>> Performing Cluster Check (using node 192.168.99.170:7001) S: 3807711e01cd28509d7ba9839e601058bf2a30cf 192.168.99.170:7001 slots: (0 slots) slave replicates a174fe6613862db8985f82caac58bde91dfbd664 M: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots:0-1364,5461-6826,10923-16383 (8192 slots) master 1 additional replica(s) M: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots:1365-5460 (4096 slots) master 1 additional replica(s) M: 5d236f5ff996f0d378914b01159a8b7c5dee383d 192.168.99.172:7007 slots: (0 slots) master 0 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 S: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots: (0 slots) slave replicates 9247dfb394441619da9da5b75b62b034c3f420e5 M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:6827-10922 (4096 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. # 询问你要迁移多少个 slots 到目标机器去? 一共是 16384/4 = 4096 How many slots do you want to move (from 1 to 16384)? 4096 # 要迁移到的目标机器 id，这个id就在上面的 M/S 后面这一串 What is the receiving node ID? 5d236f5ff996f0d378914b01159a8b7c5dee383d Please enter all the source node IDs. Type 'all' to use all the nodes as source nodes for the hash slots. Type 'done' once you entered all the source nodes IDs. # 输入被迁移的 id，这里是要把之前的 3 台上的 node 均匀的迁移 4096 到 新机器上 Source node #1:9247dfb394441619da9da5b75b62b034c3f420e5 Source node #2:a174fe6613862db8985f82caac58bde91dfbd664 Source node #3:0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 # 需要输入 done 命令完成输入源 Source node #4:done ... 接下来会一大串的日志信息， Moving slot 7850 from 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 # 是否要继续执行集群计划 Do you want to proceed with the proposed reshard plan (yes/no)? yes 又是一大堆日志刷屏，完成后 报错问题 ::: tip 难道他们不是均匀分配的吗？对于下面这种问题，报错后，我没有回复成功， 只能把所有的 /etc/redis/ 和 /etc/redis-cluster/ 下的 conf 文件删除后， 并且重启所有的 redis 后，登录上节点手动执行 flushall 和 cluster reset 后， 才能重新创建集群，重新回复到现在这个章节的过程中来的 这里我怀疑只要把所有节点都执行 flushall 和 cluster reset 就可以吧集群打散。再重建 ::: 报错问题：在下面这种 slots 分布下，7006 有 8192 个节点，7007 0 个， 本来以为 redis 会自动迁移到 7007 上面去，指定了 3 个源节点，结果执行过程中报错了 S: 3807711e01cd28509d7ba9839e601058bf2a30cf 192.168.99.170:7001 slots: (0 slots) slave replicates a174fe6613862db8985f82caac58bde91dfbd664 M: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots:0-1364,5461-6826,10923-16383 (8192 slots) master 1 additional replica(s) M: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots:1365-5460 (4096 slots) master 1 additional replica(s) M: 5d236f5ff996f0d378914b01159a8b7c5dee383d 192.168.99.172:7007 slots: (0 slots) master 0 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 S: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots: (0 slots) slave replicates 9247dfb394441619da9da5b75b62b034c3f420e5 M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:6827-10922 (4096 slots) master [ERR] Calling MIGRATE: ERR Syntax error, try CLIENT (LIST | KILL | GETNAME | SETNAME | PAUSE | REPLY) 再次尝试迁移，发现还是报错 [root@eshop-cache01 ~]# redis-trib.rb reshard 192.168.99.170:7001 >>> Performing Cluster Check (using node 192.168.99.170:7001) S: 3807711e01cd28509d7ba9839e601058bf2a30cf 192.168.99.170:7001 slots: (0 slots) slave replicates a174fe6613862db8985f82caac58bde91dfbd664 M: 9247dfb394441619da9da5b75b62b034c3f420e5 192.168.99.172:7006 slots:6144-6826,10923-16383 (6144 slots) master 1 additional replica(s) M: a174fe6613862db8985f82caac58bde91dfbd664 192.168.99.171:7004 slots:1860-5460 (3601 slots) master 1 additional replica(s) M: 5d236f5ff996f0d378914b01159a8b7c5dee383d 192.168.99.172:7007 slots:0-1859,5461-6143 (2543 slots) master 0 additional replica(s) S: b0d66be3b15e117696c50a781ff24a842456733d 192.168.99.170:7002 slots: (0 slots) slave replicates 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 S: cb2256b653cf5b7b3f9d1478cfa2953cc334c5bf 192.168.99.172:7005 slots: (0 slots) slave replicates 9247dfb394441619da9da5b75b62b034c3f420e5 M: 0ff0e8ab05a8b032aeacf24e0c7fea77be3f5c55 192.168.99.171:7003 slots:6827-10922 (4096 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... [WARNING] Node 192.168.99.171:7004 has slots in migrating state (1860). [WARNING] Node 192.168.99.172:7007 has slots in importing state (1860). [WARNING] The following slots are open: 1860 >>> Check slots coverage... [OK] All 16384 slots covered. *** Please fix your cluster problems before resharding 手动 kill 掉 7004 和 7007 后再次重启，发现还是有问题 [ERR] Nodes don't agree about configuration! >>> Check for open slots... [WARNING] Node 192.168.99.172:7007 has slots in importing state (1860). [WARNING] The following slots are open: 1860 这个时候百度到了，使用 fix 来修复，但是没有成功，由于我把 /etc/redis-cluster/node-7007.conf 内容清空了。 redis-trib.rb fix 192.168.99.172:7007 添加 node 作为 slave 在 eshop-cache03 上增加一个 7008 的 redis 节点 # redis-trib.rb add-node --slave --master-id 要挂载的 masterid ，被挂载的 slave ，随便给一个集群节点即可 # 我这里是吧 7008 挂载到了 7001 上 redis-trib.rb add-node --slave --master-id b49a4562f0164bbf9c8fc9c6059e09e420ee7abf 192.168.99.172:7008 192.168.99.170:7001 删除 node 先用 resharding 将数据都移除到其他节点，确保 node 为空之后，才能执行 remove 操作 这里删除 7007 这个，现在集群中 4 个 master 都是 4096 个 slots 需要手动计算把 4096 平均分配给另外 3 个 master ，2 个分配 1365，1个分配 1366 # 手动执行 3 次，把 7007 上的 slots 分布迁移到其他 三个 master 上 redis-trib.rb reshard 192.168.99.170:7001 # 再删除 node-id，也就是协商 7007 的 id redis-trib.rb del-node 192.168.99.170:7001 当你清空了一个 master 的 hashslot 时，redis cluster 就会自动将其 slave 挂载到其他 master 上去，这个时候就只要删除掉 master 就可以了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/031.html":{"url":"redis/031.html","title":"031. redis cluster 的自动化 slave 迁移实现更强的高可用架构的部署方案","keywords":"","body":"031. redis cluster 的自动化 slave 迁移实现更强的高可用架构的部署方案 slave 的自动迁移：比如现在有 10 个 master，每个有 1 个 slave，新增了 3 个 slave 作为冗余，有的 master 就有 2 个 slave 了（出现了salve冗余），其他的 master 还是只有 1 个 slave 如果某个 master 的 slave 挂了，那么 redis cluster 会自动迁移一个冗余的 slave 给那个 master 这样的好处：如果你每个 master 只有一个 slave，万一说一个 slave 死了，然后很快，master也死了，那可用性还是降低了，增强了高可用性 测试 # 查看集群中只有一个的 slave [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7001 >>> Performing Cluster Check (using node eshop-cache01:7001) # 7001 也只有一个 M: b49a4562f0164bbf9c8fc9c6059e09e420ee7abf eshop-cache01:7001 slots:0-5460 (5461 slots) master 1 additional replica(s) M: 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 192.168.99.172:7005 slots:5461-6825,12288-16383 (5461 slots) master 1 additional replica(s) S: 728e473d6e5e36ddb051c600c7708f23733c46f7 192.168.99.172:7008 slots: (0 slots) slave replicates b49a4562f0164bbf9c8fc9c6059e09e420ee7abf S: 4051750a5828342877f31679189697741683e3c4 192.168.99.172:7006 slots: (0 slots) slave replicates 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 S: 50545f07b14843655452a08420316d11ecd12743 192.168.99.171:7004 slots: (0 slots) slave replicates 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc S: 45db2a3a0c71a628bde2bb8d6f5a9ebd4489e387 192.168.99.170:7002 slots: (0 slots) slave replicates 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc # 7003 有两个 M: 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc 192.168.99.171:7003 slots:6826-12287 (5462 slots) master 2 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. # 查看 7001 的 slave 是哪个节点 [root@eshop-cache01 ~]# redis-cli -h 192.168.99.170 -p 7001 192.168.99.170:7001> info replication # Replication role:master connected_slaves:1 # 可以看到 7008 是他的 slave，那么我们把 7008 的给干掉 slave0:ip=192.168.99.172,port=7008,state=online,offset=2745,lag=0 master_repl_offset:2745 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:2744 192.168.99.170:7001> exit kill 掉 7008 的 slave 节点 [root@eshop-cache03 ~]# ps -ef | grep redis root 1369 1 0 12:55 ? 00:00:23 /usr/local/bin/redis-server 192.168.99.172:7005 [cluster] root 1374 1 0 12:55 ? 00:00:08 /usr/local/bin/redis-server 192.168.99.172:7006 [cluster] root 1418 1 0 13:10 ? 00:00:06 /usr/local/bin/redis-server 192.168.99.172:7008 [cluster] root 1441 1427 0 13:39 pts/1 00:00:00 grep redis [root@eshop-cache03 ~]# kill 1418 kill 掉之后，再检查集群状态，可以看到一开始是连接不上 7008 的， 稍等一会 7003 原本有 2 个 slave，现在自动迁移成 7001 的 slave 了 [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7001 [ERR] Sorry, can't connect to node 192.168.99.172:7008 >>> Performing Cluster Check (using node eshop-cache01:7001) M: b49a4562f0164bbf9c8fc9c6059e09e420ee7abf eshop-cache01:7001 slots:0-5460 (5461 slots) master 0 additional replica(s) M: 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 192.168.99.172:7005 slots:5461-6825,12288-16383 (5461 slots) master 1 additional replica(s) S: 4051750a5828342877f31679189697741683e3c4 192.168.99.172:7006 slots: (0 slots) slave replicates 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 S: 50545f07b14843655452a08420316d11ecd12743 192.168.99.171:7004 slots: (0 slots) slave replicates 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc S: 45db2a3a0c71a628bde2bb8d6f5a9ebd4489e387 192.168.99.170:7002 slots: (0 slots) slave replicates 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc M: 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc 192.168.99.171:7003 slots:6826-12287 (5462 slots) master 2 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7001 >>> Performing Cluster Check (using node eshop-cache01:7001) M: b49a4562f0164bbf9c8fc9c6059e09e420ee7abf eshop-cache01:7001 slots:0-5460 (5461 slots) master 1 additional replica(s) M: 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 192.168.99.172:7005 slots:5461-6825,12288-16383 (5461 slots) master 1 additional replica(s) S: 4051750a5828342877f31679189697741683e3c4 192.168.99.172:7006 slots: (0 slots) slave replicates 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 S: 50545f07b14843655452a08420316d11ecd12743 192.168.99.171:7004 slots: (0 slots) slave replicates 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc S: 45db2a3a0c71a628bde2bb8d6f5a9ebd4489e387 192.168.99.170:7002 slots: (0 slots) slave replicates b49a4562f0164bbf9c8fc9c6059e09e420ee7abf M: 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc 192.168.99.171:7003 slots:6826-12287 (5462 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 恢复 7008 的节点，再次查看集群状态，可以发现 7001 变成了 2 个 slave 节点 [root@eshop-cache01 ~]# redis-trib.rb check eshop-cache01:7001 >>> Performing Cluster Check (using node eshop-cache01:7001) M: b49a4562f0164bbf9c8fc9c6059e09e420ee7abf eshop-cache01:7001 slots:0-5460 (5461 slots) master 2 additional replica(s) M: 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 192.168.99.172:7005 slots:5461-6825,12288-16383 (5461 slots) master 1 additional replica(s) S: 728e473d6e5e36ddb051c600c7708f23733c46f7 192.168.99.172:7008 slots: (0 slots) slave replicates b49a4562f0164bbf9c8fc9c6059e09e420ee7abf S: 4051750a5828342877f31679189697741683e3c4 192.168.99.172:7006 slots: (0 slots) slave replicates 536e275ff8e2130c8f3c298e16c4dbdc9f765e76 S: 50545f07b14843655452a08420316d11ecd12743 192.168.99.171:7004 slots: (0 slots) slave replicates 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc S: 45db2a3a0c71a628bde2bb8d6f5a9ebd4489e387 192.168.99.170:7002 slots: (0 slots) slave replicates b49a4562f0164bbf9c8fc9c6059e09e420ee7abf M: 0e6a3d40e63cb19f2606709a8b61ed4a6e511fcc 192.168.99.171:7003 slots:6826-12287 (5462 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"redis/032.html":{"url":"redis/032.html","title":"032. redis cluster 的核心原理分析：gossip 通信、jedis smart 定位、主备切换","keywords":"","body":" 032. redis cluster 的核心原理分析：gossip 通信、jedis smart 定位、主备切换 节点间的内部通信机制 基础通信原理 redis cluster 节点间采取 gossip 协议进行通信 10000 端口 交换的信息 gossip 协议 ping 消息深入 面向集群的 jedis 内部实现原理 基于重定向的客户端 请求重定向 计算 hash slot hash slot 查找 smart jedis 什么是 smart jedis JedisCluster 的工作原理 hashslot 迁移和 ask 重定向 高可用性与主备切换原理 与哨兵比较 032. redis cluster 的核心原理分析：gossip 通信、jedis smart 定位、主备切换 节点间的内部通信机制 基础通信原理 redis cluster 节点间采取 gossip 协议进行通信 gossip：互相之间不断通信，保持整个集群所有节点的数据是完整的 而集中式是将集群元数据（节点信息，故障，等等）集中存储在某个节点上； 经典的集中式中间件 zookeeper 他们基本上都用于维护集群的元数据 集中式： 优点：数据更新及时，时效好 元数据的更新和读取，时效性非常好，一旦元数据出现了变更，立即就更新到集中式的存储中，其他节点读取的时候立即就可以感知到; 缺点：数据更新压力集中 所有的元数据的跟新压力全部集中在一个地方，可能会导致元数据的存储有压力 gossip： 优点：数据更新压力分散 元数据的更新比较分散，不是集中在一个地方，更新请求会陆陆续续，打到所有节点上去更新，有一定的延时，降低了压力; 缺点：数据更新延迟 元数据更新有延时，可能导致集群的一些操作会有一些滞后 可见 集中式 与 gossip 的优缺点是相互的。 gossip 的延迟在我们上一章节中迁移 slots 时（reshard），去做另外一个操作，会发现 configuration error，需要等待一会才能达成一致，配置数据才能同步成功 10000 端口 每个节点都有一个专门用于节点间通信的端口，就是自己提供服务的端口号 + 10000，比如 7001，那么用于节点间通信的就是 17001 端口 每个节点每隔一段时间都会往另外几个节点发送 ping 消息，同时其他几点接收到 ping 之后返回 pong 交换的信息 交换的信息有：故障信息、节点的增加和移除、hash slot 信息，等等 gossip 协议 gossip 协议包含多种消息，包括 ping、pong、meet、fail，等等 meet: 某个节点发送 meet 给新加入的节点，让新节点加入集群中，然后新节点就会开始与其他节点进行通信 redis-trib.rb add-node 其实内部就是发送了一个 gossip meet 消息，给新加入的节点，通知那个节点去加入我们的集群 ping: 每个节点都会频繁给其他节点发送 ping，其中包含自己的状态还有自己维护的集群元数据，互相通过 ping 交换元数据 每个节点每秒都会频繁发送 ping 给其他的集群，ping，频繁的互相之间交换数据，互相进行元数据的更新 pong: 返回 ping 和 meet，包含自己的状态和其他信息，也可以用于信息广播和更新 fail: 某个节点判断另一个节点 fail 之后，就发送 fail 给其他节点，通知其他节点，指定的节点宕机了 ping 消息深入 ping 很频繁，而且要携带一些元数据，所以可能会加重网络负担 每个节点每秒会执行 10 次 ping，每次会选择 5 个最久没有通信的其他节点 当然如果发现某个节点通信延时达到了 cluster_node_timeout / 2，那么立即发送 ping，避免数据交换延时过长，落后的时间太长了 比如说，两个节点之间都 10 分钟没有交换数据了，那么整个集群处于严重的元数据不一致的情况，就会有问题 所以 cluster_node_timeout 可以调节，如果调节比较大，那么会降低发送的频率 每次 ping，一个是带上自己节点的信息，还有就是带上 1/10 其他节点的信息，发送出去，进行数据交换 至少包含 3 个其他节点的信息，最多包含总节点 -2 个其他节点的信息 面向集群的 jedis 内部实现原理 后面会使用 jedis，它是 redis 的 java client 客户端，支持 redis cluster 这里会讲解 jedis cluster api 与 redis cluster 集群交互的一些基本原理 基于重定向的客户端 redis-cli -c，可以提供自动重定的功能，那么对于 jedis 来说，下面是他的实现原理 请求重定向 客户端可能会挑选任意一个 redis 实例去发送命令，每个 redis 实例接收到命令，都会计算 key 对应的 hash slot 如果在本地就在本地处理，否则返回 moved 给客户端，让客户端进行重定向 cluster keyslot mykey，可以查看一个 key 对应的 hash slot 是什么 [root@eshop-cache01 ~]# redis-cli -h 192.168.99.170 -p 7001 192.168.99.170:7001> cluster keyslot myke1 (integer) 12435 192.168.99.170:7001> cluster keyslot myke2 (integer) 240 用 redis-cli 的时候，可以加入 -c 参数，支持自动的请求重定向，redis-cli 接收到 moved 之后，会自动重定向到对应的节点执行命令 但是这样会有一个问题，可能会出现大部分命令都会接受到 moved 响应，也就是说可能一次写入会有两次请求，这个就很浪费性能 计算 hash slot 计算 hash slot 的算法，就是根据 key 计算 CRC16 值，然后对 16384 取模，拿到对应的 hash slot 用 hash tag 可以手动指定 key 对应的 slot，同一个 hash tag 下的 key，都会在一个 hash slot 中，比如 set mykey1:{100} 和 set mykey2:{100} 192.168.99.170:7001> set mykey1:{100} 1 OK 192.168.99.170:7001> set mykey2:{100} 2 OK 192.168.99.170:7001> set mykey1 1 OK 192.168.99.170:7001> set mykey2 2 (error) MOVED 14119 192.168.99.172:7005 192.168.99.170:7001> get mykey2 (error) MOVED 14119 192.168.99.172:7005 192.168.99.170:7001> get mykey2:{100} \"2\" 可以看到，这个 tag 相当于你手动指定这个 key 路由到哪一个 solt 上去，那么只要手动了，以后查询也需要手动指定才行，所以这里需要先计算出 hash slot 的值，相当于在 redis 服务端的工作挪动到客户端来做了，这样减少了大量的 moved 请求 hash slot 查找 节点间通过 gossip 协议进行数据交换，就知道每个 hash slot 在哪个节点上 smart jedis 什么是 smart jedis 基于重定向的客户端，很消耗网络 IO，因为大部分情况下，可能都会出现一次请求重定向，才能找到正确的节点 所以大部分的客户端，比如 java redis 客户端（jedis），就是 smart 的 本地维护一份 hashslot -> node 的映射表，缓存起来，大部分情况下，直接走本地缓存就可以找到 hashslot -> node，不需要通过节点进行 moved 重定向 JedisCluster 的工作原理 在 JedisCluster 初始化的时候，就会随机选择一个 node，初始化 hashslot -> node 映射表，同时为每个节点创建一个 JedisPool 连接池 每次基于 JedisCluster 执行操作，首先 JedisCluster 都会在本地计算 key的 hashslot，然后在本地映射表找到对应的节点 如果那个 node 正好还是持有那个 hashslot，那么就 ok; 如果说进行了 reshard 这样的操作，可能 hashslot 已经不在那个 node 上了，就会返回 moved 如果 JedisCluter API 发现对应的节点返回 moved，那么利用该节点的元数据，更新本地的 hashslot -> node 映射表缓存 重复上面几个步骤，直到找到对应的节点，如果重试超过 5 次，那么就报错 JedisClusterMaxRedirectionException jedis 老版本，可能会出现在集群某个节点故障还没完成自动切换恢复时，频繁更新 hash slot，频繁 ping 节点检查活跃，导致大量网络 IO 开销 jedis 最新版本，对于这些过度的 hash slot 更新和 ping，都进行了优化，避免了类似问题 hashslot 迁移和 ask 重定向 如果 hash slot 正在迁移，那么会返回 ask 重定向给 jedis jedis 接收到 ask 重定向之后，会重新定位到目标节点去执行，但是因为 ask 发生在 hash slot 迁移过程中，所以 JedisCluster API 收到 ask 是不会更新 hashslot 本地缓存 已经可以确定 hashslot 已经迁移完了，访问会返回 moved， 那么是会更新本地 hashslot->node 映射表缓存的 高可用性与主备切换原理 redis cluster 的高可用的原理，几乎跟哨兵是类似的 判断节点宕机 如果一个节点认为另外一个节点宕机，那么就是 pfail，主观宕机 如果多个节点都认为另外一个节点宕机了，那么就是 fail，客观宕机，跟哨兵的原理几乎一样，sdown、odown 在 cluster-node-timeout 内，某个节点一直没有返回 pong，那么就被认为 pfail 如果一个节点认为某个节点 pfail 了，那么会在 gossip ping 消息中，ping 给其他节点，如果超过半数的节点都认为 pfail 了，那么就会变成 fail 从节点过滤 对宕机的 master node，从其所有的 slave node 中，选择一个切换成 master node 检查每个 slave node 与 master node 断开连接的时间，如果超过了 cluster-node-timeout * cluster-slave-validity-factor，那么就没有资格切换成 master 这个也是跟哨兵是一样的，从节点超时过滤的步骤 从节点选举 哨兵：对所有从节点进行排序，slave priority，offset，run id 每个从节点，都根据自己对 master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的从节点，选举时间越靠前，优先进行选举 所有的 master node 开始 slave 选举投票，给要进行选举的 slave 进行投票，如果大部分 master node（N/2 + 1）都投票给了某个从节点，那么选举通过，那个从节点可以切换成 master 从节点执行主备切换，从节点切换为主节点 与哨兵比较 整个流程跟哨兵相比，非常类似，所以说，redis cluster 功能强大，直接集成了 replication 和 sentinal 的功能 没有办法去给大家深入讲解 redis 底层的设计的细节，核心原理和设计的细节，那个除非单独开一门课，redis 底层原理深度剖析，redis 源码 对于咱们这个架构课来说，主要关注的是架构，不是底层的细节，对于架构来说，核心的原理的基本思路，是要梳理清晰的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/033.html":{"url":"redis/033.html","title":"033. redis 在实践中的一些常见问题以及优化思路（包含 linux 内核参数优化）","keywords":"","body":" 033. redis 在实践中的一些常见问题以及优化思路（包含 linux 内核参数优化） fork 耗时导致高并发请求延时 AOF 的阻塞问题 主从复制延迟问题 主从复制风暴问题 vm.overcommit_memory 查看 linux 内核版本 如果该命令不可用，可以去百度搜索不同的版本命令 033. redis 在实践中的一些常见问题以及优化思路（包含 linux 内核参数优化） 基本讲解到现在，大家其实直接到公司里，就可以去搭建 redis 了 其实有些东西，也许没有讲解到台细节的一些东西，比如一些参数的设置，这个是因为不同的公司，不同的业务，不同的数据量，可能要调节的参数不同 到这里为止，大家就差不多了，按照这个思路，去搭建 redis 支撑高并发、高可用、海量数据的架构，部署。可以用公司里的一些已有的数据，导入进去，几百万，一千万，导入进去，做各种压力测试（使用 redis-benchmark 就可以）、性能、并发、QPS，高可用的演练，每台机器最大能存储多少数据量，横向扩容支撑更多数据 基于测试环境还有测试数据，做各种演练，去摸索一些最适合自己的一些细节的东西 旁白 你说你靠一套课程，搞定一个技术 100% 的所有的东西，几乎是不可能的 师傅领进门，修行在个人 一套好的课程，唯一的判断标准，就是在这个价格下，能教会你值得这个价格的一些技术和架构等等知识，是你从其他地方没法学到的，或者自己去学要耗费几倍的时间摸索的 这个课程的价值就已经达到了 你说你花了几百块钱，买了个课程，要求，课程，学完，立即就是独孤九剑，直接到公司里各种问题都能轻松解决 这个世界上，不存在这种课程，所以你需要有一个合理的价值观，我们大家才能有一个非常好的良性的互动的过程 之前也左右一些在线课程，比如 spark 等等课程，就有人说为什么没有成为顶尖高手，这就很尴尬了 实际学了课程去做项目，100% 会遇到大量自己没想到的问题，遇到了首先就自己尝试去解决，遇到问题，才是你的经验积累 遇到了问题，加我的 QQ，然后跟我咨询咨询，我给你看看，也是可以的 比如之前的 spark，elasticsearch，java 架构课程，基本上 70%~80% 的问题，我都可以帮你搞定，当然是我能做到的，有些不在现场也许也搞不定，就需要你自己搞定了 fork 耗时导致高并发请求延时 RDB 和 AOF 的时候会存在 RDB 快照生成、AOF rewrite，耗费磁盘 IO 的过程 主进程 fork 子进程的时候，子进程是需要拷贝父进程的空间内存页表的，也是会耗费一定的时间的 一般来说，如果父进程内存有 1 个 G 的数据，那么 fork 可能会耗费在 20ms 左右，如果是 10G~30G，那么就会耗费 20 10，甚至 20 30，也就是几百毫秒的时间 info stats 中的 latest_fork_usec，可以看到最近一次 fork 的时长 redis 单机 QPS 一般在几万，fork 可能一下子就会拖慢几万条操作的请求时长，从几毫秒变成 1 秒 优化思路：fork 耗时跟 redis 主进程的内存有关系，一般控制 redis 的内存在 10GB 以内；否则 slave -> master 在全量复制等时候就可能会出现一些问题 AOF 的阻塞问题 redis 将数据写入 AOF 缓冲区，单独开一个线程做 fsync 操作，每秒一次 但是 redis 主线程会检查两次 fsync 的时间，如果距离上次 fsync 时间超过了 2 秒，那么写请求就会阻塞 everysec，最多丢失 2 秒的数据 一旦 fsync 超过 2 秒的延时，整个 redis 就被拖慢 优化思路：优化硬盘写入速度，建议采用 SSD，不要用普通的机械硬盘，SSD 大幅度提升磁盘读写的速度 主从复制延迟问题 主从复制可能会超时严重，这个时候需要良好的监控和报警机制 在 info replication 中，可以看到 master 和 slave 复制的 offset，做一个差值就可以看到对应的延迟量，如果延迟过多，那么就进行报警（可以写一个 shell 脚本去监控） 主从复制风暴问题 如果一下子让多个 slave 从 master 去执行全量复制，一份大的 rdb 同时发送到多个 slave，会导致网络带宽被严重占用 如果一个 master 真的要挂载多个 slave，那尽量用树状结构，不要用星型结构 树，意思就是说，让一个节点下面的 slave 不要太多，可以通过 replication 的方式去配置 如果是在 redis cluster 中应该不会存在这种问题 vm.overcommit_memory 该信息是在 redis 启动的时候一些警告信息，这些警告信息可以通过调整 linux 内核配置达到性能的优化 [root@eshop-cache03 ~]# cat /var/log/redis/7008.log 1418:M 24 Mar 13:10:59.513 * Increased maximum number of open files to 10032 (it was originally set to 1024). 1418:M 24 Mar 13:10:59.513 # Warning: 32 bit instance detected but no memory limit set. Setting 3 GB maxmemory limit with 'noeviction' policy now. 1418:M 24 Mar 13:10:59.513 * No cluster configuration found, I'm 728e473d6e5e36ddb051c600c7708f23733c46f7 _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 3.2.8 (00000000/0) 32 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in cluster mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 7008 | `-._ `._ / _.-' | PID: 1418 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | http://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 1418:M 24 Mar 13:10:59.626 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 1418:M 24 Mar 13:10:59.626 # Server started, Redis version 3.2.8 1418:M 24 Mar 13:10:59.626 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add 'vm.overcommit_memory = 1' to /etc/sysctl.conf and then reboot or run the command 'sysctl vm.overcommit_memory=1' for this to take effect. 0: 检查有没有足够内存，没有的话申请内存失败 1: 允许使用内存直到用完为止 2: 内存地址空间不能超过 swap + 50% 如果是 0 的话，可能导致类似 fork 等操作执行失败，申请不到足够的内存空间 下面的命令在日志里面就已经提示出来了 cat /proc/sys/vm/overcommit_memory echo \"vm.overcommit_memory=1\" >> /etc/sysctl.conf sysctl vm.overcommit_memory=1 swapiness # 查看 linux 内核版本 cat /proc/version 如果 linux 内核版本 如果 linux 内核版本 >=3.5，那么 swapiness 设置为 1，这样系统宁愿 swap 也不会 oom killer 保证 redis 不会被杀掉 echo 0 > /proc/sys/vm/swappiness echo vm.swapiness=0 >> /etc/sysctl.conf 最大打开文件句柄 Increased maximum number of open files to 10032 (it was originally set to 1024). # 如果该命令不可用，可以去百度搜索不同的版本命令 ulimit -n 10032 10032 自己去上网搜一下，不同的操作系统，版本，设置的方式都不太一样 tcp backlog WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. cat /proc/sys/net/core/somaxconn echo 511 > /proc/sys/net/core/somaxconn 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"redis/034.html":{"url":"redis/034.html","title":"034. redis 阶段性总结：1T 以上海量数据+10 万以上 QPS 高并发+ 99.99% 高可用","keywords":"","body":" 034. redis 阶段性总结：1T 以上海量数据+10 万以上 QPS 高并发+ 99.99% 高可用 讲解 redis 是为了什么？ 讲解的 redis 可以实现什么效果？ redis 的第一套企业级的架构 redis 的第二套企业级架构 我们现在课程讲解的项目进展到哪里了？ 034. redis 阶段性总结：1T 以上海量数据+10 万以上 QPS 高并发+ 99.99% 高可用 讲解 redis 是为了什么？ 本课程主题：高并发、亿级流量、高性能、海量数据的场景，电商网站的商品详情页系统的缓存架构 商品详情页系统，大型电商网站，会有很多部分组成，但是支撑高并发、亿级流量的，主要就是其中的大型的缓存架构，在这个大型的缓存架构中，redis 是最最基础的一层 高并发，缓存架构中除了 redis，还有其他的组成部分，但是 redis 至关重要，因为大量的离散请求，随机请求 离散请求：各种你未知的用户过来的请求，上千万用户过来访问，每个用户访问 10 次; 集中式请求：1 个用户过来，一天访问 1 亿次 大量的离散请求场景下，支撑商品展示的最重要的，就是 redis cluster，去抗住每天上亿的请求流量，支撑高并发的访问 redis cluster 在整个缓存架构中，如何跟其他几个部分搭配起来组成一个大型的缓存系统，后面再讲 讲解的 redis 可以实现什么效果？ 我之前一直在 redis 的各个知识点的讲解之前都强调一下，我们要讲解的每个知识点，要解决的问题是什么？ 比如 持久化、复制（主从架构）、哨兵（高可用，主备切换）、redis cluster（海量数据+横向扩容+高可用/主备切换） 持久化：高可用的一部分 在发生 redis 集群灾难的情况下（比如说部分 master + slave 全部死掉了），如何快速进行数据恢复，快速实现服务可用，才能实现整个系统的高可用 复制：主从架构 master -> slave 复制，读写分离的架构，写 master，读 slave，横向扩容 slave 支撑更高的读吞吐， 主要解决读高并发，10万，20万，30万，上百万，QPS，横向扩容 哨兵：高可用 结合主从架构使用，在 master 故障的时候，快速将 slave 切换成 master，实现快速的灾难恢复，实现高可用性 redis cluster： 多 master 读写、数据分布式的存储、横向扩容、水平扩容、快速支撑高达的数据量 + 更高的读写 QPS、自动进行 master -> slave 的主备切换，高可用 这几块知识讲解完之后，就明白了：是让底层的缓存系统（redis），实现能够任意水平扩容，支撑海量数据（1T+，几十 T，10G * 600 redis = 6T），支撑很高的读写 QPS（redis 单机在几万 QPS，10 台，几十万 QPS），高可用性（给我们每个 redis 实例都做好 AOF+RDB 的备份策略+容灾策略，slave -> master 主备切换） 最终实现：1T+ 海量数据、10 万+ 读写 QPS、99.99% 高可用性 redis 的第一套企业级的架构 如果你的数据量不大，单 master 就可以容纳，一般来说你的缓存的总量在 10G 以内就可以，那么建议按照以下架构去部署 redis redis 持久化 备份方案 容灾方案 replication（主从 + 读写分离） sentinal（哨兵集群，3 个节点，高可用性） 这套方案可以支撑： 数据量在 10G 以内 写 QPS 在几万左右 读 QPS 可以上 10 万以上（随你的需求，水平扩容 slave 节点就可以） 可用性在 99.99% redis 的第二套企业级架构 如果你的数据量很大，比如我们课程的 topic，大型电商网站的商品详情页的架构（对标那些国内排名前三的大电商网站，如 *宝，*东，*宁易购），数据量是很大的，需要支撑海量数据 那么就建议使用 redis cluster，多 master 分布式存储数据，水平扩容， 数据量 1T+ 以上没问题，只要扩容 master 即可 上 10万 的读写 QPS，99.99% 高可用性 ::: tip redis cluster 读写分离支持不太好，readonly 才能去 slave 上读, 所以只需要横向扩容 master 即可 ::: 支撑 99.99% 可用性，也没问题，slave -> master的主备切换，冗余 slave 去进一步提升可用性的方案（每个 master 挂一个 slave，但是整个集群再加个 3 个 slave 冗余一下） 我们课程里，两套架构都讲解了，后续的业务系统的开发，主要是基于 redis cluster 去做 从架构的角度，我们的 redis 是可以做到的，水平扩容，只要机器足够，到 1T 数据量，50 万读写 QPS，99.99% 我们现在课程讲解的项目进展到哪里了？ 我们要做后续的业务系统的开发，redis 的架构部署好是第一件事情，也是非常重要的，也是你作为一个架构师而言，在对系统进行设计的时候，你必须要考虑到底层的 redis 的并发、性能、能支撑的数据量、可用性 redis：水平扩容，海量数据，上 10 万的读写 QPS，99.99% 高可用性 从架构的角度，我们的 redis 是可以做到的，水平扩容，只要机器足够，到 1T 数据量，50 万读写 QPS，99.99% 正式开始做大型电商网站的商品详情页系统，大规模的缓存架构设计 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"035.html":{"url":"035.html","title":"035. 亿级流量商品详情页的多级缓存架构以及架构中每一层的意义","keywords":"","body":" 035. 亿级流量商品详情页的多级缓存架构以及架构中每一层的意义 商品详情系统旁白 上亿流量的商品详情页系统的多级缓存架构 多级缓存架构中每一层的意义 小结 035. 亿级流量商品详情页的多级缓存架构以及架构中每一层的意义 我们之前的三十讲，主要是在讲解 redis 如何支撑海量数据、高并发读写、高可用服务的架构 redis 架构，在我们的真正类似商品详情页读高并发的系统中，redis 就是底层的缓存存储的支持 从这一讲开始，我们正式开始做业务系统的开发 商品详情系统旁白 亿级流量以上的电商网站的商品详情页的系统，真实的系统中包含大量的业务，十几个人做一两年才堆出来复杂的业务系统。 而作为本课程不可能讲解复杂的业务，因为整套课程就几十个小时的课程。 如果只是纯粹的架构，那么就是一个骨架，所以必须有少量的业务，有血有肉，把整个项目串起来，在业务背景下去学习架构，效果才会理想 讲解商品详情页系统，主要讲解缓存架构，90% 大量的业务代码（没有什么技术含量），10% 的最优技术含量的就是架构，上亿流量，每秒 QPS 几万，上十万的，读并发 支撑读并发，主要是缓存架构 上亿流量的商品详情页系统的多级缓存架构 很多人以为，做个缓存其实就是用一下 redis 访问一下，就可以了，这个只是简单的缓存 如果做复杂的缓存，支撑电商复杂的场景下的高并发的缓存，遇到的问题非常非常之多，绝对不是说简单的访问一下 redsi 就可以的 采用三级缓存：nginx 本地缓存 + redis 分布式缓存 + tomcat 堆缓存的多级缓存架构 使用这个架构主要是为了解决业务中的一些需求和问题： 时效性要求非常高的数据：库存 一般来说，显示的库存都是时效性要求会相对高一些，因为随着商品的不断的交易，库存会不断的变化 当然，我们就希望当库存变化的时候，尽可能更快将库存显示到页面上去，而不是说等了很长时间，库存才反应到页面上去 时效性要求不高的数据：商品的基本信息（名称、颜色、版本、规格参数，等等） 比如你现在改变了商品的名称，稍微晚个几分钟反应到商品页面上，也还能接受 商品价格/库存等 时效性要求高 的数据，而且种类较少，采取相关的服务系统每次发生了变更的时候，直接采取数据库和 redis 缓存双写的方案，这样缓存的时效性最高 商品基本信息等 时效性不高 的数据，而且种类繁多，来自多种不同的系统，采取 MQ 异步通知的方式，写一个数据生产服务，监听 MQ 消息，然后异步拉取服务的数据，更新 tomcat jvm 缓存 + redis 缓存 nginx+lua 脚本做页面动态生成的工作，每次请求过来，优先从 nginx 本地缓存中提取各种数据，结合页面模板，生成需要的页面，如果 nginx 本地缓存过期了，那么就从 nginx 到 redis 中去拉取数据，更新到 nginx 本地，如果 redis 中也被 LRU 算法清理掉了，那么就从 nginx走 http 接口到后端的服务中拉取数据，数据生产服务中，现在本地 tomcat 里的 jvm 堆缓存（ehcache）中找，如果也被 LRU 清理掉了，那么就重新发送请求到源头的服务中去拉取数据，然后再次更新 tomcat 堆内存缓存 + redis 缓存，并返回数据给 nginx，nginx 缓存到本地 结合上图去理解上面的文字，该架构 redis 不是应用服务主要使用的，是 nginx 通过 lua 脚本主要使用的缓存 多级缓存架构中每一层的意义 nginx 本地缓存，抗的是热数据的高并发访问，一般来说，商品的购买总是有热点的，比如每天购买 iphone、nike、海尔等知名品牌的东西的人，总是比较多的 这些热数据，利用 nginx 本地缓存，由于经常被访问，所以可以被锁定在 nginx 的本地缓存内 大量的热数据的访问，就是经常会访问的那些数据，就会被保留在 nginx 本地缓存内，那么对这些热数据的大量访问，就直接走 nginx 就可以了 那么大量的访问，直接就可以走到 nginx 就行了，不需要走后续的各种网络开销了 传统的缓存架构如下图（我自己就是这种使用的） 这样就会多出来转发的网络开销。而 nginx 直接走 redis + 本地缓存就少了很多的网络开销 redis 分布式大规模缓存，抗的是很高的离散访问，支撑海量的数据，高并发的访问，高可用的服务 redis 缓存最大量的数据，最完整的数据和缓存，1T+ 数据; 支撑高并发的访问，QPS 最高到几十万; 可用性，非常好，提供非常稳定的服务 因为 nginx 本地内存有限，也就能 cache 住部分热数据，除了各种 iphone、nike 等热数据，其他相对不那么热的数据，可能流量会经常走到 redis 中 利用 redis cluster 的多 master 写入，横向扩容，1T+ 以上海量数据支持，几十万的读写 QPS，99.99% 高可用性，那么就可以抗住大量的离散访问请求 tomcat jvm 堆内存缓存，主要是抗 redis 大规模灾难的，如果 redis 出现了大规模的宕机，导致 nginx 大量流量直接涌入数据生产服务，那么最后的 tomcat 堆内存缓存至少可以再抗一下，不至于让数据库直接裸奔 同时 tomcat jvm 堆内存缓存，也可以抗住 redis 没有 cache 住的最后那少量的部分缓存 小结 针对高并发访问的一些特点，进行了拆分： nginx 本地缓存：抗少量热数据请求 redis：抗最完整最全量的离散访问请求 应用堆内存：安全防护措施，防止 mysql 数据库裸奔 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"038.html":{"url":"038.html","title":"038. 在 linux 虚拟机中安装部署 MySQL 数据库","keywords":"","body":"038. 在 linux 虚拟机中安装部署 MySQL 数据库 后面写的各种代码，还是要基于 mysql 去做一些开发的，因为缓存的底层的数据存储肯定是数据库 mysql 部署在 eshop-cache04 机器上。 使用 yum 方式安装 # 安装的版本是 Server version: 5.1.73 Source distribution yum install -y mysql-server # 启动 mysql service mysqld start # 配置开机启动 chkconfig mysqld on mysql mysql> show databases # 安装客户端，该软件的作用是什么？因为不安装也能使用 mysql 进入客户端 yum install -y mysql-connector-java 关于密码的修改，在启动日志中有 [root@eshop-cache04 local]# service mysqld start Initializing MySQL database: Installing MySQL system tables... OK Filling help tables... OK To start mysqld at boot time you have to copy support-files/mysql.server to the right place for your system PLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER ! To do so, start the server, then issue the following commands: /usr/bin/mysqladmin -u root password 'new-password' /usr/bin/mysqladmin -u root -h eshop-cache04 password 'new-password' Alternatively you can run: /usr/bin/mysql_secure_installation which will also give you the option of removing the test databases and anonymous user created by default. This is strongly recommended for production servers. See the manual for more instructions. You can start the MySQL daemon with: cd /usr ; /usr/bin/mysqld_safe & You can test the MySQL daemon with mysql-test-run.pl cd /usr/mysql-test ; perl mysql-test-run.pl Please report any problems with the /usr/bin/mysqlbug script! [ OK ] Starting mysqld: [ OK ] 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"039.html":{"url":"039.html","title":"039. 库存服务的开发框架整合与搭建：spring boot + mybatis + jedis","keywords":"","body":" 039. 库存服务的开发框架整合与搭建：spring boot + mybatis + jedis 整合 redis cluster 039. 库存服务的开发框架整合与搭建：spring boot + mybatis + jedis ::: tip 项目搭建，视频中使用 mavn，本笔记使用 gradle-4.8.1 + spring boot 2.1.3 gradle 版本的不同对于生成的 build.gradle 语法可能不太同 同样，对于 spring boot 来说，2.0.4 版本与 2.1.3 版本生成的 build.gradle 写法相差很大 本次练习项目 eshop-inventory 的 GitHub 地址 ::: 本次使用 idea 的 Spring Initializr 工具辅助创建 gradle 项目 生成的 build.gradle 内容如下 plugins { id 'org.springframework.boot' version '2.1.3.RELEASE' id 'java' } apply plugin: 'io.spring.dependency-management' group = 'cn.mrcode.cachepdp' version = '0.0.1-SNAPSHOT' sourceCompatibility = '1.8' repositories { maven { url 'https://repo.spring.io/libs-snapshot' } maven { url \"http://maven.aliyun.com/nexus/content/groups/public\" } maven { url \"https://maven.repository.redhat.com/ga/\" } maven { url \"http://maven.nuiton.org/nexus/content/groups/releases/\" } maven { url \"https://repository.cloudera.com/artifactory/cloudera-repos/\" } mavenCentral() } dependencies { implementation 'org.springframework.boot:spring-boot-starter-actuator' implementation 'org.springframework.boot:spring-boot-starter-jdbc' implementation 'org.springframework.boot:spring-boot-starter-thymeleaf' implementation 'org.springframework.boot:spring-boot-starter-web' implementation 'org.mybatis.spring.boot:mybatis-spring-boot-starter:2.0.0' runtimeOnly 'mysql:mysql-connector-java' testImplementation 'org.springframework.boot:spring-boot-starter-test' compile 'com.alibaba:fastjson:1.1.43' } # 增加数据库用户 eshop/eshop grant all privileges on eshop.* to 'eshop'@'%' identified by 'eshop'; # 以下操作：创建库和创建表建议使用图形化工具创建，下面语句不完整，没有字符集会乱码 create database if not exists eshop; use eshop; # 创建 user 表 CREATE TABLE `user` ( `name` varchar(255) CHARACTER SET utf8 DEFAULT NULL, `age` int(11) DEFAULT NULL ) ENGINE=MyISAM DEFAULT CHARSET=utf8; insert into user values('张三', 25) mybatis 整合测试 整个联通测试目录结构如图： EshopInventoryApplication @SpringBootApplication @MapperScan(value =\"cn.mrcode.cachepdp.eshop.inventory.mapper\") public class EshopInventoryApplication { public static void main(String[] args) { SpringApplication.run(EshopInventoryApplication.class, args); } } application.yml server: # 我这里指定 6000 会发现起来之后不能访问到 port: 6001 logging: level: root: info # 可以打印 sql cn.mrcode.cachepdp.eshop.inventory: debug # path: ./ spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://192.168.99.173:3306/eshop?useUnicode=yes&characterEncoding=UTF-8&useSSL=false username: eshop password: eshop jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 mybatis: # type-aliases-package: cn.mrcode.cachepdp.eshop.inventory.model mapper-locations: classpath*:mapper/*.xml UserMapper public interface UserMapper { User findUserInfo(); } UserMapper.xml select name,age from user; User public class User { private String name; private int age; public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } service public interface UserService { User getUserInfo(); } @Service public class UserServiceImpl implements UserService { @Autowired private UserMapper userMapper; @Override public User getUserInfo() { return userMapper.findUserInfo(); } } @RestController public class UserController { @Autowired private UserService userService; @RequestMapping(\"/getUserInfo\") @ResponseBody public User getUserInfo() { User user = userService.getUserInfo(); return user; } } 启动之后，能正常访问 http://localhost:6001/getUserInfo 整合 redis cluster 这里是使用 jedis 手动创建的 cluster 客户端 compile 'redis.clients:jedis' 直接在某个配置类中增加一个即可 @Bean public JedisCluster jedisCluster() { // 这里使用 redis-trib.rb check 192.168.99.170:7001 找到 3 个 master 节点，添加进来 Set jedisClusterNodes = new HashSet<>(); jedisClusterNodes.add(new HostAndPort(\"192.168.99.170\", 7001)); jedisClusterNodes.add(new HostAndPort(\"192.168.99.172\", 7005)); jedisClusterNodes.add(new HostAndPort(\"192.168.99.171\", 7003)); JedisCluster jedisCluster = new JedisCluster(jedisClusterNodes); return jedisCluster; } 接下来写一个测试 // RedisDAO 就是一个自定义的接口 @Repository public class RedisDAOImpl implements RedisDAO { @Autowired private JedisCluster jedisCluster; @Override public void set(String key, String value) { jedisCluster.set(key, value); } @Override public String get(String key) { return jedisCluster.get(key); } } // 在 UserServiceImpl 中增加获取 缓存用户的信息 @Override public User getCachedUserInfo() { redisDAO.set(\"cached_user\", \"{\\\"name\\\": \\\"zhangsan\\\", \\\"age\\\": 25}\"); String json = redisDAO.get(\"cached_user\"); JSONObject jsonObject = JSONObject.parseObject(json); User user = new User(); user.setName(jsonObject.getString(\"name\")); user.setAge(jsonObject.getInteger(\"age\")); return user; } // 在 UserController 中增加获取接口 @RequestMapping(\"/getCachedUserInfo\") @ResponseBody public User getCachedUserInfo() { User user = userService.getCachedUserInfo(); return user; } 启动后，访问成功即可表示整合成功 http://localhost:6001/getCachedUserInfo { \"name\": \"zhangsan\", \"age\": 25 } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"040.html":{"url":"040.html","title":"040. 在库存服务中实现缓存与数据库双写一致性保障方案（一、二、三、四）","keywords":"","body":" 040. 在库存服务中实现缓存与数据库双写一致性保障方案 系统初启动时，初始化线程池与内存队列 两种请求对象封装 请求异步执行 service 封装 两种请求 Controller 封装 读请求去重优化 空数据读请求过滤优化 深入解决去读请求去重优化 总结 040. 在库存服务中实现缓存与数据库双写一致性保障方案 由于该章节视频是分段录制的，然而整体是写代码耗费的时间，这里合并为一个章节，对应原始视频章节 040、041、042、043 ::: tip 对于这个案例背景，是简化的，本课程重点是亿级流量高并发缓存架构， 代码也是简化的比较简单的能把主要流程走通。 所以不要对代码有太高的期待，对于实际的代码组织，本人（笔记者） 会根据思路理解自行组织，当然大方向还是会跟着视频中的走，部分是在是太乱的代码， 会进行重组。 040~044 章节代码对于自己来说是一个从来没有接触过的思路写法， 这个写法单独开辟分支保留了出来 gitHub 传送门 ::: 再来回顾下之前的思路： 数据更新：根据唯一标识路由到一个队里中，「删除缓存 + 更新数据」 数据读取：如果不在缓存中，根据唯一标识路由到一个队里中，「读取数据 + 写入缓存」 投入队里之后，就等待结果完成，由于同一个标识路由到的是同一个队列中， 所以相当于加锁了。 下面就来实现这个思路，分几步走： 系统初启动时，初始化线程池与内存队列 两种请求对象封装 请求异步执行 service 封装 两种请求 Controller 封装 读请求去重优化 空数据读请求过滤优化 由于该代码核心的就几个地方，其他的都是基础的业务与数据库的常规操作， 故而笔记只记录重点思路地方 系统初启动时，初始化线程池与内存队列 通过 ApplicationRunner 机制，在系统初始化时，对线程池进行初始化操作 /** * 线程与队列初始化 * * @author : zhuqiang * @date : 2019/4/3 22:44 */ @Component public class RequestQueue implements ApplicationRunner { private final List> queues = new ArrayList<>(); @Override public void run(ApplicationArguments args) throws Exception { int workThread = 10; ExecutorService executorService = Executors.newFixedThreadPool(workThread); for (int i = 0; i queue = new ArrayBlockingQueue<>(100); executorService.submit(new RequestProcessorThread(queue)); queues.add(queue); } } public ArrayBlockingQueue getQueue(int index) { return queues.get(index); } } /** * 处理请求的线程 * * @author : zhuqiang * @date : 2019/4/3 22:38 */ public class RequestProcessorThread implements Callable { private final ArrayBlockingQueue queue; public RequestProcessorThread(ArrayBlockingQueue queue) { this.queue = queue; } @Override public Boolean call() throws Exception { try { while (true) { Request take = queue.take(); take.process(); } } catch (InterruptedException e) { e.printStackTrace(); } return false; } } 两种请求对象封装 数据更新请求 /** * 数据更新请求 * * @author : zhuqiang * @date : 2019/4/3 23:05 */ public class ProductInventoryDBUpdateRequest implements Request { private final ProductInventory productInventory; private final ProductInventoryService productInventoryService; public ProductInventoryDBUpdateRequest(ProductInventory productInventory, ProductInventoryService productInventoryService) { this.productInventory = productInventory; this.productInventoryService = productInventoryService; } @Override public void process() { //1. 删除缓存 productInventoryService.removeProductInventoryCache(productInventory.getProductId()); //2. 更新库存 productInventoryService.updateProductInventory(productInventory); } } 数据刷新请求 /** * 缓存刷新请求 * * @author : zhuqiang * @date : 2019/4/6 14:13 */ public class ProductInventoryCacheRefreshRequest implements Request { private final Integer productId; private final ProductInventoryService productInventoryService; public ProductInventoryCacheRefreshRequest(Integer productId, ProductInventoryService productInventoryService) { this.productId = productId; this.productInventoryService = productInventoryService; } @Override public void process() { // 1. 读取数据库库存 ProductInventory productInventory = productInventoryService.findProductInventory(productId); // 2. 设置缓存 productInventoryService.setProductInventoryCache(productInventory); } } 请求异步执行 service 封装 @Service public class RequestAsyncProcessServiceImpl implements RequestAsyncProcessService { @Autowired private RequestQueue requestQueue; @Override public void process(Request request) { try { // 1. 根据商品 id 路由到具体的队列 ArrayBlockingQueue queue = getRoutingQueue(request.getProductId()); // 2. 放入队列 queue.put(request); } catch (InterruptedException e) { e.printStackTrace(); } } private ArrayBlockingQueue getRoutingQueue(Integer productId) { // 先获取 productId 的 hash 值 String key = String.valueOf(productId); int h; int hash = (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16); // 对hash值取模，将hash值路由到指定的内存队列中，比如内存队列大小8 // 用内存队列的数量对hash值取模之后，结果一定是在0~7之间 // 所以任何一个商品id都会被固定路由到同样的一个内存队列中去的 int index = (requestQueue.queueSize() - 1) & hash; return requestQueue.getQueue(index); } } 两种请求 Controller 封装 /** * 商品库存 * * @author : zhuqiang * @date : 2019/4/6 15:23 */ @RestController public class ProductInventoryController { @Autowired private RequestAsyncProcessService requestAsyncProcessService; @Autowired private ProductInventoryService productInventoryService; /** * 更新商品库存 */ @RequestMapping(\"/updateProductInventory\") public Response updateProductInventory(ProductInventory productInventory) { try { ProductInventoryDBUpdateRequest request = new ProductInventoryDBUpdateRequest(productInventory, productInventoryService); requestAsyncProcessService.process(request); return new Response(Response.SUCCESS); } catch (Exception e) { e.printStackTrace(); return new Response(Response.FAILURE); } } @RequestMapping(\"/getProductInventory\") public ProductInventory getProductInventory(Integer productId) { try { // 异步获取 ProductInventoryCacheRefreshRequest request = new ProductInventoryCacheRefreshRequest(productId, productInventoryService); requestAsyncProcessService.process(request); ProductInventory productInventory = null; long startTime = System.currentTimeMillis(); long endTime = 0L; long waitTime = 0L; // 最多等待 200 毫秒 while (true) { if (waitTime > 200) { break; } // 尝试去redis中读取一次商品库存的缓存数据 productInventory = productInventoryService.getProductInventoryCache(productId); // 如果读取到了结果，那么就返回 if (productInventory != null) { return productInventory; } // 如果没有读取到结果，那么等待一段时间 else { Thread.sleep(20); endTime = System.currentTimeMillis(); waitTime = endTime - startTime; } } // 直接尝试从数据库中读取数据 productInventory = productInventoryService.findProductInventory(productId); if (productInventory != null) { return productInventory; } } catch (Exception e) { e.printStackTrace(); } return new ProductInventory(productId, -1L); } } 读请求去重优化 核心思路是通过：map 来保存写标志 @Service public class RequestAsyncProcessServiceImpl implements RequestAsyncProcessService { @Autowired private RequestQueue requestQueue; @Override public void process(Request request) { try { Map flagMap = requestQueue.getFlagMap(); // 如果是一个更新数据库请求 if (request instanceof ProductInventoryDBUpdateRequest) { flagMap.put(request.getProductId(), true); } else if (request instanceof ProductInventoryCacheRefreshRequest) { Boolean flag = flagMap.get(request.getProductId()); // 系统启动后，就没有写请求，全是读，可能导致 flas = null if (flag == null) { flagMap.put(request.getProductId(), false); } // 已经有过读或写的请求 并且前面已经有一个写请求了 if (flag != null && flag) { // 读取请求把，写请求标志冲掉 flagMap.put(request.getProductId(), false); } // 如果是读请求，直接返回，等待写完成即可 else if (flag != null && !flag) { return; } } // 1. 根据商品 id 路由到具体的队列 ArrayBlockingQueue queue = getRoutingQueue(request.getProductId()); // 2. 放入队列 queue.put(request); } catch (InterruptedException e) { e.printStackTrace(); } } 空数据读请求过滤优化 上面的逻辑，会让这种场景下的请求不执行，但是在 getProductInventory 中，如果从缓存中没有读取到，则最终会走一次数据库。 // 系统启动后，就没有写请求，全是读，可能导致 flas = null if (flag == null) { flagMap.put(request.getProductId(), false); } 这里就存在一个 bug 了，会导致缓存一直被穿透，如果没有写请求的话，读请求被去重了，一直请求数据库。 修复这个 bug 的话，最简单的办法就是在读取数据库后，直接写入缓存中，如果不考虑并发问题的话，直接在 getProductInventory 中读取数据库后写入缓存即可。 那么就还有一个场景会导致缓存会穿透：数据库中没有这个数据，就会一直走查库的操作，这个问题后续会有解决方案； 深入解决去读请求去重优化 上面的代码存在几个问题： RequestAsyncProcessServiceImpl.process 判定与设置 flag 值在并发情况下会导致 flag 值问题 查库之后直接写缓存在并发情况下会导致数据不一致的情况（多个请求写数据，队列无意义了） 在 ProductInventoryController 中只要走了数据库后，就强制请求刷新缓存 // 直接尝试从数据库中读取数据 productInventory = productInventoryService.findProductInventory(productId); if (productInventory != null) { // 读取到了数据，强制刷新缓存 ProductInventoryCacheRefreshRequest forceRfreshRequest = new ProductInventoryCacheRefreshRequest(productId, productInventoryService, true); requestAsyncProcessService.process(forceRfreshRequest); return productInventory; } 每个工作线程，自己处理自己队列的读去重请求 /** * 处理请求的线程 * * @author : zhuqiang * @date : 2019/4/3 22:38 */ public class RequestProcessorThread implements Callable { private final ArrayBlockingQueue queue; /** * k: 商品 id v：请求标志： true : 有更新请求 */ private final Map flagMap = new ConcurrentHashMap<>(); public RequestProcessorThread(ArrayBlockingQueue queue) { this.queue = queue; } @Override public Boolean call() throws Exception { try { while (true) { Request request = queue.take(); // 非强制刷新请求的话，就是一个正常的读请求 if (!request.isForceRfresh()) { // 如果是一个更新数据库请求 if (request instanceof ProductInventoryDBUpdateRequest) { flagMap.put(request.getProductId(), true); } else if (request instanceof ProductInventoryCacheRefreshRequest) { Boolean flag = flagMap.get(request.getProductId()); if (flag == null) { flagMap.put(request.getProductId(), false); } // 已经有过读或写的请求 并且前面已经有一个写请求了 if (flag != null && flag) { // 读取请求把，写请求标志冲掉 // 本次读会正常的执行，组成 1+1 （1 写 1 读） // 后续的正常读请求会被过滤掉 flagMap.put(request.getProductId(), false); } // 如果是读请求，直接返回，等待写完成即可 else if (flag != null && !flag) { continue; } } } request.process(); } } catch (InterruptedException e) { e.printStackTrace(); } return false; } } 这样一改造之后，并发的地方，就利用队列串行起来了，那么此代码还存在以下场景的缺陷： 当大量请求超过 200 毫秒未获取到缓存，会导致大量请求汇聚到数据库 这种情况的发生场景有： 大量的写请求在前面，导致后续的大量读请求超时，直接读库 数据库压根就没有这个商品，导致缓存被穿透，一直读库 当大量请求超过 200 毫秒后，在数据库获取到了，并请求强制刷新缓存，导致大量请求又回去到数据库了 这种情况是由于增加了强制刷新标志，导致的另外一个 bug，这个时候的思路可以再增加一个强制刷新队列来做强制读请求去重 总结 异步串行化的实现核心思路： 使用队列来避免数据竞争 删除缓存 + 更新数据库 封装成一个写请求 读取数据库 + 写缓存 封装成一个读请求 根据商品 id 路由到同一个队列中（此方案暂未考虑多服务实例的场景） 有写 1+1（1 写 1 读）时，需要过滤掉大量的读请求 这部分正常读请求如不过滤掉，会进入数据库，且库存并未更新，浪费性能资源与缓存穿透（有数据，且数据已经进入了缓存，但是队列中还一直去数据库执行并刷新缓存） 等待读请求需要超时机制，一旦超时则从数据库获取 此类场景出现的时候可能的原因有如下几点： 每个读或写请求测试耗时不准确 测试不准确导致服务实例不够（当然此章节并未解决多服务实例怎么路由或者解决并发的问题） 缓存被穿透，使用不存在的数据一致访问 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"044.html":{"url":"044.html","title":"044. 库存服务代码调试以及打印日志观察服务的运行流程是否正确","keywords":"","body":" 044. 库存服务代码调试以及打印日志观察服务的运行流程是否正确 redis 中无数据情况下 基于缓存中有库存测试 基于 200 毫秒内返回写操作完成测试 讲师旁白 044. 库存服务代码调试以及打印日志观察服务的运行流程是否正确 创建数据库 product_inventory，两个字段 Integer product_id、Long inventory_cnt 测试场景： 一个写请求： 写请求模拟耗时操作：休眠 10 秒 在写休眠中，来一个读请求 观察日志，是否按照预想流程和结果进行； 在这之前，需要再关键位置添加日志打印，笔记就不贴代码了； 在数据库插入一条数据 INSERT INTO `eshop`.`product_inventory` (`product_id`, `inventory_cnt`) VALUES ('1', '100'); redis 中无数据情况下 在以上场景的基础下，先来模拟 redis 中无数据的情况下的流程是否正确，因为刚好往数据库中增加了数据，还没有往 redis 中增加数据。 正好测试这个场景 // 写请求 http://localhost:6001/updateProductInventory?productId=1&inventoryCnt=99 // 读请求 http://localhost:6001/getProductInventory?productId=1 日志信息 2019-04-06 20:55:16.257 INFO 9420 --- [nio-6001-exec-1] c.m.c.e.i.w.ProductInventoryController : 更新商品库存请求：商品id=1，库存=99 2019-04-06 20:55:16.258 INFO 9420 --- [nio-6001-exec-1] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 20:55:16.375 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 20:55:16.376 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 写请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 20:55:16.440 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已删除缓存：商品 ID=1 2019-04-06 20:55:16.440 INFO 9420 --- [pool-1-thread-2] .c.e.i.r.ProductInventoryDBUpdateRequest : 写请求：模拟写耗时操作，休眠 10 秒钟 // 上面开始模拟耗时操作了 2019-04-06 20:55:17.970 INFO 9420 --- [nio-6001-exec-2] c.m.c.e.i.w.ProductInventoryController : 读取商品库存请求：商品id=1 2019-04-06 20:55:17.971 INFO 9420 --- [nio-6001-exec-2] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 20:55:18.190 INFO 9420 --- [nio-6001-exec-2] c.m.c.e.i.w.ProductInventoryController : 超时 200 毫秒退出尝试，商品 ID=1 2019-04-06 20:55:18.190 INFO 9420 --- [nio-6001-exec-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 // 等待超时，并从数据库获取，获取到了 100 的库存 2019-04-06 20:55:18.234 INFO 9420 --- [nio-6001-exec-2] c.m.c.e.i.w.ProductInventoryController : 缓存未命中，在数据库中查找，商品 ID=1，结果={\"inventoryCnt\":100,\"productId\":1} 2019-04-06 20:55:18.234 INFO 9420 --- [nio-6001-exec-2] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 20:55:26.497 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已更新数据库：商品 ID=1，库存=99 // 写完成之后，开始读请求的处理 2019-04-06 20:55:26.499 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 20:55:26.499 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 读请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 20:55:26.499 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 1+1 达成，1 写 1 读：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 20:55:26.499 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 2019-04-06 20:55:26.503 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 设置缓存：{\"inventoryCnt\":99,\"productId\":1} // 下面的是强制刷新缓存，由于前面耗时操作，导致直接读库并强制刷新缓存操作 2019-04-06 20:55:26.515 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":true,\"productId\":1} 2019-04-06 20:55:26.515 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 2019-04-06 20:55:26.523 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 设置缓存：{\"inventoryCnt\":99,\"productId\":1} 基于缓存中有库存测试 与上面例子一样，只不过通过了一次测试，现在缓存中有数据了，再继续执行相同的测试操作，观察日志, 库存由 99 变成 98 // 写请求 http://localhost:6001/updateProductInventory?productId=1&inventoryCnt=98 // 读请求 http://localhost:6001/getProductInventory?productId=1 日志输出 2019-04-06 21:03:00.913 INFO 9420 --- [nio-6001-exec-6] c.m.c.e.i.w.ProductInventoryController : 更新商品库存请求：商品id=1，库存=98 2019-04-06 21:03:00.913 INFO 9420 --- [nio-6001-exec-6] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 21:03:00.913 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:03:00.913 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 写请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:03:00.924 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已删除缓存：商品 ID=1 2019-04-06 21:03:00.924 INFO 9420 --- [pool-1-thread-2] .c.e.i.r.ProductInventoryDBUpdateRequest : 写请求：模拟写耗时操作，休眠 10 秒钟 2019-04-06 21:03:02.925 INFO 9420 --- [nio-6001-exec-7] c.m.c.e.i.w.ProductInventoryController : 读取商品库存请求：商品id=1 2019-04-06 21:03:02.925 INFO 9420 --- [nio-6001-exec-7] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 21:03:03.147 INFO 9420 --- [nio-6001-exec-7] c.m.c.e.i.w.ProductInventoryController : 超时 200 毫秒退出尝试，商品 ID=1 2019-04-06 21:03:03.147 INFO 9420 --- [nio-6001-exec-7] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 2019-04-06 21:03:03.159 INFO 9420 --- [nio-6001-exec-7] c.m.c.e.i.w.ProductInventoryController : 缓存未命中，在数据库中查找，商品 ID=1，结果={\"inventoryCnt\":99,\"productId\":1} 2019-04-06 21:03:03.159 INFO 9420 --- [nio-6001-exec-7] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 21:03:10.937 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已更新数据库：商品 ID=1，库存=98 2019-04-06 21:03:10.938 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:03:10.938 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 读请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:03:10.938 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 1+1 达成，1 写 1 读：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:03:10.938 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 2019-04-06 21:03:10.945 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 设置缓存：{\"inventoryCnt\":98,\"productId\":1} 2019-04-06 21:03:10.957 INFO 9420 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":true,\"productId\":1} 2019-04-06 21:03:10.957 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 2019-04-06 21:03:10.962 INFO 9420 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 设置缓存：{\"inventoryCnt\":98,\"productId\":1} 对于在 200 毫秒内无法返回的数据，无论 redis 中是否存在初始库存数据，流程都一样 基于 200 毫秒内返回写操作完成测试 修改休眠时间，让写操作在 200 毫秒内能正常完成 // 写请求 http://localhost:6001/updateProductInventory?productId=1&inventoryCnt=97 // 读请求 http://localhost:6001/getProductInventory?productId=1 日志输出 2019-04-06 21:07:42.997 INFO 20676 --- [nio-6001-exec-1] c.m.c.e.i.w.ProductInventoryController : 更新商品库存请求：商品id=1，库存=97 2019-04-06 21:07:42.998 INFO 20676 --- [nio-6001-exec-1] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 21:07:43.209 INFO 20676 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:07:43.209 INFO 20676 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 写请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:07:43.242 INFO 20676 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已删除缓存：商品 ID=1 2019-04-06 21:07:43.242 INFO 20676 --- [pool-1-thread-2] .c.e.i.r.ProductInventoryDBUpdateRequest : 写请求：模拟写耗时操作，休眠 100 毫秒 2019-04-06 21:07:43.302 INFO 20676 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已更新数据库：商品 ID=1，库存=97 2019-04-06 21:07:43.622 INFO 20676 --- [nio-6001-exec-2] c.m.c.e.i.w.ProductInventoryController : 读取商品库存请求：商品id=1 2019-04-06 21:07:43.624 INFO 20676 --- [nio-6001-exec-2] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 21:07:43.629 INFO 20676 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:07:43.630 INFO 20676 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 读请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:07:43.630 INFO 20676 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 1+1 达成，1 写 1 读：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:07:43.630 INFO 20676 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 2019-04-06 21:07:43.658 INFO 20676 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 设置缓存：{\"inventoryCnt\":97,\"productId\":1} 2019-04-06 21:07:43.687 INFO 20676 --- [nio-6001-exec-2] c.m.c.e.i.w.ProductInventoryController : 在缓存中找到，商品 ID=1 由于上面是休眠 100 毫秒，200 毫秒超时，所以人工请求没法模拟出来。 下面的日志输出是休眠 5 秒，10 秒超时 2019-04-06 21:12:22.725 INFO 21740 --- [nio-6001-exec-1] c.m.c.e.i.w.ProductInventoryController : 更新商品库存请求：商品id=1，库存=97 2019-04-06 21:12:22.726 INFO 21740 --- [nio-6001-exec-1] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 21:12:22.830 INFO 21740 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:12:22.831 INFO 21740 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 写请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:12:23.006 INFO 21740 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已删除缓存：商品 ID=1 2019-04-06 21:12:23.006 INFO 21740 --- [pool-1-thread-2] .c.e.i.r.ProductInventoryDBUpdateRequest : 写请求：模拟写耗时操作，休眠 5 秒钟 2019-04-06 21:12:23.939 INFO 21740 --- [nio-6001-exec-2] c.m.c.e.i.w.ProductInventoryController : 读取商品库存请求：商品id=1 2019-04-06 21:12:23.940 INFO 21740 --- [nio-6001-exec-2] c.e.i.s.i.RequestAsyncProcessServiceImpl : 路由信息：key=1,商品 ID =1,队列 index=1 2019-04-06 21:12:28.047 INFO 21740 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 已更新数据库：商品 ID=1，库存=97 2019-04-06 21:12:28.050 INFO 21740 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 处理请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:12:28.050 INFO 21740 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 读请求：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:12:28.050 INFO 21740 --- [pool-1-thread-2] c.m.c.e.i.r.RequestProcessorThread : 1+1 达成，1 写 1 读：{\"forceRfresh\":false,\"productId\":1} 2019-04-06 21:12:28.050 INFO 21740 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 数据库获取商品，商品 ID=1 2019-04-06 21:12:28.070 INFO 21740 --- [pool-1-thread-2] .m.c.e.i.s.i.ProductInventoryServiceImpl : 设置缓存：{\"inventoryCnt\":97,\"productId\":1} 2019-04-06 21:12:28.086 INFO 21740 --- [nio-6001-exec-2] c.m.c.e.i.w.ProductInventoryController : 在缓存中找到，商品 ID=1 可以看到，在等待超时中，会不断获取缓存中的信息，找到则返回。 此时查看数据库和缓存中的数据都是一致的。 讲师旁白 后续方案，包括本次的方案都是针对我自己遇到过的特殊场景设计出来的， 可能这个方案不一定 100% 适合其他场景，需要改造 再者，可能方案比较复杂，及时之前实施过，可能有一部分细节没有讲解到，会导致一些漏洞， 这个也是可以理解的 本课程主题是缓存架构，主要是架构和设计思想，有些许漏洞希望理解 课程真正最重要的，不是给你一套 100% 包打天下的代码，而是一种设计思想， 多种设计思想组合起来就是某种架构 ::: tip 该方案目前并未关注分布式情况下的多服务实例场景， 从这个来看，架构上和一定的代码落地，的确是比较明确和清晰， 是不是说在面试中只要掌握了这种架构思想就能加分一点呢？ ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"045.html":{"url":"045.html","title":"045. 商品详情页结构分析、缓存全量更新问题以及缓存维度化解决方案","keywords":"","body":" 045. 商品详情页结构分析、缓存全量更新问题以及缓存维度化解决方案 大型电商网站中的商品详情页的数据结构分析 大型缓存全量更新问题 缓存维度化解决方案 045. 商品详情页结构分析、缓存全量更新问题以及缓存维度化解决方案 我们讲解过，咱们的整个缓存的技术方案，分成两块 实时性较高数据 比如说库存，销量之类的这种数据，我们采取的实时的缓存+数据库双写的技术方案，双写一致性保障的方案 实时性要求不高的数据 比如说商品的基本信息，等等，我们采取的是三级缓存架构的技术方案，就是说由一个专门的数据生产的服务，去获取整个商品详情页需要的各种数据，经过处理后，将数据放入各级缓存中，每一级缓存都有自己的作用 我们先来看看一下，所谓的这种实时性要求不高的数据，在商品详情页中，都有哪些 大型电商网站中的商品详情页的数据结构分析 商品的基本信息 标题：【限时直降】Apple/苹果 iPhone 7 128G 全网通4G智能手机正品 短描述：限时优惠 原封国行 正品保障 颜色 存储容量 图片列表 规格参数 打开淘宝找一个 iPhone7 的商品详情页参考，这里只是一部分 其他信息：店铺信息、分类信息，等等，非商品维度的信息 商品介绍：放缓存，图片是懒加载的，不放我们这里讲解 实时信息：实时广告推荐、实时价格、实时活动推送，等等，ajax 加载，如我们前面做的库存缓存 我们不是带着大家用几十讲的时间去做一套完整的商品详情页的系统，电商网站的话，都几百个人做好几年的，所以这里主要还是架构思路 将商品的各种基本信息，分类放到缓存中，每次请求过来，动态从缓存中取数据，然后动态渲染到模板中 数据放缓存，性能高，动态渲染模板，灵活性好 大型缓存全量更新问题 上图是把一个详情页的信息都拼成一个 json 串放一个 value 中， 那么这样只要有其中一点信息更改，就需要全部取出来，更新后，再放回去。 这样做的缺点有： 网络耗费的资源大 每次对redis 都存取大数据，对 redis 的压力也比较大 redis 的性能和吞吐量能够支撑到多大，基本跟数据本身的大小有很大的关系 如果数据越大，那么可能导致 redis 的吞吐量就会急剧下降 可以使用缓存维度化来解决 缓存维度化解决方案 维度：比如商品基本信息维度、商品分类维度、商品店铺维度 不同的维度，可以看做是不同的角度去观察一个东西，那么每个商品详情页中，都包含了不同的维度数据 举个例子：如果不维度化，就导致多个维度的数据混合在一个缓存 value 中，但是不同维度的数据，可能更新的频率都大不一样 比如，现在只是将 1000 个商品的分类批量调整了一下，但是如果商品分类的数据和商品本身的数据混杂在一起，那么可能导致需要将包括商品在内的大缓存 value 取出来，进行更新，再写回去，就会很坑爹，耗费大量的资源，redis 压力也很大 但是如果我们对缓存进行维度化：将每个维度的数据都存一份，比如说商品维度的数据存一份，商品分类的数据存一份，商品店铺的数据存一份，那么在不同的维度数据更新的时候，只要去更新对应的维度就可以了 包括我们之前讲解的那种实时性较高的数据，也可以理解为一个维度，那么维度拆分后如下图 这样一来就解决了，全量更新的问题 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"046.html":{"url":"046.html","title":"046. 缓存数据生产服务的工作流程分析以及工程环境搭建","keywords":"","body":" 046. 缓存数据生产服务的工作流程分析以及工程环境搭建 商品详情页缓存数据生产服务的工作流程分析 创建缓存数据生产服务项目 046. 缓存数据生产服务的工作流程分析以及工程环境搭建 接下来要做这个多级缓存架构，从底往上做，先做缓存数据的生产这一块 商品详情页缓存数据生产服务的工作流程分析 如上图 监听多个 kafka topic，每个 kafka topic 对应一个服务 简化一下，监听一个 kafka topic 如果一个服务发生了数据变更，那么就发送一个消息到 kafka topic 中 缓存数据生产服务监听到了消息以后，就发送请求到对应的服务中调用接口以及拉取数据，此时是从 mysql 中查询的 缓存数据生产服务拉取到了数据之后，会将数据在本地缓存中写入一份，就是 ehcache 中 同时会将数据在 redis 中写入一份 而缓存数据生产服务是前两层（nginx、redis）的基石 创建缓存数据生产服务项目 项目名：eshop-cache 项目基础架子可以参考 039. 库存服务的开发框架整合与搭建：spring boot + mybatis + jedis 除了包名不同，基础架子就是 spring boot + mybatis + jedis 整合的架子 搭建好后：可以访问 http://localhost:6002/ 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"047.html":{"url":"047.html","title":"047. 完成 spring boot 整合 ehcache 的搭建以支持服务本地堆缓存","keywords":"","body":" 047. 完成 spring boot 整合 ehcache 的搭建以支持服务本地堆缓存 ehcache 配置 spring boo 1.x spring boot 2.x spring cache 测试 启动程序测试 047. 完成 spring boot 整合 ehcache 的搭建以支持服务本地堆缓存 因为之前跟大家提过，三级缓存，多级缓存，服务本地堆缓存 + redis 分布式缓存 + nginx 本地缓存组成的 每一层缓存在高并发的场景下，都有其特殊的用途，需要综合利用多级的缓存，才能支撑住高并发场景下各种各样的特殊情况 服务本地堆缓存的作用是预防 redis 层的彻底崩溃，作为缓存的最后一道防线，避免数据库直接裸奔 服务本地堆缓存，我们用什么来做缓存？除了最简单的使用 map 来手动管理缓存之外，还有很多流行的框架可选，Guava cache、ehcache 等，在 spring 中，有一个 Cache 抽象，可以用来支持整合多个缓存框架。我们这里使用 ehcache spring boot + ehcache 整合起来，演示一下是怎么使用的 添加依赖，由于本人是第一次使用 ehcache，所以跟着老师的版本走 compile 'net.sf.ehcache:ehcache:2.8.3' // spring cache 缓存抽象支持 compile 'org.springframework:spring-context-support' ehcache 配置 spring boo 1.x import org.springframework.beans.factory.annotation.Configurable; import org.springframework.cache.annotation.EnableCaching; import org.springframework.cache.ehcache.EhCacheCacheManager; import org.springframework.cache.ehcache.EhCacheManagerFactoryBean; import org.springframework.context.annotation.Bean; import org.springframework.core.io.ClassPathResource; /** * * @author : zhuqiang * @date : 2019/4/7 10:03 */ @Configurable @EnableCaching public class CacheConfiguration { @Bean public EhCacheManagerFactoryBean ehCacheManagerFactoryBean() { EhCacheManagerFactoryBean cacheManagerFactoryBean = new EhCacheManagerFactoryBean(); cacheManagerFactoryBean.setConfigLocation(new ClassPathResource(\"ehcache.xml\")); cacheManagerFactoryBean.setShared(true); return cacheManagerFactoryBean; } @Bean public EhCacheCacheManager ehCacheCacheManager(EhCacheManagerFactoryBean ehCacheManagerFactoryBean) { return new EhCacheCacheManager(ehCacheManagerFactoryBean.getObject()); } } ehcache.xml spring boot 2.x 由于本次笔记使用 2.x，所以配置方式不太一样了，没有深入研究，是因为提供了自动配置类，自动配置类中其实和手动配置的类似 spring: cache: type: ehcache ehcache: config: classpath:/ehcache.xml 以上配置在 yml 中写，加上 ehcache.xml 就可以了 spring cache 测试 import org.springframework.cache.annotation.CachePut; import org.springframework.cache.annotation.Cacheable; import org.springframework.stereotype.Service; import cn.mrcode.cachepdp.eshop.cache.model.ProductInfo; import cn.mrcode.cachepdp.eshop.cache.service.CacheService; @Service public class CacheServiceImpl implements CacheService { public static final String CACHE_NAME = \"local\"; /** * 将商品信息保存到本地缓存中 */ @CachePut(value = CACHE_NAME, key = \"'key_'+#productInfo.getId()\") public ProductInfo saveLocalCache(ProductInfo productInfo) { return productInfo; } /** * 从本地缓存中获取商品信息 */ @Cacheable(value = CACHE_NAME, key = \"'key_'+#id\") public ProductInfo getLocalCache(Long id) { return null; } } 可以看到，在存在的时候直接返回了传递进来的对象， 但是在读取的时候返回了 null，如果先存后取成功了的话，那么久说明目前正和 ehache 是成功的 @Controller public class CacheController { @Autowired private CacheService cacheService; @RequestMapping(\"/testPutCache\") @ResponseBody public String testPutCache(ProductInfo productInfo) { cacheService.saveLocalCache(productInfo); return \"success\"; } @RequestMapping(\"/testGetCache\") @ResponseBody public ProductInfo testGetCache(Long id) { return cacheService.getLocalCache(id); } } 启动程序测试 http://localhost:6002/testPutCache?id=1&name=iphone7&price=100.80 http://localhost:6002/testGetCache?id=1 注意：由于一开始使用 boot 1.x 的手动配置方式，导致存储后，再获取为 null 的情况出现，修改成 boot 2.x 的配置就可以了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"048.html":{"url":"048.html","title":"048. redis 的 LRU 缓存清除算法讲解以及相关配置使用","keywords":"","body":" 048. redis 的 LRU 缓存清除算法讲解以及相关配置使用 LRU 算法概述 缓存清理设置 缓存清理的流程 redis 的 LRU 近似算法 048. redis 的 LRU 缓存清除算法讲解以及相关配置使用 多级缓存架构，缓存数据生产服务监听各个数据源服务的数据变更的消息， 得到消息之后，然后调用接口拉去数据，将拉取到的数据，写入本地 ehcache 缓存一份， 上一章节 spring boot 整合并演示过 数据还会写入 redis 分布式缓存中一份，你不断的将数据写入 redis，然而 redis 的内存是有限的， 每个 redis 实例最大一般也就是设置 10G，当数据写入的量超过了 redis 能承受的范围之后， 该怎么玩儿呢？ redis 是会在数据达到一定程度之后，超过了一个最大的限度之后，就会将数据进行一定的清理， 从内存中清理掉一些数据，只有清理掉一些数据之后，才能将新的数据写入内存中 LRU 算法概述 LRU：Least Recently Used 最近最少使用算法 redis 默认情况下就是使用 LRU 策略的，因为内存是有限的，但是如果你不断地往 redis 里面写入数据， 那肯定是没法存放下所有的数据在内存的（这个默认貌似不太对，在配置文件中的默认配置不是 LRU） 将最近一段时间内，最少使用的一些数据给干掉。比如说有一个 key，在最近 1 个小时内， 只被访问了一次; 还有一个 key 在最近 1 个小时内，被访问了 1 万次， 当内存满的时候，那么 1 小时内只被访问了 1 次的那条数据将会被清理掉 缓存清理设置 配置文件：redis.conf maxmemory：设置 redis 用来存放数据的最大的内存大小 一旦超出这个内存大小之后，就会立即使用 LRU 算法清理掉部分数据 如果用 LRU，那么就是将最近最少使用的数据从缓存中清除出去 对于 64 bit 的机器，如果 maxmemory 设置为 0，那么就默认不限制内存的使用，直到耗尽机器中所有的内存为止; 但是对于 32 bit 的机器，有一个隐式的限制就是 3GB maxmemory-policy 可以设置内存达到最大限制后，采取什么策略来处理 noeviction: 如果内存使用达到了 maxmemory，client 还要继续写入数据，那么就直接报错给客户端 allkeys-lru: 就是我们常说的 LRU 算法，移除掉最近最少使用的那些 keys 对应的数据 volatile-lru: 也是采取 LRU 算法，但是仅仅针对那些设置了指定存活时间（TTL）的 key 才会清理掉 allkeys-random: 随机选择一些 key 来删除掉 volatile-random: 随机选择一些设置了 TTL 的 key 来删除掉 volatile-ttl: 移除掉部分 keys，选择那些 TTL 时间比较短的 keys 对于以上的解释在配置文件中有英文的说明，和上面的基本上一致。 TTL：也就是开发中常说的过期时间，redis 中支持给 key 配置 ttl 这边拓展一下思路，对技术的研究，一旦将一些技术研究的比较透彻之后，就喜欢横向对比底层的一些原理， 玩儿大数据的人知道实时计算领域中 storm 比较流行，storm 有很多的流分组的一些策略， 按 shuffle 分组、global 全局分组、direct 直接分组、fields 按字段值 hash 后分组， 分组策略也很多，但是真正公司里 99% 的场景下，使用的也就是 shuffle 和 fields 两种策略 redis 也一样，给了这么多种乱七八糟的缓存清理的算法，其实真正常用的可能也就那么一两种，allkeys-lru 是最常用的 缓存清理的流程 客户端执行数据写入操作 redis server 接收到写入操作之后，检查 maxmemory 的限制，如果超过了限制，那么就根据对应的 policy 清理掉部分数据 写入操作完成执行 redis 的 LRU 近似算法 科普一个相对来说稍微高级一丢丢的知识点 redis 采取的是 LRU 近似算法，也就是对 keys 进行采样，然后在采样结果中进行数据清理 redis 3.0 开始，在 LRU 近似算法中引入了 pool 机制，表现可以跟真正的 LRU 算法相当， 但是还是有所差距的，不过这样可以减少内存的消耗 redis LRU 算法，是采样之后再做 LRU 清理的，跟真正的、传统、全量的 LRU 算法是不太一样的 maxmemory-samples：比如 5，可以设置采样的大小，如果设置为 10，那么效果会更好，不过也会耗费更多的 CPU 资源 maxmemory-samples 在配置文件中也有一部分解释，根据机翻来看，设置 5 会检查 5 个键来进行对比检查， 所以说是近似的，但是具体的思路没有提及到 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"049.html":{"url":"049.html","title":"049. zookeeper + kafka 集群的安装部署以及如何简单使用的介绍","keywords":"","body":" 049. zookeeper + kafka 集群的安装部署以及如何简单使用的介绍 回顾 老师旁白 zookeeper 集群安装 ` scala 安装 kafka 集群 049. zookeeper + kafka 集群的安装部署以及如何简单使用的介绍 回顾 多级缓存的架构主要用来解决的问题是：时效高低数据的维度存储 时效性不高的数据，比如一些商品的基本信息，如果发生了变更，假设在 5 分钟之后再更新到页面中， 供用户观察到，也是 ok 的，那么我们采取的是异步更新缓存的策略 时效性要求很高的数据，如库存，采取的是数据库 + 缓存双写的技术方案，也解决了双写的一致性的问题 上面这两条可能直接看觉得好像差不多的，这里忽略了一个解释，对于页面来说，需要静态的生成页面， 这个过程可能稍微耗时一些，而对于双写来说则快太多了，它不负责页面渲染等工作，只需要把缓存数据更新即可 缓存数据生产服务，监听一个消息队列，然后数据源服务（商品信息管理服务）发生了数据变更之后， 就将数据变更的消息推送到消息队列中 缓存数据生产服务可以去消费到这个数据变更的消息，然后根据消息的指示提取一些参数， 然后调用对应的数据源服务的接口拉取数据，这个时候一般是从 mysql 库中拉去的 消息队列这里采用的是 kafka，这里选择 kafka 另外一个原因： 后面我们还要用 zookeeper 来解决缓存的分布式并发更新的问题（如分布式锁解决） 而 kafka 集群是依赖 zookeeper 集群，所以先搭建 zookeeper 集群，再搭建 kafka 集群 zookeeper + kafka 的集群，都至少是三节点 老师旁白 我工作的时候，很多项目是跟大数据相关的，当然也有很多是纯 java 系统的架构，最近用 kafka 用得比较多 kafka 比较简单易用，讲课来说，很方便 解释一下，我们当然是不可能对课程中涉及的各种技术都深入浅出的讲解的了，比如 kafka，花上 20个小时给你讲解一下，不可能的 所以说呢，在这里，一些技术的组合，用什么都 ok 笑傲江湖中的风清扬，手中无剑胜有剑，还有任何东西都可以当做兵器，哪怕是一根草也可以 搞技术，kafka 和 activemq 肯定有区别，但是说，在有些场景下，其实可能没有那么大的区分度，kafka 和 activemq 其实是一样的 生产者+消费者的场景，kafka + activemq 都 ok 涉及的这种架构，对时效性要求高和时效性要求低的数据，分别采取什么技术方案？数据库+缓存双写一致性？异步+多级缓存架构？大缓存的维度化拆分？ 你要关注的，是一些架构上的东西和思想，而不是具体的什么 mq 的使用 作为一名架构师，需要宏观的思考，通盘去考虑整个架构，还有未来的技术规划，业务的发展方向， 架构的演进方向和路线，站在一个 java 高级工程师的角度来思考的话，就是这么去落地实现， 这个侧重点不一样 把课程里讲解的各种技术方案组合成、修改成你需要的适合你的业务的缓存架构 ::: warning 建议不要跟着老师的版本走，如果你一开始所有的版本都跟着走的话，就可以 否则，请安装最新版的 kafka，因为 kafka 的客户端与 linux 服务端通信的兼容性问题很麻烦， 且老版本的配置在 boot 2.x 中不兼容，如果手工安装的话（也就是本笔记跟着老师视频走的）， 会出现 debug 这个消费者的时候，debug 异常，不知道什么原因 ::: zookeeper 集群安装 版本：zookeeper-3.4.5.tar.gz 安装机器：eshop-01、eshop-02、eshop-03 cd /usr/local wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.5/zookeeper-3.4.5.tar.gz tar -zxvf zookeeper-3.4.5.tar.gz # 配置环境变量，原来 .bashrc 才是环境变量的配置处 vi ~/.bashrc # 内容新增 export ZOOKEEPER_HOME=/usr/local/zookeeper-3.4.5 export PATH=$PATH:$ZOOKEEPER_HOME/bin # 刷新环境变量配置 source ~/.bashrc # 修改配置文件 cd /usr/local/zookeeper-3.4.5/conf cp zoo_sample.cfg zoo.cfg vi zoo.cfg # 涉及到的内容如下 dataDir=/usr/local/zookeeper-3.4.5/data server.0=eshop-cache01:2888:3888 server.1=eshop-cache02:2888:3888 server.2=eshop-cache03:2888:3888 # data mkdir /usr/local/zookeeper-3.4.5/data # 编辑集群 id，给一个 0，从 0 开始起 # vi 一个不存在的文件，保存后就存在了 cd /usr/local/zookeeper-3.4.5/data vi myid # 一台搞定之后，使用 scp 命令把环境变量和 zookper 复制到 02 和 03 上 # scp --help 中查看支持同时 scp 到多个机器，但是这里尝试一部分成功，一部分失败，所以分开 scp ~/.bashrc root@eshop-cache02:~/ scp ~/.bashrc root@eshop-cache03:~/ scp -r /usr/local/zookeeper-3.4.5 root@eshop-cache02:/usr/local scp -r /usr/local/zookeeper-3.4.5 root@eshop-cache03:/usr/local 复制之后，需要修改的地方有：myid ，分别修改为 1 和 2，唯一即可 zookper 启动与检查；三台机器都执行操作 # 由于之前配置了环境变量，这里可以任意目录执行 zkServer.sh start # 检查 ZooKeeper 状态：一个 leader，两个 follower zkServer.sh status # jps：检查三个节点是否都有 QuromPeerMain 进程 jps scala 安装 kafka 依赖 scala ，所以需要先安装 cd /usr/local wget https://downloads.lightbend.com/scala/2.11.12/scala-2.11.12.tgz tar -zxvf scala-2.11.12.tgz # 配置环境变量，原来 .bashrc 才是环境变量的配置处 vi ~/.bashrc # 内容新增 export SCALA_HOME=/usr/local/scala-2.11.12 export PATH=$PATH:$SCALA_HOME/bin # 刷新环境变量配置 source ~/.bashrc # 查看是否已经配置好 scala -version scp ~/.bashrc root@eshop-cache02:~/ scp ~/.bashrc root@eshop-cache03:~/ scp -r /usr/local/scala-2.11.12 root@eshop-cache02:/usr/local/ scp -r /usr/local/scala-2.11.12 root@eshop-cache03:/usr/local/ kafka 集群 版本：kafka_2.9.2-0.8.1.1.tgz，该版本是 2014 年发布的，很老的一个版本了 cd /usr/local wget https://archive.apache.org/dist/kafka/0.8.1.1/kafka_2.9.2-0.8.1.1.tgz tar -zxvf kafka_2.9.2-0.8.1.1.tgz # 配置环境变量，原来 .bashrc 才是环境变量的配置处 vi ~/.bashrc # 内容新增 export KAFKA_HOME=/usr/local/kafka_2.9.2-0.8.1.1 export PATH=$PATH:$KAFKA_HOME/bin # 刷新环境变量配置 source ~/.bashrc 配置 kafka vi /usr/local/kafka_2.9.2-0.8.1.1/config/server.properties # 涉及到的内容如下 broker.id # 类似 myid。每台机器唯一即可 zookeeper.connect=192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181 安装 slf4j 日志 slf4j-1.7.6.zip 包解压后 slf4j-nop-1.7.6.jar 包放在 kafka/libs 目录下 解决 kafka 启动 报错 Unrecognized VM option 'UseCompressedOops' 问题 vi /usr/local/kafka_2.9.2-0.8.1.1/bin/kafka-run-class.sh # 去掉 -XX:+UseCompressedOops 即可 if [ -z \"$KAFKA_JVM_PERFORMANCE_OPTS\" ]; then KAFKA_JVM_PERFORMANCE_OPTS=\"-server -XX:+UseCompressedOops -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSClassUnloadingEnabled -XX:+CMSScavengeBeforeRemark -XX:+DisableExplicitGC -Djava.awt.headless=true\" fi 启动命令 cd /usr/local/kafka_2.9.2-0.8.1.1 nohup bin/kafka-server-start.sh config/server.properties & # 查看是否启动是否报错 cat nohup.out 单机启动没有问题后，可以按照之前的方法把环境变量和 kafka 目录 copy 到另外两台机器 scp ~/.bashrc root@eshop-cache02:~/ scp ~/.bashrc root@eshop-cache03:~/ scp -r /usr/local/kafka_2.9.2-0.8.1.1 root@eshop-cache02:/usr/local/ scp -r /usr/local/kafka_2.9.2-0.8.1.1 root@eshop-cache03:/usr/local/ # 记得修改每台机器的 broker.id 。修改之后才能启动成功 检查集群状态 使用 jps 检查启动是否成功 使用基本命令检查kafka是否搭建成功 cd /usr/local/kafka_2.9.2-0.8.1.1 # 创建测试的 topic，名称为 test bin/kafka-topics.sh --zookeeper 192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181 --topic test --replication-factor 1 --partitions 1 --create # 生产者和消费者是一个命令行互动程序，所以需要开启两个终端连接 # 在生产者程序中输入字符串发送后，会在消费者上显示出来 # 创建一个生产者 bin/kafka-console-producer.sh --broker-list 192.168.99.170:9092,192.168.99.171:9092,192.168.99.172:9092 --topic test # 创建一个消费者 bin/kafka-console-consumer.sh --zookeeper 192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181 --topic test --from-beginning 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"050.html":{"url":"050.html","title":"050. 基于 kafka + ehcache + redis 完成缓存数据生产服务的开发与测试","keywords":"","body":" 050. 基于 kafka + ehcache + redis 完成缓存数据生产服务的开发与测试 将 kafka 整合到 spring boot 中 编写业务逻辑 测试业务逻辑 050. 基于 kafka + ehcache + redis 完成缓存数据生产服务的开发与测试 将 kafka 整合到 spring boot 中 这里的整合是 boot 1.x 的使用方式，也就是手工整合，由于我第一次接触 kafka， 就跟着老师的脚步走了。 增加依赖，需要与服务器上一样的版本，否则可能会有问题 compile 'org.apache.kafka:kafka_2.9.2:0.8.1.1' 创建消费者 import kafka.consumer.Consumer; import kafka.consumer.ConsumerConfig; import kafka.consumer.KafkaStream; import kafka.javaapi.consumer.ConsumerConnector; /** * * @author : zhuqiang * @date : 2019/4/7 16:25 */ public class KafkaConcusmer implements Runnable { private final ConsumerConnector consumer; private final String topic; public KafkaConcusmer(String topic, CacheService cacheService) { consumer = Consumer.createJavaConsumerConnector(createConsumerConfig( \"192.168.99.170:2181,\" + \"192.168.99.171:2181,\" + \"192.168.99.172:2181\", \"eshop-cache-group\")); this.topic = topic; this.cacheService = cacheService; } private static ConsumerConfig createConsumerConfig(String a_zookeeper, String a_groupId) { Properties props = new Properties(); props.put(\"zookeeper.connect\", a_zookeeper); props.put(\"group.id\", a_groupId); props.put(\"zookeeper.session.timeout.ms\", \"40000\"); props.put(\"zookeeper.sync.time.ms\", \"200\"); props.put(\"auto.commit.interval.ms\", \"1000\"); return new ConsumerConfig(props); } @Override public void run() { Map topicCountMap = new HashMap<>(); topicCountMap.put(topic, 1); Map>> consumerMap = consumer.createMessageStreams(topicCountMap); List> streams = consumerMap.get(topic); for (final KafkaStream stream : streams) { new Thread(new KafkaMessageProcessor(stream)).start(); } } } 消费者处理线程 import kafka.consumer.ConsumerIterator; import kafka.consumer.KafkaStream; /** * @author : zhuqiang * @date : 2019/4/7 16:29 */ public class KafkaMessageProcessor implements Runnable { private final KafkaStream kafkaStream; public KafkaMessageProcessor(KafkaStream kafkaStream) { this.kafkaStream = kafkaStream; } public void run() { ConsumerIterator it = kafkaStream.iterator(); while (it.hasNext()) { String message = new String(it.next().message()); System.out.println(message); } } } 项目启动的时候开始这消费者线程 @Component public class KafkaInit implements ApplicationRunner { @Override public void run(ApplicationArguments args) throws Exception { new Thread(new KafkaConcusmer(\"eshop-message\")).start(); } } 以上代码做的工作是：对接 kafka 消息队列，监听一个 topic。等待其他服务的修改事件 编写业务逻辑 两种服务会发送来数据变更消息：商品信息服务和商品店铺信息服务，每个消息都包含服务名以及商品 id 接收到消息之后，根据商品 id 到对应的服务拉取数据 这一步，我们采取简化的模拟方式，就是在代码里面写死，会获取到什么数据，不去实际再写其他的服务去调用了 商品信息：id、名称、价格、图片列表、商品规格、售后信息、颜色、尺寸 商品店铺信息：其他维度 用这个维度模拟出来缓存数据维度化拆分：id、店铺名称、店铺等级、店铺好评率 分别拉取到了数据之后，将数据组织成 json 串，然后分别存储到 ehcache 中和 redis 缓存中 这里的业务逻辑代码，因为是模拟这个场景，所以事先比较简单，重要的类如下 接收到事件之后，分别处理事件 public class KafkaMessageProcessor implements Runnable { private final KafkaStream kafkaStream; private final CacheService cacheService; private final Logger log = LoggerFactory.getLogger(getClass()); public KafkaMessageProcessor(KafkaStream kafkaStream, CacheService cacheService) { this.kafkaStream = kafkaStream; this.cacheService = cacheService; } public void run() { ConsumerIterator it = kafkaStream.iterator(); while (it.hasNext()) { String message = new String(it.next().message()); // 首先将message转换成json对象 JSONObject messageJSONObject = JSONObject.parseObject(message); // 从这里提取出消息对应的服务的标识 String serviceId = messageJSONObject.getString(\"serviceId\"); // 如果是商品信息服务 if (\"productInfoService\".equals(serviceId)) { processProductInfoChangeMessage(messageJSONObject); } else if (\"shopInfoService\".equals(serviceId)) { processShopInfoChangeMessage(messageJSONObject); } } } /** * 处理商品信息变更的消息 */ private void processProductInfoChangeMessage(JSONObject messageJSONObject) { // 提取出商品id Long productId = messageJSONObject.getLong(\"productId\"); // 调用商品信息服务的接口 // 直接用注释模拟：getProductInfo?productId=1，传递过去 // 商品信息服务，一般来说就会去查询数据库，去获取productId=1的商品信息，然后返回回来 String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1}\"; ProductInfo productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); cacheService.saveProductInfo2LocalCache(productInfo); log.info(\"获取刚保存到本地缓存的商品信息：\" + cacheService.getProductInfoFromLocalCache(productId)); cacheService.saveProductInfo2ReidsCache(productInfo); } /** * 处理店铺信息变更的消息 */ private void processShopInfoChangeMessage(JSONObject messageJSONObject) { // 提取出商品id Long productId = messageJSONObject.getLong(\"productId\"); Long shopId = messageJSONObject.getLong(\"shopId\"); // 这里也是模拟去数据库获取到了信息 String shopInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"小王的手机店\\\", \\\"level\\\": 5, \\\"goodCommentRate\\\":0.99}\"; ShopInfo shopInfo = JSONObject.parseObject(shopInfoJSON, ShopInfo.class); cacheService.saveShopInfo2LocalCache(shopInfo); log.info(\"获取刚保存到本地缓存的店铺信息：\" + cacheService.getShopInfoFromLocalCache(shopId)); cacheService.saveShopInfo2ReidsCache(shopInfo); } } 把缓存的读写封装了到 service 中。上面通过 service 去操作缓存 @Service public class CacheServiceImpl implements CacheService { public static final String CACHE_NAME = \"local\"; @Resource private JedisCluster jedisCluster; /** * 将商品信息保存到本地缓存中 */ @CachePut(value = CACHE_NAME, key = \"'key_'+#productInfo.getId()\") public ProductInfo saveLocalCache(ProductInfo productInfo) { return productInfo; } /** * 从本地缓存中获取商品信息 */ @Cacheable(value = CACHE_NAME, key = \"'key_'+#id\") public ProductInfo getLocalCache(Long id) { return null; } /** * 将商品信息保存到本地的ehcache缓存中 */ @CachePut(value = CACHE_NAME, key = \"'product_info_'+#productInfo.getId()\") public ProductInfo saveProductInfo2LocalCache(ProductInfo productInfo) { return productInfo; } /** * 从本地ehcache缓存中获取商品信息 */ @Cacheable(value = CACHE_NAME, key = \"'product_info_'+#productId\") public ProductInfo getProductInfoFromLocalCache(Long productId) { return null; } /** * 将店铺信息保存到本地的ehcache缓存中 */ @CachePut(value = CACHE_NAME, key = \"'shop_info_'+#shopInfo.getId()\") public ShopInfo saveShopInfo2LocalCache(ShopInfo shopInfo) { return shopInfo; } /** * 从本地ehcache缓存中获取店铺信息 */ @Cacheable(value = CACHE_NAME, key = \"'shop_info_'+#shopId\") public ShopInfo getShopInfoFromLocalCache(Long shopId) { return null; } /** * 将商品信息保存到redis中 */ public void saveProductInfo2ReidsCache(ProductInfo productInfo) { String key = \"product_info_\" + productInfo.getId(); jedisCluster.set(key, JSONObject.toJSONString(productInfo)); } /** * 将店铺信息保存到redis中 */ public void saveShopInfo2ReidsCache(ShopInfo shopInfo) { String key = \"shop_info_\" + shopInfo.getId(); jedisCluster.set(key, JSONObject.toJSONString(shopInfo)); } } 测试业务逻辑 命令行创建 topic 和 producer 可以参考前一章节 中的 检查集群状态 创建一个 kafka topic # 创建 topic ，需要和程序中的一致 ：eshop-message bin/kafka-topics.sh --zookeeper 192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181 --topic eshop-message --replication-factor 1 --partitions 1 --create 在命令行启动一个 kafka producer # 创建一个生产者 bin/kafka-console-producer.sh --broker-list 192.168.99.170:9092,192.168.99.171:9092,192.168.99.172:9092 --topic eshop-message 启动系统，消费者开始监听 kafka topic 注意：在 boot 2.1.x 中，连不上也没有日志打印。需要把几个虚拟机 hostname 映射到本地 C:\\Windows\\System32\\drivers\\etc\\hosts 192.168.99.170 eshop-cache01 192.168.99.171 eshop-cache02 192.168.99.172 eshop-cache03 192.168.99.173 eshop-cache04 在 producer 中，分别发送两条消息，一个是商品信息服务的消息，一个是商品店铺信息服务的消息 由于在本次模拟中，cn.mrcode.cachepdp.eshop.cache.kafka.KafkaMessageProcessor 只使用 serviceId 作为了判定，其他数据是程序中写死的，所以这里推送两条携带 serviceId 的信息即可 {\"serviceId\":\"productInfoService\",\"productId\":\"1\"} {\"serviceId\":\"shopInfoService\",\"shopId\":\"1\"} 能否接收到两条消息，并模拟拉取到两条数据，同时将数据写入 ehcache 中，并写入 redis 缓存中 ehcache 通过打印日志方式来观察，redis 通过手工连接上去来查询 ::: warning 由于 kafka 安装与使用感觉比较难，而且又是老版本的 kafka， 在 boot 2.x 中使用问题很多，不能 debug 等问题 虽然有效果了，但是不能 debug，感觉这个消费者的代码很薄弱 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"051.html":{"url":"051.html","title":"051. 基于“分发层 + 应用层”双层 nginx 架构提升缓存命中率方案分析","keywords":"","body":" 051. 基于“分发层 + 应用层”双层 nginx 架构提升缓存命中率方案分析 缓存命中率低 如何提升缓存命中率？ 051. 基于“分发层 + 应用层”双层 nginx 架构提升缓存命中率方案分析 缓存数据生产服务那一层已经搞定了，相当于三层缓存架构中的本地堆缓存 + redis 分布式缓存都搞定了 就要来做三级缓存中的 nginx 那一层的缓存了 缓存命中率低 如果一般来说，你默认会部署多个 nginx，在里面都会放一些缓存，就默认情况下，此时缓存命中率是比较低的 如上图，被均衡分发了，所以命中率很低。 如何提升缓存命中率？ 方案：分发层+应用层，双层 nginx 分发层 nginx，负责流量分发的逻辑和策略，这个里面它可以根据你自己定义的一些规则， 比如根据 productId 去进行 hash，然后对后端的 nginx 数量取模，将某一个商品的访问的请求， 就固定路由到一个 nginx 后端服务器上去，保证说只会从 redis 中获取一次缓存数据， 后面全都是走 nginx 本地缓存了 后端的 nginx 服务器，就称之为应用服务器; 最前端的 nginx 服务器，被称之为分发服务器 看似很简单，其实很有用，在实际的生产环境中，可以大幅度提升你的 nginx 本地缓存这一层的命中率， 大幅度减少 redis 后端的压力，提升性能 ::: tip 疑问 到现在我都有好多疑问，是基于自己使用 spring cloud 来看待这种路由到固定的机器上面去， 其实这就相当于有状态了，在缓冲架构里面他的确能解决缓存命中问题，但是在灵活性上， 增加服务，减少服务就有问题了。 这个疑问现在还不是特别的明确，后续再看吧 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"052.html":{"url":"052.html","title":"052. 基于 OpenResty 部署应用层 nginx 以及 nginx + lua 开发 hello world","keywords":"","body":" 052. 基于 OpenResty 部署应用层 nginx 以及 nginx + lua 开发 hello world 部署 openresty cd /usr/servers/ngx_openresty-1.7.7.2 nginx+lua 开发的 hello world 工程化的 nginx+lua 项目结构 搭建另外一个应用层 nginx 讲师旁白 052. 基于 OpenResty 部署应用层 nginx 以及 nginx + lua 开发 hello world 我们这里玩儿 nginx，全都会在 nginx 里去写 lua 脚本，因为我们需要自定义一些特殊的业务逻辑 比如说，流量分发，自己用 lua 去写分发的逻辑，在分发层 nginx 里去写的 再比如说，要用 lua 去写多级缓存架构存取的控制逻辑，在应用层 nginx 里去写的 后面还要做热点数据的自动降级机制，也是用 lua 脚本去写降级机制的，在分发层 nginx 里去写的 因为我们要用 nginx+lua 去开发，所以会选择用最流行的开源方案，就是用 OpenResty nginx+lua 打包在一起，而且提供了包括 redis 客户端，mysql 客户端，http 客户端在内的大量的组件 我们这一讲是去部署应用层 nginx，会采用 OpenResty 的方式去部署 nginx， 而且会带着大家写一个 nginx+lua 开发的一个 hello world 部署第一个 nginx，作为应用层 nginx，部署机器：eshop-01 部署 openresty mkdir -p /usr/servers cd /usr/servers/ # 安装依赖 yum install -y readline-devel pcre-devel openssl-devel gcc # 可能需要翻墙才能下载，所以可以离线下载好 wget http://openresty.org/download/ngx_openresty-1.7.7.2.tar.gz tar -xzvf ngx_openresty-1.7.7.2.tar.gz rm -rf ngx_openresty-1.7.7.2.tar.gz # 安装 lua 等相关组件 cd /usr/servers/ngx_openresty-1.7.7.2/ cd bundle/LuaJIT-2.1-20150120/ make clean && make && make install ln -sf luajit-2.1.0-alpha /usr/local/bin/luajit cd ../ wget https://github.com/FRiCKLE/ngx_cache_purge/archive/2.3.tar.gz tar -xvf 2.3.tar.gz wget https://github.com/yaoweibin/nginx_upstream_check_module/archive/v0.3.0.tar.gz tar -xvf v0.3.0.tar.gz # cd /usr/servers/ngx_openresty-1.7.7.2 ./configure --prefix=/usr/servers --with-http_realip_module --with-pcre --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2 make && make install # 会发现多了好多目录 [root@eshop-cache01 servers]# ll total 3316 drwxr-xr-x 2 root root 4096 Apr 1 23:38 bin drwxr-xr-x 6 root root 4096 Apr 1 23:38 luajit drwxr-xr-x 5 root root 4096 Apr 1 23:38 lualib drwxr-xr-x 6 root root 4096 Apr 1 23:38 nginx drwxrwxr-x 4 1000 1000 4096 Apr 1 23:35 ngx_openresty-1.7.7.2 # 启动nginx: /usr/servers/nginx/sbin/nginx nginx+lua 开发的 hello world vi /usr/servers/nginx/conf/nginx.conf # 在 http 部分添加如下内容 lua_package_path \"/usr/servers/lualib/?.lua;;\"; lua_package_cpath \"/usr/servers/lualib/?.so;;\"; 如下位置 http { lua_package_path \"/usr/servers/lualib/?.lua;;\"; lua_package_cpath \"/usr/servers/lualib/?.so;;\"; include mime.types; default_type application/octet-stream; /usr/servers/nginx/conf 下，创建一个 lua.conf，内容如下 server { listen 80; server_name _; } 接着在 nginx.conf 的 http 部分添加：include lua.conf; 验证配置是否正确： [root@eshop-cache01 conf]# /usr/servers/nginx/sbin/nginx -t nginx: the configuration file /usr/servers/nginx/conf/nginx.conf syntax is ok nginx: configuration file /usr/servers/nginx/conf/nginx.conf test is successful 在 lua.conf 的 server 部分添加： location /lua { default_type 'text/html'; content_by_lua 'ngx.say(\"hello world\")'; } 添加完成后，再次测试验证是否正确：/usr/servers/nginx/sbin/nginx -t 重新加载 nginx 配置: /usr/servers/nginx/sbin/nginx -s reload 访问 http://192.168.99.170/lua 会看到页面输出 「hello world」 这里把嵌入配置文件的代码迁移到独立文件中 mkdir -p /usr/servers/nginx/conf/lua vi /usr/servers/nginx/conf/lua/test.lua # 在文件中写入迁入的代码 ngx.say(\"hello world\"); # 并修改 lua.conf ，配置成 file 方式 location /lua { default_type 'text/html'; content_by_lua_file conf/lua/test.lua; } 查看异常日志 : tail -f /usr/servers/nginx/logs/error.log 工程化的 nginx+lua 项目结构 项目工程结构 |- hello # 项目名 |- hello.conf |- lua |- hello.lua |- lualib # 该包的文件可以在之前的 /usr/servers/lualib 全部 copy 过来 |- *.lua |- *.so 放在 /usr/hello 目录下 搭建另外一个应用层 nginx 如法炮制，在另外一个机器上，也用 OpenResty 部署一个 nginx 部署机器：eshop-02 这次的 hello word 就按照工程化的目录来放置和配置 mkdir /usr/hello vi hello.conf # 内容如下 server { listen 80; server_name _; location /lua { default_type 'text/html'; content_by_lua_file /usr/hello/lua/hello.lua; } } mkdir lua vi lua/hello.lua # 内容如下 ngx.say(\"hello world,工程化结构\") # 复制 lualib scp -r /usr/servers/lualib/ ./ # 编辑 nginx.conf vi /usr/servers/nginx/conf/nginx.conf # 添加内容 http { lua_package_path \"/usr/hello/lualib/?.lua;;\"; lua_package_cpath \"/usr/hello/lualib/?.so;;\"; include /usr/hello/hello.conf; include mime.types; # 测试是否成功 /usr/servers/nginx/sbin/nginx -t # 重新加载 nginx 配置 /usr/servers/nginx/sbin/nginx -s reload 浏览器访问 http://eshop-cache02/lua 显示出信息，成功 讲师旁白 对于我的课程来说，主要还是关注我们核心的 topic：大型缓存的架构 那么对于课程里涉及到的各种技术来说，比如 nginx，lua 脚本，你说让我给你讲成 nginx从入门到精通，也不太现实; 讲一个 lua 脚本开发从入门到精通，也不太现实 我只能说，跟着整个项目的思路去走，把项目里涉及的相关技术的知识给你讲解一下，然后保证说，带着你手把手的去做，让你至少可以学会项目里讲解的这些知识，可以做出来 如果你后面真的是要自己去用 nginx+lua 去做项目，其实个人建议你还是得去查询和学习一些更多的资料，比如：nginx 的一些知识、lua 的一些语法 如龙果里面最受欢迎的一套课程，就是 dubbo 实战课程，里面也是 dubbo 整合了各种技术：active mq、zookeeper、redis 3.0 分布式集群、mysql读写分离 但是有个问题，每个课程，我相信一个好的课程，它总是可以让你学到很多知识的 但是任何一个好的课程，它都不是万能的，比如 dubbo 这个课程中，你可能能学习到 zookeeper 怎么当注册中心，但是 zookeeper 分布式锁、分布式协调、分布式选举等等技术，你能学到吗？ dubbo 它也不可能说是给你把 zookeeper、redis、mysql 全部讲解到从入门到精通这样子 topic 主题，基于 dubbo 复杂的分布式系统的通用架构，分布式系统，dubbo rpc 的调用，服务的开发; zookeeper 做注册中心; redis 分布式集群; mysql 读写分离; tomcat 集群; hudson 持续集成 它告诉你的是一个通用的分布式系统的架构 我这里的也会带着你做 nginx 的部署、openresty、nginx+lua 的开发，redis 集群/高可用/高并发/读写分离/持久化/数据备份恢复、zookeeper 分布式锁、kafka 去做消息通信，hystrix 去做限流 但是任何一个技术都不可能给你从入门到精通讲解完 大家可以去关注一下我的 es 的课程，你如果录一套课程，基本选择方向就两个，要不就是讲解技术本身，大量案例实战贯穿，把技术本身讲解的很细致 我之前有两个 es 顶尖高手系列的课程属于技术课程，把 es 这个技术讲解的非常非常的细致 像我们现在这个课程，大规模缓存、支撑高并发、高性能、海量数据，类似之前 dubbo 实战课程，它讲解的还是一种架构课程，那么就关注点在整个架构的整体、整合、架构方案、架构设计、架构思想 里面涉及的技术是不可能给你去深入讲解的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"053.html":{"url":"053.html","title":"053. 部署分发层 nginx 以及基于 lua 完成基于商品 id 的定向流量分发策略","keywords":"","body":"053. 部署分发层 nginx 以及基于 lua 完成基于商品 id 的定向流量分发策略 eshop-03 上也需要搭建一个 应用层 nginx，参考上一章节 nginx 三台的作用： eshop-01：应用层 eshop-02：应用层 eshop-03：分发层 在 eshop-cache03 中编写 lua 脚本，完成基于商品 id 的流量分发策略 这里简化业务逻辑，实际上在你的公司中，你可以随意根据自己的业务逻辑和场景，去制定自己的流量分发策略 步骤如下： 获取请求参数，比如 productId 对 productId 进行 hash hash 值对应用服务器数量取模，获取到一个应用服务器 利用 http 发送请求到应用层 nginx 获取响应后返回 为了能分发看出来效果，我们把之前 hello 输出信息修改为自己的主机名，如 hello world,eshop-cache03 使用 lua 转发需要用到 lua 的 http 包，这里导入下 cd /usr/hello/lualib/resty/ wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua 脚本编写 /usr/hello/lua/hello.lua -- 拿一个地址来说明：http://eshop-cache03/lua?method=hello&productId=1 -- 获取问号后面的参数列表 local uri_args = ngx.req.get_uri_args() -- 获取参数 local productId = uri_args[\"productId\"] -- 定义后端应用 ip local host = {\"192.168.99.170\", \"192.168.99.171\"} -- 对商品 id 取模并计算 hash 值 local hash = ngx.crc32_long(productId) hash = (hash % 2) + 1 -- 拼接 http 前缀 backend = \"http://\"..host[hash] -- 获取到参数中的路径，比如你要访问 /hello，这个例子中是需要传递访问路径的 local method = uri_args[\"method\"] -- 拼接具体的访问地址不带 host，如：/hello?productId=1 local requestBody = \"/\"..method..\"?productId=\"..productId -- 获取 http 包 local http = require(\"resty.http\") local httpc = http.new() -- 访问，这里有疑问：万一有 cooke 这些脚本支持吗？会很麻烦吗？ local resp, err = httpc:request_uri(backend, { method = \"GET\", path = requestBody, keepalive=false }) -- 如果没有响应则输出一个 err 信息 if not resp then ngx.say(\"request error :\", err) return end -- 有响应测输出响应信息 ngx.say(resp.body) -- 关闭 http 客户端实例 httpc:close() 刷新配置：/usr/servers/nginx/sbin/nginx -s reload 访问 这里的意思是，访问 03 这台分发 nginx，但是需要告诉它我访问后端哪个服务路径 由于后端两个 nginx 都只有 /lua 这个请求路径，所以就访问了它 http://eshop-cache03/lua?method=lua&productId=1 页面输出：hello world,eshop-cache02 http://eshop-cache03/lua?method=lua&productId=4 页面输出：hello world,eshop-cache01 基于商品 id 的定向流量分发策略的 lua 脚本就开发完了，而且也测试过了 我们就可以看到，如果你请求的是固定的某一个商品，那么就一定会将流量打到固定的一个应用 nginx 上面去 ::: tip 如果访问报错，可以查看 nginx 的错误日志：cat /usr/servers/nginx/logs/error.log ::: 一开始访问报错，看错误日志看到的，百度了下解决了在请求的时候增加了 keepalive=false 参数 2019/04/02 02:00:33 [error] 8451#0: *16 lua entry thread aborted: runtime error: /usr/hello/lualib/resty/http.lua:926: bad argument #2 to 'set_keepalive' (number expected, got nil) stack traceback: coroutine 0: [C]: in function 'set_keepalive' /usr/hello/lualib/resty/http.lua:926: in function 'request_uri' /usr/hello/lua/hello.lua:25: in function , client: 192.168.99.111, server: _, request: \"GET /lua?method=lua&productId=1 HTTP/1.1\", host: \"eshop-cache03\" 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"054.html":{"url":"054.html","title":"054. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（一）","keywords":"","body":" 054. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（一） 分发层 lua 应用层 nginx 错误解决 测试 054. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（一） 本节讲解如何自 nginx 中写 lua 脚本把缓存放到 nginx 本地缓存中 脚本思路： 应用 nginx 的 lua 脚本接收到请求 获取请求参数中的商品 id，以及商品店铺 id 根据商品 id 和商品店铺 id，在 nginx 本地缓存中尝试获取数据 如果在 nginx 本地缓存中没有获取到数据，那么就到 redis 分布式缓存中获取数据，如果获取到了数据，还要设置到 nginx 本地缓存中 但是这里有个问题，建议不要用 nginx+lua 直接去获取 redis 数据 因为 openresty 没有太好的 redis cluster 的支持包，所以建议是发送 http 请求到缓存数据生产服务，由该服务提供一个 http 接口 缓存数生产服务可以基于 redis cluster api 从 redis 中直接获取数据，并返回给 nginx 如果缓存数据生产服务没有在 redis 分布式缓存中没有获取到数据，那么就在自己本地 ehcache 中获取数据，返回数据给 nginx，也要设置到 nginx 本地缓存中 如果 ehcache 本地缓存都没有数据，那么就需要去原始的服务中拉取数据，该服务会从 mysql 中查询，拉取到数据之后，返回给 nginx，并重新设置到 ehcache和 redis 中 这里先不考虑并发问题，后面要专门讲解一套分布式缓存重建并发冲突的问题和解决方案 nginx 最终利用获取到的数据，动态渲染网页模板 将渲染后的网页模板作为 http 响应，返回给分发层 nginx 下面来一步一步做； 分发层 lua eshop-cache03 服务器上 /usr/hello/hello.conf server { listen 80; server_name _; location /lua { default_type 'text/html'; # 防止响应中文乱码 charset utf-8; content_by_lua_file /usr/hello/lua/hello.lua; } # 分发逻辑脚本映射 location /product { default_type 'text/html'; # 防止响应中文乱码 charset utf-8; content_by_lua_file /usr/hello/lua/distribute.lua; } } /usr/hello/lua/distribute.lua，这里使用之前写好的分发逻辑修改， 因为想在一个映射中写完商品和店铺信息的分发，所以这里还需要添加一个 shopId local uri_args = ngx.req.get_uri_args() -- 获取参数 local productId = uri_args[\"productId\"] local shopId = uri_args[\"shopId\"] -- 定义后端应用 ip local host = {\"192.168.99.170\", \"192.168.99.171\"} -- 对商品 id 取模并计算 hash 值 local hash = ngx.crc32_long(productId) hash = (hash % 2) + 1 -- 拼接 http 前缀 backend = \"http://\"..host[hash] -- 获取到参数中的路径，比如你要访问 /hello，这个例子中是需要传递访问路径的 local method = uri_args[\"method\"] -- 拼接具体的访问地址不带 host，如：/hello?productId=1 local requestBody = \"/\"..method..\"?productId=\"..productId..\"&shopId=\"..shopId -- 获取 http 包 local http = require(\"resty.http\") local httpc = http.new() -- 访问，这里有疑问：万一有 cooke 这些脚本支持吗？会很麻烦吗？ local resp, err = httpc:request_uri(backend, { method = \"GET\", path = requestBody, keepalive=false }) -- 如果没有响应则输出一个 err 信息 if not resp then ngx.say(\"request error :\", err) return end -- 有响应测输出响应信息 ngx.say(resp.body) -- 关闭 http 客户端实例 httpc:close() 应用层 nginx 应用层在 eshop-cache01 和 eshop-cache02 上。 这里可以再 01 上写完逻辑，然后再 copy 到 02 上。 先安装依赖： # 需要再后端服务获取信息，安装 http 依赖 cd /usr/hello/lualib/resty/ wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http_headers.lua wget https://raw.githubusercontent.com/pintsized/lua-resty-http/master/lib/resty/http.lua # 拿到数据之后需要进行模板渲染，添加 template 依赖 # 这里渲染也是使用 lua 来完成 cd /usr/hello/lualib/resty/ wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template.lua mkdir /usr/hello/lualib/resty/html cd /usr/hello/lualib/resty/html wget https://raw.githubusercontent.com/bungle/lua-resty-template/master/lib/resty/template/html.lua /usr/hello/hello.conf # 配置 lua 的一个缓存实例，my_cache 是我们自定义的一块缓存名称 # 要配置在 http 中，server 外，否则会报错 # nginx: [emerg] \"lua_shared_dict\" directive is not allowed here in /usr/hello/hello.conf:11 lua_shared_dict my_cache 128m; server { listen 80; server_name _; # 配置模板路径 set $template_location \"/templates\"; # 当然这个路径需要存在，因为后续需要用来存放 html set $template_root \"/usr/hello/templates\"; location /lua { default_type 'text/html'; # 防止响应中文乱码 charset utf-8; content_by_lua_file /usr/hello/lua/hello.lua; } # 配置一个脚本映射，访问 product 的时候 # 就执行 product.lua 脚本来完成 获取缓存渲染 html 并返回 html 的功能 location /product { default_type 'text/html'; # 防止响应中文乱码 charset utf-8; content_by_lua_file /usr/hello/lua/product.lua; } } /usr/hello/lua/product.lua local uri_args = ngx.req.get_uri_args() local productId = uri_args[\"productId\"] local shopId = uri_args[\"shopId\"] -- 获取到之前配置中分配的缓存对象 local cache_ngx = ngx.shared.my_cache -- 拼接两个缓存 key local productCacheKey = \"product_info_\"..productId local shopCacheKey = \"shop_info_\"..shopId -- 通过缓存对象获取缓存中的 value local productCache = cache_ngx:get(productCacheKey) local shopCache = cache_ngx:get(shopCacheKey) -- 如果缓存中不存在对于的 value -- 就走后端缓存服务获取数据（缓存服务先走 redis ，不存在再走 ehcache，再走数据库） if productCache == \"\" or productCache == nil then local http = require(\"resty.http\") local httpc = http.new() -- 这里地址是开发机器 ip，因为我们在 windows 上开发的， -- 这里直接访问开发环境比较方便 local resp, err = httpc:request_uri(\"http://192.168.99.111:6002\",{ method = \"GET\", path = \"/getProductInfo?productId=\"..productId, keepalive=false }) productCache = resp.body -- 获取到之后，再设置到缓存中 cache_ngx:set(productCacheKey, productCache, 10 * 60) end if shopCache == \"\" or shopCache == nil then local http = require(\"resty.http\") local httpc = http.new() local resp, err = httpc:request_uri(\"http://192.168.99.111:6002\",{ method = \"GET\", path = \"/getShopInfo?shopId=\"..shopId, keepalive=false }) shopCache = resp.body cache_ngx:set(shopCacheKey, shopCache, 10 * 60) end -- 因为存到缓存中是一个字符串 -- 所以使用 cjson 库把字符串转成 json 对象 local cjson = require(\"cjson\") local productCacheJSON = cjson.decode(productCache) local shopCacheJSON = cjson.decode(shopCache) -- 把商品信息和店铺信息拼接到一个大 json 对象中 -- 这样做的原因是：template 渲染需要这样做 local context = { productId = productCacheJSON.id, productName = productCacheJSON.name, productPrice = productCacheJSON.price, productPictureList = productCacheJSON.pictureList, productSpecification = productCacheJSON.specification, productService = productCacheJSON.service, productColor = productCacheJSON.color, productSize = productCacheJSON.size, shopId = shopCacheJSON.id, shopName = shopCacheJSON.name, shopLevel = shopCacheJSON.level, shopGoodCommentRate = shopCacheJSON.goodCommentRate } -- 使用 template 渲染 product.html 模板 local template = require(\"resty.template\") template.render(\"product.html\", context) product.html 内容，就是很简单的插值占位 product id: {* productId *} product name: {* productName *} product picture list: {* productPictureList *} product specification: {* productSpecification *} product service: {* productService *} product color: {* productColor *} product size: {* productSize *} shop id: {* shopId *} shop name: {* shopName *} shop level: {* shopLevel *} shop good cooment rate: {* shopGoodCommentRate *} 配置完成后，记得测试配置文件和重启 nginx /usr/servers/nginx/sbin/nginx -t /usr/servers/nginx/sbin/nginx -s reload 错误解决 如果报错 product.lua:20: module 'resty.http' not found: 那么请检查 vi /usr/servers/nginx/conf/nginx.conf 这里引入的内容是否是 hello 目录下的。 http { lua_package_path \"/usr/hello/lualib/?.lua;;\"; lua_package_cpath \"/usr/hello/lualib/?.so;;\"; 测试 访问地址：http://eshop-cache02/product?productId=1&shopId=1 肯定会报错，因为后端服务都没有写的。但是可以看看日志报错信息 tail -f /usr/servers/nginx/logs/error.log 可以看到如下的错误： 2019/05/06 22:19:10 [error] 8758#0: *34 connect() failed (113: No route to host), client: 192.168.99.1, server: _, request: \"GET /product?productId=1&shopId=1 HTTP/1.1\", host: \"eshop-cache02\" 2019/05/06 22:19:10 [error] 8758#0: *34 lua entry thread aborted: runtime error: /usr/hello/lua/product.lua:29: attempt to index local 'resp' (a nil value) stack traceback: coroutine 0: /usr/hello/lua/product.lua: in function , client: 192.168.99.1, server: _, request: \"GET /product?productId=1&shopId=1 HTTP/1.1\", host: \"eshop-cache02\" 看到去访问后端服务了，没有返回信息。下一章继续后端服务的编写 ::: tip 由于搬家，宿主机 IP 变更了，但是虚拟机上已经安装了好多软件。 没有耐心再修改一次 IP 了，所以改用了 hostonly 来保证虚拟机和宿主机的联通，并且虚拟机可以上网。设置方法在这篇文章中； 所以后续对宿主机的访问，会更改 IP ，与这之前的笔记中看到的 IP 会不一致 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"055.html":{"url":"055.html","title":"055. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（二）","keywords":"","body":" 055. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（二） 缓存服务实现 测试应用层 优化模板文件 测试分发层 055. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（二） 缓存服务实现 cn.mrcode.cachepdp.eshop.cache.controller.CacheController /** * 这里的代码别看着奇怪，简单回顾下之前的流程： 1. nginx 获取 redis 缓存 2. 获取不到再获取服务的堆缓存（也就是这里的 ecache） 3. * 还获取不到就需要去数据库获取并重建缓存 */ @RequestMapping(\"/getProductInfo\") @ResponseBody public ProductInfo getProductInfo(Long productId) { ProductInfo productInfo = cacheService.getProductInfoOfReidsCache(productId); log.info(\"从 redis 中获取商品信息\"); if (productInfo == null) { productInfo = cacheService.getProductInfoFromLocalCache(productId); log.info(\"从 ehcache 中获取商品信息\"); } if (productInfo == null) { // 两级缓存中都获取不到数据，那么就需要从数据源重新拉取数据，重建缓存 // 但是这里暂时不讲 log.info(\"缓存重建 商品信息\"); } return productInfo; } @RequestMapping(\"/getShopInfo\") @ResponseBody public ShopInfo getShopInfo(Long shopId) { ShopInfo shopInfo = cacheService.getShopInfoOfReidsCache(shopId); log.info(\"从 redis 中获取店铺信息\"); if (shopInfo == null) { shopInfo = cacheService.getShopInfoFromLocalCache(shopId); log.info(\"从 ehcache 中获取店铺信息\"); } if (shopInfo == null) { // 两级缓存中都获取不到数据，那么就需要从数据源重新拉取数据，重建缓存 // 但是这里暂时不讲 log.info(\"缓存重建 店铺信息\"); } return shopInfo; } cn.mrcode.cachepdp.eshop.cache.service.CacheService /** * 从 redis 中获取商品 */ ProductInfo getProductInfoOfReidsCache(Long productId); /** * 从 redis 中获取店铺信息 */ ShopInfo getShopInfoOfReidsCache(Long shopId); cn.mrcode.cachepdp.eshop.cache.service.impl.CacheServiceImpl @Override public ProductInfo getProductInfoOfReidsCache(Long productId) { String key = \"product_info_\" + productId; String json = jedisCluster.get(key); return JSON.parseObject(json, ProductInfo.class); } @Override public ShopInfo getShopInfoOfReidsCache(Long shopId) { String key = \"shop_info_\" + shopId; String json = jedisCluster.get(key); return JSON.parseObject(json, ShopInfo.class); } 测试应用层 访问地址：http://eshop-cache02/product?productId=1&shopId=1 tail -f /usr/servers/nginx/logs/error.log 可以看到如下的错误： 2019/05/06 22:46:59 [error] 8834#0: *46 lua entry thread aborted: runtime error: /usr/hello/lualib/resty/http.lua:929: bad argument #2 to 'set_keepalive' (number expected, got nil) stack traceback: coroutine 0: [C]: in function 'set_keepalive' /usr/hello/lualib/resty/http.lua:929: in function 'request_uri' /usr/hello/lua/product.lua:24: in function , client: 192.168.99.1, server: _, request: \"GET /product?productId=1&shopId=1 HTTP/1.1\", host: \"eshop-cache02\" 这次通过 debug 后端服务，服务中能请求到了，响应之后报错 bad argument #2 to 'set_keepalive' 这个问题在前面记忆中已经解决过了，设置下 keepalive=false 即可 local resp, err = httpc:request_uri(\"http://192.168.99.111:6002\",{ method = \"GET\", path = \"/getShopInfo?shopId=\"..shopId, keepalive=false }) 再次访问：http://eshop-cache02/product?productId=1&shopId=1 product id: 1 product name: iphone7手机 product picture list: a.jpg,b.jpg product specification: iphone7的规格 product service: iphone7的售后服务 product color: 红色,白色,黑色 product size: 5.5 shop id: 1 shop name: 小王的手机店 shop level: 5 shop good cooment rate: 0.99 如果响应的中文乱码，需要在拦截的地方添加编码 location /product { default_type 'text/html'; charset utf-8; content_by_lua_file /usr/hello/lua/product.lua; } 优化模板文件 vi /usr/hello/templates/product.html 商品详情页 商品 ID: {* productId *} 商品名称: {* productName *} 商品图片列表: {* productPictureList *} 商品规格: {* productSpecification *} 商品售后服务: {* productService *} 商品颜色: {* productColor *} 商品尺寸: {* productSize *} 店铺 ID: {* shopId *} 店铺名称: {* shopName *} 店铺级别: {* shopLevel *} 店铺评分: {* shopGoodCommentRate *} 记得重启 /usr/servers/nginx/sbin/nginx -s reload 再次访问：http://eshop-cache02/product?productId=1&shopId=1 商品 ID: 1 商品名称: iphone7手机 商品图片列表: a.jpg,b.jpg 商品规格: iphone7的规格 商品售后服务: iphone7的售后服务 商品颜色: 红色,白色,黑色 商品尺寸: 5.5 店铺 ID: 1 店铺名称: 小王的手机店 店铺级别: 5 店铺评分: 0.99 测试分发层 刚刚应用层已经测试通过，现在来从分发层测试 访问：http://eshop-cache03/product?method=product&productId=1&shopId=1 ::: tip 注意，由于使用的是 hash 分发，可以在 eshop-cache01 和 eshop-cache02 上显示访问日志 tail -f /usr/servers/nginx/logs/access.log 这样就能看到被分发到哪台机器上去了。 ::: 成功响应 html 信息 商品 ID: 1 商品名称: iphone7手机 商品图片列表: a.jpg,b.jpg 商品规格: iphone7的规格 商品售后服务: iphone7的售后服务 商品颜色: 红色,白色,黑色 商品尺寸: 5.5 店铺 ID: 1 店铺名称: 小王的手机店 店铺级别: 5 店铺评分: 0.99 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"056.html":{"url":"056.html","title":"056. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（三）","keywords":"","body":"056. 基于 nginx + lua + java 完成多级缓存架构的核心业务逻辑（三） 前面章节已经测试通了流程，但是漏掉了一个核心：没有检测 nginx 本地缓存是否有效 这里来多次访问：http://eshop-cache03/product?method=product&productId=1&shopId=1, 并观察后台打印日志信息 2019-05-07 21:37:24.009 INFO 5792 --- [nio-6002-exec-1] c.m.c.e.c.controller.CacheController : 从 redis 中获取商品信息 2019-05-07 21:37:24.275 INFO 5792 --- [nio-6002-exec-3] c.m.c.e.c.controller.CacheController : 从 redis 中获取店铺信息 第一次访问的时候，nginx 本地缓存没有，会去 redis 中获取，后面多次访问的时候， 就会走 nginx 本地缓存了，过期时间设置的是 10 分钟 来回顾下流程： 缓存数据生产 有数据变更则主动更新两级缓存（ehcache + redis） 通过缓存维度化拆分，来达到细粒度和小影响更新缓存 分发层 ngix + 应用层 nginx 自定义流量分发策略，提高缓存命中 nginx shared dice 缓存 -> redis 和 ehcache， 渲染 htm 模板并返回客户端 但是还差一个关键的要点，当前面的三级缓存失效（nginx、redis、ehcache）时， 就需要缓存服务重新拉取数据，去更新到 redis 和 ehcache 中。 这个关键点涉及到分布式缓存重建并发冲突问题 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"057.html":{"url":"057.html","title":"057. 分布式缓存重建并发冲突问题以及 zookeeper 分布式锁解决方案","keywords":"","body":"057. 分布式缓存重建并发冲突问题以及 zookeeper 分布式锁解决方案 整个三级缓存的架构已经走通了，对于 3 级缓存失效，缓存重建并发冲突问题还没有解决。 什么是分布式缓存重建并发冲突问题？ 很简单，多个缓存服务实例提供服务，发现缓存失效，那么就会去重建，这个时候回出现以下几种情况： 多个缓存实例都去数据库获取一份数据，然后放入缓存中 新数据被旧数据覆盖 缓存 a 和 b 都拿了一份数据，a 拿到 12:00:01 的数据，b 拿到 12:00:05 的数据 缓存 b 先写入 redis，缓存 a 后写入。 以上问题有多重解决方案，如： 利用 hash 分发 相同商品分发到同一个服务中，服务中再用队列去重建 但是这就变成了有状态的缓存服务，压力全部集中到同一个服务上 利用 kafka 队列 源头信息服务，在发送消息到 kafka topic 的时候，都需要按照 product id 去分区 和上面 hash 方案类似 基于 zookeeper 分布式锁的解决方案 分布式锁：多个机器在访问同一个共享资源，需要给这个资源加一把锁，让多个机器串行访问 对于分布式锁，有很多种实现方式，比如 redis 也可以实现。 这里讲解 zk 分布式锁，zk 做分布式协调比较流程，大数据应用里面 hadoop、storm 都是基于 zk 去做分布式协调 zk 分布式锁的解决并发冲突的方案 变更缓存重建以及空缓存请求重建，更新 redis 之前，都需要先获取对应商品 id 的分布式锁 拿到分布式锁之后，需要根据时间版本去比较一下，如果自己的版本新于 redis 中的版本，那么就更新，否则就不更新 如果拿不到分布式锁，那么就等待，不断轮询等待，直到自己获取到分布式的锁 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"058.html":{"url":"058.html","title":"058. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（一）","keywords":"","body":" 058. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（一） zk 分布式锁原理简单介绍 基于 zkClient 封装分布式锁工具 058. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（一） 本节基于 zk 进行分布式锁的代码封装； zk 分布式锁原理简单介绍 创建一个 zk 临时 node，来模拟一个商品 id 加锁 zk 会保证一个 node path 只会被创建一次，如果已经被创建，则抛出 NodeExistsException 这个时候可以去做业务操作 释放锁，则是删除这个临时 node。 当一个多个缓存服务去对同一个商品 id 加锁时，只有一个成功， 其他的则轮循等待锁被释放，获取到锁之后，对比一下商品的时间版本，较新则重建缓存，否则放弃重建 基于 zkClient 封装分布式锁工具 zk 分布式锁有很多种实现方式，这里演示一种最简单的，但是比较实用的分布式锁 添加依赖: compile 'org.apache.zookeeper:zookeeper:3.4.5' zk client 初始化代码 /** * ${todo} * * @author : zhuqiang * @date : 2019/5/7 22:37 */ public class ZooKeeperSession { private final ZooKeeper zookeeper; private final CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeperSession() { String connectString = \"192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181\"; int sessionTimeout = 5000; try { // 异步连接，所以需要一个 org.apache.zookeeper.Watcher 来通知 // 由于是异步，利用 CountDownLatch 来让构造函数等待 zookeeper = new ZooKeeper(connectString, sessionTimeout, event -> { Watcher.Event.KeeperState state = event.getState(); System.out.println(\"watch event：\" + state); if (state == Watcher.Event.KeeperState.SyncConnected) { System.out.println(\"zookeeper 已连接\"); connectedSemaphore.countDown(); } }); } catch (IOException e) { e.printStackTrace(); } try { connectedSemaphore.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"zookeeper 初始化成功\"); } private static final ZooKeeperSession instance = new ZooKeeperSession(); public static ZooKeeperSession getInstance() { return instance; } public static void main(String[] args) { ZooKeeperSession instance = ZooKeeperSession.getInstance(); } } 运行测试之后输出信息 watch event：SyncConnected zookeeper 已连接 zookeeper 初始化成功 接下来编写加锁与释放锁的逻辑 public class ZooKeeperSession { private final ZooKeeper zookeeper; private final CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeperSession() { String connectString = \"192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181\"; int sessionTimeout = 5000; try { // 异步连接，所以需要一个 org.apache.zookeeper.Watcher 来通知 // 由于是异步，利用 CountDownLatch 来让构造函数等待 zookeeper = new ZooKeeper(connectString, sessionTimeout, event -> { Watcher.Event.KeeperState state = event.getState(); System.out.println(\"watch event：\" + state); if (state == Watcher.Event.KeeperState.SyncConnected) { System.out.println(\"zookeeper 已连接\"); connectedSemaphore.countDown(); } }); } catch (IOException e) { e.printStackTrace(); } try { connectedSemaphore.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"zookeeper 初始化成功\"); } /** * 获取分布式锁 */ public void acquireDistributedLock(Long productId) { String path = \"/product-lock-\" + productId; byte[] data = \"\".getBytes(); try { // 创建一个临时节点，后面两个参数一个安全策略，一个临时节点类型 // EPHEMERAL：客户端被断开时，该节点自动被删除 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(\"获取锁成功 product[id=\" + productId + \"]\"); } catch (Exception e) { e.printStackTrace(); // 如果锁已经被创建，那么将异常 // 循环等待锁的释放 int count = 0; while (true) { try { TimeUnit.MILLISECONDS.sleep(20); // 休眠 20 毫秒后再次尝试创建 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); } catch (Exception e1) { e1.printStackTrace(); count++; continue; } System.out.println(\"获取锁成功 product[id=\" + productId + \"] 尝试了 \" + count + \" 次.\"); break; } } } /** * 释放分布式锁 */ public void releaseDistributedLock(Long productId) { String path = \"/product-lock-\" + productId; try { zookeeper.delete(path, -1); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } private static final ZooKeeperSession instance = new ZooKeeperSession(); public static ZooKeeperSession getInstance() { return instance; } public static void main(String[] args) throws InterruptedException { ZooKeeperSession instance = ZooKeeperSession.getInstance(); CountDownLatch downLatch = new CountDownLatch(2); IntStream.of(1, 2).forEach(i -> new Thread(() -> { instance.acquireDistributedLock(1L); System.out.println(Thread.currentThread().getName() + \" 得到锁并休眠 10 秒\"); try { TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } instance.releaseDistributedLock(1L); System.out.println(Thread.currentThread().getName() + \" 释放锁\"); downLatch.countDown(); }).start()); downLatch.await(); } } 运行 main 测试两个线程获取锁的等待过程如下 watch event：SyncConnected zookeeper 已连接 zookeeper 初始化成功 获取锁成功 product[id=1] Thread-1 得到锁并休眠 10 秒 循环异常中... org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /product-lock-1 at org.apache.zookeeper.KeeperException.create(KeeperException.java:119) Thread-1 释放锁 获取锁成功 product[id=1] 尝试了 285 次. Thread-0 得到锁并休眠 10 秒 Thread-0 释放锁 可以看到，日志输出，证明分布式锁已经编写成功 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"059.html":{"url":"059.html","title":"059. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（二）","keywords":"","body":" 059. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（二） 主动更新 缓存重建 059. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（二） 主动更新 缓存生产服务接收基础信息更改事件的时候，有一个操作是更新本地缓存和 redis 中的缓存， 这个场景下也存可能存在并发冲突情况。所以这里也可以使用分布式锁来保证数据错乱问题 cn.mrcode.cachepdp.eshop.cache.kafka.KafkaMessageProcessor#processProductInfoChangeMessage 回顾下现在的实现代码。以商品为例，来展示怎么使用分布式锁 /** * 处理商品信息变更的消息 */ private void processProductInfoChangeMessage(JSONObject messageJSONObject) { // 提取出商品id Long productId = messageJSONObject.getLong(\"productId\"); // 调用商品信息服务的接口 // 直接用注释模拟：getProductInfo?productId=1，传递过去 // 商品信息服务，一般来说就会去查询数据库，去获取productId=1的商品信息，然后返回回来 String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1}\"; ProductInfo productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); cacheService.saveProductInfo2LocalCache(productInfo); log.info(\"获取刚保存到本地缓存的商品信息：\" + cacheService.getProductInfoFromLocalCache(productId)); cacheService.saveProductInfo2ReidsCache(productInfo); } 使用分布式锁之后 private void processProductInfoChangeMessage(JSONObject messageJSONObject) { // 提取出商品id Long productId = messageJSONObject.getLong(\"productId\"); // 增加了一个 modifyTime 字段，来比较数据修改先后顺序 String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; ProductInfo productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); // 加锁 ZooKeeperSession zks = ZooKeeperSession.getInstance(); zks.acquireDistributedLock(productId); try { // 先获取一次 redis ，防止其他实例已经放入数据了 ProductInfo existedProduct = cacheService.getProductInfoOfReidsCache(productId); if (existedProduct != null) { // 判定通过消息获取到的数据版本和 redis 中的谁最新 Date existedModifyTime = existedProduct.getModifyTime(); Date modifyTime = productInfo.getModifyTime(); // 如果本次获取到的修改时间大于 redis 中的，那么说明此数据是最新的，可以放入 redis 中 if (modifyTime.after(existedModifyTime)) { cacheService.saveProductInfo2LocalCache(productInfo); log.info(\"最新数据覆盖 redis 中的数据：\" + cacheService.getProductInfoFromLocalCache(productId)); cacheService.saveProductInfo2ReidsCache(productInfo); } } else { // redis 中没有数据，直接放入 cacheService.saveProductInfo2LocalCache(productInfo); log.info(\"获取刚保存到本地缓存的商品信息：\" + cacheService.getProductInfoFromLocalCache(productId)); cacheService.saveProductInfo2ReidsCache(productInfo); } } finally { // 最后释放锁 zks.releaseDistributedLock(productId); } } 缓存重建 回顾下重建的地方 /** * 这里的代码别看着奇怪，简单回顾下之前的流程： 1. nginx 获取 redis 缓存 2. 获取不到再获取服务的堆缓存（也就是这里的 ecache） 3. * 还获取不到就需要去数据库获取并重建缓存 */ @RequestMapping(\"/getProductInfo\") @ResponseBody public ProductInfo getProductInfo(Long productId) { ProductInfo productInfo = cacheService.getProductInfoOfReidsCache(productId); log.info(\"从 redis 中获取商品信息\"); if (productInfo == null) { productInfo = cacheService.getProductInfoFromLocalCache(productId); log.info(\"从 ehcache 中获取商品信息\"); } if (productInfo == null) { // 两级缓存中都获取不到数据，那么就需要从数据源重新拉取数据，重建缓存 // 但是这里暂时不讲 log.info(\"缓存重建 商品信息\"); } return productInfo; } 如上代码，都获取不到数据的时候，就需要从数据库读取数据进行重建。 第一版思路： 从数据库读取数据 队列异步重建 返回第一步的数据 下面来实现下这个代码（先不考虑该思路是否有问题） cn.mrcode.cachepdp.eshop.cache.controller.RebuildCache /** * 缓存重建；一个队列对应一个消费线程 * * @author : zhuqiang * @date : 2019/5/14 21:06 */ @Component public class RebuildCache { private final Logger log = LoggerFactory.getLogger(getClass()); private final ArrayBlockingQueue queue = new ArrayBlockingQueue<>(100); private final CacheService cacheService; public RebuildCache(CacheService cacheService) { this.cacheService = cacheService; start(); } public void put(ProductInfo productInfo) { try { queue.put(productInfo); } catch (InterruptedException e) { e.printStackTrace(); } } public ProductInfo take() { try { return queue.take(); } catch (InterruptedException e) { e.printStackTrace(); } return null; } // 启动一个线程来消费 private void start() { new Thread(() -> { while (true) { try { ProductInfo productInfo = queue.take(); Long productId = productInfo.getId(); ZooKeeperSession zks = ZooKeeperSession.getInstance(); zks.acquireDistributedLock(productId); try { // 先获取一次 redis ，防止其他实例已经放入数据了 ProductInfo existedProduct = cacheService.getProductInfoOfReidsCache(productId); if (existedProduct != null) { // 判定通过消息获取到的数据版本和 redis 中的谁最新 Date existedModifyTime = existedProduct.getModifyTime(); Date modifyTime = productInfo.getModifyTime(); // 如果本次获取到的修改时间大于 redis 中的，那么说明此数据是最新的，可以放入 redis 中 if (modifyTime.after(existedModifyTime)) { cacheService.saveProductInfo2LocalCache(productInfo); log.info(\"最新数据覆盖 redis 中的数据：\" + cacheService.getProductInfoFromLocalCache(productId)); cacheService.saveProductInfo2ReidsCache(productInfo); } else { log.info(\"此次数据版本落后，放弃重建\"); } } else { // redis 中没有数据，直接放入 cacheService.saveProductInfo2LocalCache(productInfo); log.info(\"缓存重建成功\" + cacheService.getProductInfoFromLocalCache(productId)); cacheService.saveProductInfo2ReidsCache(productInfo); } } finally { // 最后释放锁 zks.releaseDistributedLock(productId); } } catch (InterruptedException e) { e.printStackTrace(); } } }).start(); } } controller 中使用该队列 /** * 这里的代码别看着奇怪，简单回顾下之前的流程： 1. nginx 获取 redis 缓存 2. 获取不到再获取服务的堆缓存（也就是这里的 ecache） 3. * 还获取不到就需要去数据库获取并重建缓存 */ @RequestMapping(\"/getProductInfo\") @ResponseBody public ProductInfo getProductInfo(Long productId) { ProductInfo productInfo = cacheService.getProductInfoOfReidsCache(productId); log.info(\"从 redis 中获取商品信息\"); if (productInfo == null) { productInfo = cacheService.getProductInfoFromLocalCache(productId); log.info(\"从 ehcache 中获取商品信息\"); } if (productInfo == null) { // 两级缓存中都获取不到数据，那么就需要从数据源重新拉取数据，重建缓存 // 假设这里从数据库中获取的数据 String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); rebuildCache.put(productInfo); } return productInfo; } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"060.html":{"url":"060.html","title":"060. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（三）","keywords":"","body":" 060. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（三） gradle 解决冲突&查找依赖树 测试缓存重建流程 测试商品信息变更缓存重建 测试两个流程 小结 思想总结 疑问 060. 缓存数据生产服务中的 zk 分布式锁解决方案的代码实现（三） 启动项目来测试，会发现报错了。找到两个 StaticLoggerBinder，来自两个 jar 中； SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/E:/SoftwareDevelop/Repository/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.25/110cefe2df103412849d72ef7a67e4e91e4266b4/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/E:/SoftwareDevelop/Repository/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.3/7c4f3c474fb2c041d8028740440937705ebb473a/logback-classic-1.2.3.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] Exception in thread \"main\" java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.impl.Log4jLoggerFactory loaded from file:/E:/SoftwareDevelop/Repository/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.25/110cefe2df103412849d72ef7a67e4e91e4266b4/slf4j-log4j12-1.7.25.jar). If you are using WebLogic you will need to add 'org.slf4j' to prefer-application-packages in WEB-INF/weblogic.xml: org.slf4j.impl.Log4jLoggerFactory at org.springframework.util.Assert.instanceCheckFailed(Assert.java:655) 这里排除 zookper 中的 gradle 解决冲突&查找依赖树 gradle dependencyInsight --dependency slf4j-log4j12 H:\\dev\\project\\mrcode\\cache-pdp\\eshop-cache>gradle dependencyInsight --dependency slf4j-log4j12 > Task :eshop-cache:dependencyInsight org.slf4j:slf4j-log4j12:1.7.25 (selected by rule) variant \"runtime\" [ org.gradle.status = release (not requested) Requested attributes not found in the selected variant: org.gradle.usage = java-api ] org.slf4j:slf4j-log4j12:1.6.1 -> 1.7.25 variant \"runtime\" [ org.gradle.status = release (not requested) Requested attributes not found in the selected variant: org.gradle.usage = java-api ] \\--- org.apache.zookeeper:zookeeper:3.4.5 +--- compileClasspath +--- org.apache.kafka:kafka_2.9.2:0.8.1.1 | \\--- compileClasspath \\--- com.101tec:zkclient:0.3 \\--- org.apache.kafka:kafka_2.9.2:0.8.1.1 (*) (*) - dependencies omitted (listed previously) A web-based, searchable dependency report is available by adding the --scan option. BUILD SUCCESSFUL in 7s 1 actionable task: 1 executed 可以看到 在 zk 里面有引用，然后 kafka 里面有引用 zookper。所以需要排除两个地方 compile ('org.apache.kafka:kafka_2.9.2:0.8.1.1'){ exclude group: 'org.slf4j', module: 'slf4j-log4j12' } compile ('org.apache.zookeeper:zookeeper:3.4.5'){ exclude group: 'org.slf4j', module: 'slf4j-log4j12' } 测试缓存重建流程 首先先删除 redis 中的数据，通过之前的 redis 章节中的命令登录并删除 redis-cli -h eshop-cache02 -p 7003 eshop-cache02:7003> del product_info_1 再启动 eshop-cache 服务； 访问：http://localhost:6002/getProductInfo?productId=1,查看打印日志 2019-05-14 22:22:38.618 INFO 11616 --- [nio-6002-exec-1] c.m.c.e.c.controller.CacheController : 从 redis 中获取商品信息 2019-05-14 22:22:38.643 INFO 11616 --- [nio-6002-exec-1] c.m.c.e.c.controller.CacheController : 从 ehcache 中获取商品信息 2019-05-14 22:22:45.276 INFO 11616 --- [p-cache03:2181)] org.apache.zookeeper.ClientCnxn : Session establishment complete on server eshop-cache03/192.168.99.172:2181, sessionid = 0x26a8e8df54a0001, negotiated timeout = 5000 watch event：SyncConnected zookeeper 已连接 zookeeper 初始化成功 获取锁成功 product[id=1] 2019-05-14 22:22:45.295 INFO 11616 --- [ Thread-14] c.m.c.e.cache.controller.RebuildCache : 缓存重建成功：cn.mrcode.cachepdp.eshop.cache.model.ProductInfo@1ce9e40a 再次访问，只有一条日志了 2019-05-14 22:23:52.328 INFO 11616 --- [nio-6002-exec-4] c.m.c.e.c.controller.CacheController : 从 redis 中获取商品信息 测试商品信息变更缓存重建 也就是 KafkaMessageProcessor 中的逻辑 这个测试需要手动往 kafka 中植入一条数据，触发这个消费逻辑 手动植入数据参考 这里 bin/kafka-console-producer.sh --broker-list 192.168.99.170:9092,192.168.99.171:9092,192.168.99.172:9092 --topic eshop-message {\"serviceId\":\"productInfoService\",\"productId\":\"1\"} 日志打印 watch event：SyncConnected zookeeper 已连接 zookeeper 初始化成功 获取锁成功 product[id=1] 2019-05-14 22:37:32.884 INFO 6692 --- [ Thread-29] c.m.c.e.c.kafka.KafkaMessageProcessor : 获取刚保存到本地缓存的商品信息：cn.mrcode.cachepdp.eshop.cache.model.ProductInfo@37582749 获取锁成功 product[id=1] 2019-05-14 22:37:38.648 INFO 6692 --- [ Thread-29] c.m.c.e.c.kafka.KafkaMessageProcessor : 数据未变更过 测试两个流程 增加 kafka 获取锁后的休眠，模拟耗时操作 删除 redis 缓存，重启项目后 先触发 kafka 再访问 http://localhost:6002/getProductInfo?productId=1 watch event：SyncConnected zookeeper 已连接 zookeeper 初始化成功 获取锁成功 product[id=1] 2019-05-14 22:44:56.225 INFO 9376 --- [ Thread-30] c.m.c.e.c.kafka.KafkaMessageProcessor : kafka 休眠 10 秒 2019-05-14 22:44:59.640 TRACE 9376 --- [nio-6002-exec-1] o.s.web.servlet.DispatcherServlet : GET \"/getProductInfo?productId=1\", parameters={masked}, headers={masked} in DispatcherServlet 'dispatcherServlet' 2019-05-14 22:44:59.649 TRACE 9376 --- [nio-6002-exec-1] s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped to public cn.mrcode.cachepdp.eshop.cache.model.ProductInfo cn.mrcode.cachepdp.eshop.cache.controller.CacheController.getProductInfo(java.lang.Long) 2019-05-14 22:44:59.672 TRACE 9376 --- [nio-6002-exec-1] .w.s.m.m.a.ServletInvocableHandlerMethod : Arguments: [1] 2019-05-14 22:44:59.682 INFO 9376 --- [nio-6002-exec-1] c.m.c.e.c.controller.CacheController : 从 redis 中获取商品信息 2019-05-14 22:44:59.705 INFO 9376 --- [nio-6002-exec-1] c.m.c.e.c.controller.CacheController : 从 ehcache 中获取商品信息 org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /product-lock-1 at org.apache.zookeeper.KeeperException.create(KeeperException.java:119) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) at cn.mrcode.cachepdp.eshop.cache.ZooKeeperSession.acquireDistributedLock(ZooKeeperSession.java:58) at cn.mrcode.cachepdp.eshop.cache.controller.RebuildCache.lambda$start$0(RebuildCache.java:58) at cn.mrcode.cachepdp.eshop.cache.controller.RebuildCache$$Lambda$433/1623148876.run(Unknown Source) at java.lang.Thread.run(Thread.java:745) 2019-05-14 22:45:06.243 INFO 9376 --- [ Thread-30] c.m.c.e.c.kafka.KafkaMessageProcessor : 获取刚保存到本地缓存的商品信息：cn.mrcode.cachepdp.eshop.cache.model.ProductInfo@9fbc9fb 获取锁成功 product[id=1] 尝试了 180 次. 2019-05-14 22:45:06.293 INFO 9376 --- [ Thread-13] c.m.c.e.cache.controller.RebuildCache : 此次数据版本落后，放弃重建 上面的日志可以看到： 在休眠过程中，触发了缓存重建操作 缓存重建一直在等待锁的释放，最后尝试了 180 次，才获取到 获取到之后，发现 kafka 线程重建缓存后的数据比自己的新（其实是一样的时间，日志打印问题），所以放弃了往 redis 中放入数据的操作 小结 思想总结 缓存重建出现在两个地方： 当基础服务信息变更之后（被动） 当所有缓存失效之后（主动） 一个主动一个被动，他们的执行逻辑都相同，其实可以使用一个队列逻辑来处理缓存重建 缓存重建重要依赖「zk 分布式锁」让多个实例/操作 串行化起来。避免脏数据覆盖新数据 疑问 此处的 kafka 调试还是不能 debug。一 debug 线程就卡卡住； 另外这个逻辑细节有些地方是不严谨的，比如从数据库获取数据，再用分布式锁去， 那么会不会出现在分布式锁之前很多请求全部打到数据库中去了呢？ 我这里只是提出来课程中的一点疑问，现在最主要的还是要跟着课程思路走 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"061.html":{"url":"061.html","title":"061. Java 程序员、缓存架构以及 Storm 大数据实时计算之间的关系","keywords":"","body":" 061. Java 程序员、缓存架构以及 Storm 大数据实时计算之间的关系 缓存架构和 storm 的关系 java 工程师与 storm 之间的关系是什么？ java 系统与大数据的关系 旁白 061. Java 程序员、缓存架构以及 Storm 大数据实时计算之间的关系 接下来，我们是要讲解这个商品详情页缓存架构，缓存预热问题和解决方案，缓存热点数据可能导致整个系统崩溃的问题，以及解决方案 缓存相关的「热」：预热、热数据 在解决方案和架构设计中，会引入大数据的实时计算的技术 storm。为什么要引入这个 storm，难道必须是 storm 吗？我们后面去讲解那个解决方案的时候再说 缓存架构和 storm 的关系 因为有些热点数据相关的一些实时处理的一些方案，比如快速预热，热点数据的实时感知和快速降级，全部要用到 storm 因为我们可能需要实时的去计算出热点缓存数据，我们的业务场景是亿级流量、高并发、大量的请求过来 ，这个时候，你要做一些实时的计算，那么必须涉及到分布式的一些技术，才能处理高并发，大量的请求，目前在时候计算的领域，最成熟的大数据的技术，就是 storm java 工程师与 storm 之间的关系是什么？ 由于一直在大公司（BAT 等一线公司），所以我（讲师）认识的很多的 java 工程师； 本身自己之前是做 java 开发和架构，后来开始大数据的架构。 大公司里的很多 java 工程师，都是会用一些大数据的一些技术的，比如 storm、hbase、zookeeper、hive、spark， 因为在大公司里，容易遇到一些复杂的挑战和场景，比如高并发、海量数据的场景 你做一些 java 相关的项目和系统，可能也会遇到这种问题，很多时候，直接用大数据的一些技术， 实时计算，你是自己去写个系统，还是用现成的 storm？ 我也只是说部分 java 的人，但是也有很多搞 java 的工程师就是纯 java 技术栈 java 系统与大数据的关系 大数据不仅仅只是大数据工程师要关注的东西， 大数据也是 Java 程序员在构建各类系统的时候一种全新的思维，以及架构理念， 比如 Storm、Hive、Spark、ZooKeeper、HBase、Elasticsearch 等等 举例说明： Storm：实时计算 实时缓存热点数据统计 -> 缓存预热 -> 缓存热点数据自动降级 Hive：数据仓库 Hadoop 生态栈里面，比如做一个数据仓库的系统，高并发访问下，海量请求日志的批量统计分析，日报周报月报，接口调用情况，业务使用情况，等等 我所知，在一些大公司里面，是有些人是将海量的请求日志打到 hive 里面，做离线的分析，然后反过来去优化自己的系统 Spark：离线批量数据处理 比如从 DB 中一次性批量处理几亿数据，清洗和处理后写入 Redis 中供后续的系统使用，大型互联网公司的用户相关数据 ZooKeeper：分布式系统的协调 分布式锁，分布式选举->高可用 HA 架构，轻量级元数据存储 如：用 java 开发了分布式的系统架构，你的整套系统拆分成了多个部分，每个部分都会负责一些功能， 互相之间需要交互和协调； 服务 A 说：我在处理某件事情的时候，服务 B 你就别处理了 服务 A 说：我一旦发生了某些状况，希望服务 B 你立即感知到，然后做出相应的对策 HBase：海量数据的在线存储和简单查询，替代 MySQL 分库分表，提供更好的伸缩性 如：java 底层对应的是海量数据，然后要做一些简单的存储和查询，同时数据增多的时候要快速扩容 这种场景下 mysql 分库分表就不太合适了，mysql 分库分表扩容，还是比较麻烦的 Elasticsearch：海量数据的复杂检索以及搜索引擎的构建 支撑有大量数据的各种企业信息化系统的搜索引擎，电商/新闻等网站的搜索引擎，等等 比用 mysql 的 like \"%xxxx%\"，更加合适一些，性能更加好 旁白 大家不要说觉得来听课程，就必须每堂课都是代码，代码，代码，就不喜欢听我这些废话 我告诉大家，这些还真不是废话，代码很重要，手写代码，不能出现 copy。 我可能做为一个过来人，很多项目都做过，很多技术都用过，也做过。 站在我的角度，去给大家讲一讲，行业，一些技术领域的问题 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"storm/062.html":{"url":"storm/062.html","title":"062. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：大白话介绍","keywords":"","body":" 062. 大白话介绍 主题简介 主题讲解 mysq、hadoop 与 strom 2、我们能不能自己搞一套 storm？ storm 的特点是什么? 062. 大白话介绍 主题简介 一大段话简而言之如下： nginx、zookeeper、lua 主要讲解基于他们的一些架构和解决方案的设计，不会细讲 redis 花了很大的篇幅是因为高并发高可用底层是他来支撑，值得花时间来细讲 一个重点：数据库 + 缓存双写，多级缓存架构 重点理解方案设计和架构思想 storm 和 hystrix 比较重要 热数据处理和缓存雪崩需要依赖这两个技术；造就了系统可用性和稳定性 zookper 主要讲解了分布式锁，redis 也可做，所以不细讲 lua 脚本语言，自己查资料了解 storm 在做热数据这块，如果要做复杂的热数据的统计和分析，在亿级流量、高并发的场景下，我还真觉得，最合适的技术就是 storm，没有其他（成熟、稳定） 缓存架构、热数据相关的架构设计中最重要的唯一的可选技术 storm 会好好的去讲一下 后续会讲解 hystrix：提供分布式系统高可用性，限流、熔断、降级等措施；后续会讲解缓存雪崩方案，复杂的限流措施 主题讲解 讲给 Java 工程师的史上最通俗易懂 Storm 教程 怎么理解？ 讲给 Java 工程师 我知道你没什么大数据的背景和经验、基础，那么我就把你当做一个大数据小白，主要是 java背景和基础 史上最通俗易懂 市面上其他的 storm 视频课程，或者是一些书籍，我告诉，storm 还是挺难的，事务，云里雾里，云里雾里 有些搞 storm 大数据的，连这个并行度和流分组的本质它都说不清楚，因为市面上的资料也说不清楚 本课程会把你当做小白，用最最通俗易懂的语言，给你去讲解这块的知识，画图 接下来就讲解 stom 到底是什么？ mysq、hadoop 与 strom mysql：事务性系统，面临海量数据的尴尬 先不考虑分布式 mysql，因为技术还不成熟，实现起来也比较复杂 hadoop：离线批处理 strom：实时计算 2、我们能不能自己搞一套 storm？ 实时计算：来一条数据，我理解就算一条，来一条，算一条 唯一的坑：海量高并发大数据，高并发的请求数据，分布式的系统，流式处理的分布式系统 如果自己搞一套实时流系统出来，也是可以的，但是。。。。 花费大量的时间在底层技术细节上：如何部署各种中间队列、节点间的通信、容错、资源调配、计算节点的迁移和部署，等等 花费大量的时间在系统的高可用上问题上：如何保证各种节点能够高可用稳定运行 花费大量的时间在系统扩容上：吞吐量需要扩容的时候，你需要花费大量的时间去增加节点，修改配置、测试，等等 如 5万/s 扩容到 10万/s，需要大两岁时间去增加节点测试 国内国产的实时大数据计算系统，唯一做出来的，做得好的，做得影响力特别大，特别牛逼的，就是 JStorm，但是阿里技术实力，可以说是世界一流，国内顶尖的 JStorm 原本是用 clojure 编程语言写的，阿里用 Java 重新写了一遍；后来又开发了一个 Galaxy 流式计算的系统；百度，腾讯，也都自己做了，也能做得很好， 但是一个普通程序员想做出来就真的太难了 storm 的特点是什么? 支撑各种实时类的项目场景 实时处理消息以及更新数据库，基于最基础的实时计算语义和 API（实时数据处理领域）；对实时的数据流持续的进行查询或计算，同时将最新的计算结果持续的推送给客户端展示，同样基于最基础的实时计算语义和 API（实时数据分析领域）；对耗时的查询进行并行化，基于 DRPC，即分布式 RPC 调用，如单表 30 天数据，并行化每个进程查询一天数据，最后组装结果 总之 storm 在实时类项目场景的时候都能很好的去支撑 高度的可伸缩性 如果要扩容，直接加机器，调整 storm 计算作业的并行度就可以了，storm 会自动部署更多的进程和线程到其他的机器上去，无缝快速扩容 扩容起来，超方便 数据不丢失的保证 storm 的消息可靠机制开启后，可以保证一条数据都不丢 数据不丢失，也不重复计算 超强的健壮性 从历史经验来看，storm 比 hadoop、spark 等大数据类系统，健壮的多的多，因为元数据全部放 zookeeper，不在内存中，随便挂都不要紧 特别的健壮，稳定性和可用性很高 使用的便捷性：核心语义非常的简单，开发起来效率很高 用起来很简单，开发 API 还是很简单的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"storm/063.html":{"url":"storm/063.html","title":"063. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：大白话讲集群架构与核心概念","keywords":"","body":" 063. 大白话讲集群架构与核心概念 Storm 的集群架构 Storm 的核心概念 063. 大白话讲集群架构与核心概念 Storm 的集群架构 Nimbus、Supervisor、ZooKeeper、Worker、Executor、Task Nimbus：资源调度 Supervisor：相当于一台机器上的代理管家 ZooKeeper：用于存放 Nimbus 和 Supervisor 的调度元数据信息 Worker：根据配置可启动多个 worker 进程 Executor：根据配置可启动多个线程 Task：就是业务代码，不是线程，可能就是 stom 中你需要实现的业务代码 整体架构流程如图：一个事实计算作业启动后，Nimbus 通知 Supervisor 去启动 n 个 Worker，Worker 又启动 n 个 Executor，Executor 执行具体的 业务代码 Storm 的核心概念 Topology、Spout、Bolt、Tuple、Stream Topology（拓扑）：虚的抽象的概念 Spout：数据源代码组件 可以理解为：用 java 实现一个 Spout 接口，在该代码中尝试去数据源获取数据，如 mysql、kafka Bolt：业务处理代码组件 可以理解为：spout 会将数据传送给 bolt，各种 bolt 还可以串联成一个计算链条，同样是实现一个 bolt 接口 一堆 spout + bolt，就会组成一个 topology（拓扑），也可以叫做一个实时计算作业； 一个拓扑涵盖数据源获取/生产 + 数据处理的所有的代码逻辑 Tuple：一条数据 每条数据都会被封装在 tuple 中，在多个 spout 和 bolt 之间传递 Stream：一个流 虚的抽象的概念，源源不断过来的 tuple，就组成了一条数据流 了解了核心的基本概念之后，上图清晰的示意了他们是怎么配合工作的， 业务代码层面的概念通过配置，被调度到具体的机器上的集群概念中去执行 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"storm/064.html":{"url":"storm/064.html","title":"064. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：大白话讲并行度和流分组","keywords":"","body":"064. 大白话讲并行度和流分组 对于 java 工程师来说，先不说精通 storm ，本教程希望达到的一个效果如下： 对 storm 的核心的基本原理要清楚：集群架构、核心概念、并行度和流分组 掌握最常见的 storm 开发范式 spout 消费 kafka，后面跟一堆 bolt，bolt 之间设定好流分组的策略， bolt 中填充各种代码逻辑 了解如何将 storm 拓扑打包后提交到 storm 集群上去运行 掌握如何能够通过 storm ui 去查看你的实时计算拓扑的运行现状 如果你所在公司有大数据团队并且维护了一个 storm 集群，那么掌握如何开发和部署即可， 如果没有，那么你就需要去深入学习下 storm 了。如果你的场景不是特别复杂， 整个数据量也不是特别大，其实自己主要研究一下，怎么部署 storm 集群也可以，本教程也会讲解 Storm 的并行度以及流分组是重要的一个概念，但是没有几个人能说的清楚。 好多年前，我第一次接触 storm 的时候，真的我觉得都没几个人能彻底讲清楚，用一句话讲清楚什么是并行度，什么是流分组， 很多时候，你以外你明白了，其实你不明白，比如我经常面试一些做过 storm 的人过来，我就问一个问题， 就知道它的水深水浅，流分组的时候，数据在 storm 集群中的流向，你画一下，比如你自己随便设想一个拓扑结果出来， 几个 spout，几个 bolt，各种流分组情况下，数据是怎么流向的，要求具体画出集群架构中的流向， worker，executor，task，supervisor，数据是怎么流转的；几乎没几个人能画对，为什么呢， 很多人就没搞明白这个并行度和流分组到底是什么 那么这里一句话总结： 并行度：Worker->Executor->Task，没错，是 Task 默认情况下，一个 Executor 对应一个 Task 简单说就是 task 越多，并行度越高 流分组：Task 与 Task 之间的数据流向关系 一个拓扑中，可以有很多 Spout + Bolt，那么 bolt1 的数据流向 bolt2 的时候的一个策略 就是流分组 流分组策略： Shuffle Grouping：随机发射，负载均衡 Fields Grouping：根据一个或多个字段进行分组 那一个或者多个 fields 如果值完全相同的话，那么这些 tuple，就会发送给下游 bolt 的其中固定的一个 task 你发射的每条数据是一个 tuple，每个 tuple 中有多个 field 作为字段 比如 tuple 3 个字段，name，age，salary {\"name\": \"tom\", \"age\": 25, \"salary\": 10000} -> tuple -> 3个 field，name，age，salary All Grouping：广播分发 Global Grouping：选择其中一个 task 最小的 id 分发 None Grouping：与 shuffle 类似 Direct Grouping：指定一个 task id 发送 Local or Shuffle Grouping： 只在本地同一个进程（worker）中国随机分发 最常用的是前两种 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"storm/065.html":{"url":"storm/065.html","title":"065. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：纯手敲 WordCount 程序","keywords":"","body":" 065. 纯手敲 WordCount 程序 构建项目 编写代码 小结 065. 纯手敲 WordCount 程序 构建项目 storm-helloword/build.gradle dependencies { compile 'org.apache.storm:storm-core:1.1.0' compile 'commons-collections:commons-collections:3.2.1' } // 因为需要打包 jar 独立运行，所以需要配置打包第三方依赖 // 注意 不要写成 boot 项目，否则就只能打成 bootjar 了 jar { manifest { attributes( \"Manifest-Version\": 1.0, \"Main-Class\": \"cn.mrcode.cachepdp.storm.helloword.WordCountTopology\") } from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } } into('assets') { from 'assets' } } 编写代码 package cn.mrcode.cachepdp.storm.helloword; import org.apache.storm.Config; import org.apache.storm.LocalCluster; import org.apache.storm.StormSubmitter; import org.apache.storm.generated.AlreadyAliveException; import org.apache.storm.generated.AuthorizationException; import org.apache.storm.generated.InvalidTopologyException; import org.apache.storm.spout.SpoutOutputCollector; import org.apache.storm.task.OutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.TopologyBuilder; import org.apache.storm.topology.base.BaseRichBolt; import org.apache.storm.topology.base.BaseRichSpout; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Tuple; import org.apache.storm.tuple.Values; import org.apache.storm.utils.Utils; import java.util.HashMap; import java.util.Map; import java.util.Random; import java.util.concurrent.TimeUnit; import java.util.logging.Logger; /** * * 需求：统计一些句子中单词出现的次数 * * * @author : zhuqiang * @date : 2019/5/19 13:56 */ public class WordCountTopology { /** * 定义一个数据源；这里直接伪造一个假数据 */ public static class RandomSentenceSpout extends BaseRichSpout { private static final Logger logger = Logger.getLogger(RandomSentenceSpout.class.getName()); private Random random; private SpoutOutputCollector collector; private String[] sentences; /** * * 对 spout 进行初始化工作 * 比如：创建一个线程池、创建一个数据库连接、构造一个 httpclient * * * @param collector 数据写出对象 */ @Override public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) { random = new Random(); this.collector = collector; sentences = new String[]{\"the cow jumped over the moon\", \"an apple a day keeps the doctor away\", \"four score and seven years ago\", \"snow white and the seven dwarfs\", \"i am at two with nature\"}; logger.info(\"RandomSentenceSpout open\"); } /** * * 本类（Spout）最终会运行在 task 中，某个 worker 进程的某个 executor 线程内部的某个 task 中 * 该 task 会负责无限循环调用 nextTuple 方法 * 就可以达到不断的发射最新的数据，形成一个数据流 * */ @Override public void nextTuple() { Utils.sleep(2000); String sentence = this.sentences[random.nextInt(this.sentences.length)]; System.err.println(\"RandomSentenceSpout sentence:\" + sentence); collector.emit(new Values(sentence)); } /** * * 定义发射出去的每个 tuple 中的每个 field 的名称是什么？ * 这里只有一个值，只需要写一个字段名称 * */ @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(\"sentence\")); } } /** * * 定义一个 bolt ，用于对数据的加工， * 这里拆分接收到的句子，拆分成一个一个的单词 * */ public static class SplitSentence extends BaseRichBolt { private OutputCollector collector; /** * 该类初始化方法，这里可以拿到发射器 */ @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.collector = collector; } /** * 每接收到一条数据，就会调用该方法，进行加工处理 */ @Override public void execute(Tuple input) { String sentence = input.getStringByField(\"sentence\"); for (String word : sentence.split(\" \")) { // 拆分成一个一个单词之后，再发射出去 collector.emit(new Values(word)); } } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { // 定义发数据的字段名称 declarer.declare(new Fields(\"word\")); } } /** * 在定义一个 bolt ，用于对单词的统计 */ public static class WordCount extends BaseRichBolt { private OutputCollector collector; /** * 用来存储每个单词的统计数量 */ private Map counts; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.collector = collector; this.counts = new HashMap<>(); } @Override public void execute(Tuple input) { String word = input.getStringByField(\"word\"); Integer count = counts.get(word); if (count == null) { count = 1; counts.put(word, count); } counts.put(word, ++count); System.err.println(Thread.currentThread().getName() + \"WordCount word:\" + word + \", count :\" + count); collector.emit(new Values(word, count)); } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(\"wordk\", \"count\")); } } public static void main(String[] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException, InterruptedException { // 构建拓扑，也就是手动定义业务流程 // 其他的提交到 storm 集群后，由 storm 去调度在哪些机器上启动你所定义的 拓扑 TopologyBuilder builder = new TopologyBuilder(); // id、spout、并发数量 builder.setSpout(RandomSentenceSpout.class.getSimpleName(), new RandomSentenceSpout(), 2); builder.setBolt(SplitSentence.class.getSimpleName(), new SplitSentence(), 5) // 默认是一个 executor 一个 task // 这里设置 5 个 executor，但是 task 设置了 10 个，相当于 每个 executor 2 个 task .setNumTasks(10) // 配置该 bolt 以何种方式从哪里获取数据 .shuffleGrouping(RandomSentenceSpout.class.getSimpleName()); builder.setBolt(WordCount.class.getSimpleName(), new WordCount(), 5) .setNumTasks(10) // 配置按字段形式去 SplitSentence 中获取数据 // 相同的单词始终都会被发射到同一个 task 中去 .fieldsGrouping(SplitSentence.class.getSimpleName(), new Fields(\"word\")); // 上面代码配置有点像是主动获取数据，实际上是被动接受吗？ Config conf = new Config(); conf.setDebug(false); if (args != null && args.length > 0) { // 表示在命令行中运行的，需要提交的 storm 集群中去 conf.setNumWorkers(3); StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology()); } else { conf.setMaxTaskParallelism(3); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"word-count\", conf, builder.createTopology()); TimeUnit.SECONDS.sleep(10); cluster.shutdown(); } } } 运行后输出日志 RandomSentenceSpout sentence:snow white and the seven dwarfs Thread-28-WordCount-executor[6 6]WordCount word:snow, count :2 Thread-22-WordCount-executor[7 7]WordCount word:the, count :2 Thread-18-WordCount-executor[8 8]WordCount word:and, count :2 Thread-28-WordCount-executor[6 6]WordCount word:white, count :2 Thread-18-WordCount-executor[8 8]WordCount word:dwarfs, count :2 Thread-28-WordCount-executor[6 6]WordCount word:seven, count :2 RandomSentenceSpout sentence:an apple a day keeps the doctor away Thread-18-WordCount-executor[8 8]WordCount word:a, count :2 Thread-22-WordCount-executor[7 7]WordCount word:an, count :2 Thread-28-WordCount-executor[6 6]WordCount word:apple, count :2 Thread-22-WordCount-executor[7 7]WordCount word:day, count :2 Thread-28-WordCount-executor[6 6]WordCount word:keeps, count :2 Thread-22-WordCount-executor[7 7]WordCount word:the, count :3 Thread-28-WordCount-executor[6 6]WordCount word:doctor, count :2 Thread-28-WordCount-executor[6 6]WordCount word:away, count :2 RandomSentenceSpout sentence:an apple a day keeps the doctor away Thread-18-WordCount-executor[8 8]WordCount word:a, count :3 Thread-22-WordCount-executor[7 7]WordCount word:an, count :3 Thread-28-WordCount-executor[6 6]WordCount word:apple, count :3 Thread-22-WordCount-executor[7 7]WordCount word:day, count :3 Thread-28-WordCount-executor[6 6]WordCount word:keeps, count :3 Thread-22-WordCount-executor[7 7]WordCount word:the, count :4 Thread-28-WordCount-executor[6 6]WordCount word:doctor, count :3 Thread-28-WordCount-executor[6 6]WordCount word:away, count :3 RandomSentenceSpout sentence:snow white and the seven dwarfs Thread-18-WordCount-executor[8 8]WordCount word:and, count :3 Thread-28-WordCount-executor[6 6]WordCount word:snow, count :3 Thread-22-WordCount-executor[7 7]WordCount word:the, count :5 Thread-28-WordCount-executor[6 6]WordCount word:white, count :3 Thread-18-WordCount-executor[8 8]WordCount word:dwarfs, count :3 Thread-28-WordCount-executor[6 6]WordCount word:seven, count :3 RandomSentenceSpout sentence:the cow jumped over the moon Thread-22-WordCount-executor[7 7]WordCount word:the, count :6 Thread-28-WordCount-executor[6 6]WordCount word:cow, count :2 Thread-28-WordCount-executor[6 6]WordCount word:jumped, count :2 Thread-22-WordCount-executor[7 7]WordCount word:over, count :2 Thread-22-WordCount-executor[7 7]WordCount word:the, count :7 Thread-22-WordCount-executor[7 7]WordCount word:moon, count :2 RandomSentenceSpout sentence:the cow jumped over the moon Thread-22-WordCount-executor[7 7]WordCount word:the, count :8 Thread-28-WordCount-executor[6 6]WordCount word:cow, count :3 Thread-22-WordCount-executor[7 7]WordCount word:over, count :3 Thread-28-WordCount-executor[6 6]WordCount word:jumped, count :3 Thread-22-WordCount-executor[7 7]WordCount word:the, count :9 Thread-22-WordCount-executor[7 7]WordCount word:moon, count :3 从上面的输出日志来看，Thread-28-WordCount-executor[6 6] 中每次都是处理 snow， 这里需要注意下，应该这样说，snow 每次都在 28-6-6 上被处理，因为当单词种类大于 maxtask 配置的时候， 其实一个 task 会处理多个单词的，但是能保证相同的单词一定会落在同一个线程中 小结 作为 java 工程师，会一些大数据的基本技术就够了， 使用 storm 主要是用它成熟稳定的易于扩展的分布式系统特性。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"storm/066.html":{"url":"storm/066.html","title":"066. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：纯手工集群部署","keywords":"","body":" 066. 纯手工集群部署 确认是否安装了依赖 storm 安装 修改 storm 配置文件 启动 storm 集群和 ui 界面 066. 纯手工集群部署 确认是否安装了依赖 java 1.7 + pythong 2.6.6 该软件之前被安装过 storm 安装 cd /usr/local tar -zxvf apache-storm-1.1.0.tar.gz mv apache-storm-1.1.0 storm # 配置环境变量，添加 /usr/local/bin vi ~/.bashrc source ~/.bashrc # 查看 storm 版本 storm version 修改 storm 配置文件 mkdir /var/storm vi /usr/local/storm/conf/storm.yaml # 需要修改的配置内容如下： # ------------------------- storm.zookeeper.servers: - \"192.168.99.170\" - \"192.168.99.171\" - \"192.168.99.172\" # storm.zookeeper.port: 2181 nimbus.seeds: [\"192.168.99.170\"] storm.local.dir: \"/var/storm\" # 指定每个机器上可以启动多少个 worker，一个端口号代表一个 supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 # ------------------------- 一台装好了，再装另外的几台，通过 scp 命令 copy scp ~/.bashrc root@eshop-cache02:~/ scp -r /usr/local/storm/ root@eshop-cache02:/usr/local/ 并创建 mkdir /var/storm 和刷新环境变量文件 source ~/.bashrc 一共创建三个节点：在 eshop-cache01、eshop-cache02、eshop-cache03 启动 storm 集群和 ui 界面 # 一个节点 在 01 上 storm nimbus >/dev/null 2>&1 & # 三个节点 storm supervisor >/dev/null 2>&1 & # 也是需要三个节点，否则在 ui 中访问 8000 端口的日志浏览会404 storm logviewer > /dev/null 2>&1 & # 一个节点 在 01 上 storm ui >/dev/null 2>&1 & # 可以通过 jps 命令查看是否已经启动 [root@eshop-cache01 conf]# jps 10116 Jps 1576 Kafka 1447 QuorumPeerMain 9944 nimbus 这里已经被启动了 ui 访问地址：http://eshop-cache01:8080/index.html 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"storm/067.html":{"url":"storm/067.html","title":"067. 讲给 Java 工程师的史上最通俗易懂 Storm 教程：基于集群运行计算拓扑","keywords":"","body":" 067. 基于集群运行计算拓扑 打包 提交作业到 storm 集群 ui 查看 kill 掉某个 storm 作业 067. 基于集群运行计算拓扑 打包 之前配置的 jar 任务，把依赖也打到 jar 中 由于在提交到 storm 集群中遇到的一些坑，最后把调试通过后的 gradle 配置内容贴出来 //指定编译的编码 tasks.withType(JavaCompile) { options.encoding = \"UTF-8\" } version = '1.0.0' // 打包与集群中的服务器中的 java 版本要一致 sourceCompatibility = '1.7' dependencies { // 打包的时候不打包该依赖，因为提交的作业是会被 storm 程序加载 // 所以 storm-core 已经被加载到程序中了 compileOnly 'org.apache.storm:storm-core:1.1.0' compile 'commons-collections:commons-collections:3.2.1' } jar { manifestContentCharset 'utf-8' metadataCharset 'utf-8' manifest { attributes( \"Manifest-Version\": 1.0, \"Main-Class\": \"cn.mrcode.cachepdp.storm.helloword.WordCountTopology\") } from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } } } 提交作业到 storm 集群 语法：storm jar path/to/allmycode.jar org.me.MyTopology arg1 arg2 arg3 # 本次练习，后面的参数是自定义的 # StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology()); # 之前代码中用到了 第一个参数当做 提交的 topology 名称 storm jar storm-helloword-1.0.0.jar cn.mrcode.cachepdp.storm.helloword.WordCountTopology WordCountTopology 启动过程中报错，compileOnly 'org.apache.storm:storm-core:1.1.0' 使用 compileOnly 打包不把该依赖打包 Caused by: java.lang.RuntimeException: java.io.IOException: Found multiple defaults.yaml resources. You're probably bundling the Storm jars with your topology jar. [jar:file:/usr/local/storm/lib/storm-core-1.1.0.jar!/defaults.yaml, jar:file:/usr/local/storm/storm-helloword-0.0.1-SNAPSHOT.jar!/defaults.yaml] at org.apache.storm.utils.Utils.findAndReadConfigFile(Utils.java:383) at org.apache.storm.utils.Utils.readDefaultConfig(Utils.java:427) at org.apache.storm.utils.Utils.readStormConfig(Utils.java:463) at org.apache.storm.utils.Utils.(Utils.java:177) ... 39 more 再次运行报错，有可能是你打包是用 1.8 ，运行是用 1.7 Exception in thread \"main\" java.lang.UnsupportedClassVersionError: cn/mrcode/cachepdp/storm/helloword/WordCountTopology : Unsupported major.minor version 52.0 at java.lang.ClassLoader.defineClass1(Native Method) 运行成功显示 [root@eshop-cache01 storm]# storm jar storm-helloword-1.0.0.jar cn.mrcode.cachepdp.storm.helloword.WordCountTopology WordCountTopology Running: /usr/java/latest/bin/java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/usr/local/storm -Dstorm.log.dir=/usr/local/storm/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /usr/local/storm/storm-helloword-1.0.0.jar:/usr/local/storm/lib/log4j-over-slf4j-1.6.6.jar:/usr/local/storm/lib/reflectasm-1.10.1.jar:/usr/local/storm/lib/storm-core-1.1.0.jar:/usr/local/storm/lib/storm-rename-hack-1.1.0.jar:/usr/local/storm/lib/log4j-api-2.8.jar:/usr/local/storm/lib/ring-cors-0.1.5.jar:/usr/local/storm/lib/log4j-slf4j-impl-2.8.jar:/usr/local/storm/lib/clojure-1.7.0.jar:/usr/local/storm/lib/servlet-api-2.5.jar:/usr/local/storm/lib/asm-5.0.3.jar:/usr/local/storm/lib/objenesis-2.1.jar:/usr/local/storm/lib/disruptor-3.3.2.jar:/usr/local/storm/lib/minlog-1.3.0.jar:/usr/local/storm/lib/log4j-core-2.8.jar:/usr/local/storm/lib/slf4j-api-1.7.21.jar:/usr/local/storm/lib/kryo-3.0.3.jar:storm-helloword-1.0.0.jar:/usr/local/storm/conf:/usr/local/storm/bin -Dstorm.jar=storm-helloword-1.0.0.jar -Dstorm.dependency.jars= -Dstorm.dependency.artifacts={} cn.mrcode.cachepdp.storm.helloword.WordCountTopology WordCountTopology 2228 [main] INFO o.a.s.StormSubmitter - Generated ZooKeeper secret payload for MD5-digest: -6684020447691510709:-7027356525674661306 2524 [main] INFO o.a.s.u.NimbusClient - Found leader nimbus : eshop-cache01:6627 2583 [main] INFO o.a.s.s.a.AuthUtils - Got AutoCreds [] 2589 [main] INFO o.a.s.u.NimbusClient - Found leader nimbus : eshop-cache01:6627 2678 [main] INFO o.a.s.StormSubmitter - Uploading dependencies - jars... 2680 [main] INFO o.a.s.StormSubmitter - Uploading dependencies - artifacts... 2681 [main] INFO o.a.s.StormSubmitter - Dependency Blob keys - jars : [] / artifacts : [] 2713 [main] INFO o.a.s.StormSubmitter - Uploading topology jar storm-helloword-1.0.0.jar to assigned location: /var/storm/nimbus/inbox/stormjar-84efb0f9-e1a4-4d54-a55f-a4fe68adfd62.jar Start uploading file 'storm-helloword-1.0.0.jar' to '/var/storm/nimbus/inbox/stormjar-84efb0f9-e1a4-4d54-a55f-a4fe68adfd62.jar' (581266 bytes) [==================================================] 581266 / 581266 File 'storm-helloword-1.0.0.jar' uploaded to '/var/storm/nimbus/inbox/stormjar-84efb0f9-e1a4-4d54-a55f-a4fe68adfd62.jar' (581266 bytes) 2836 [main] INFO o.a.s.StormSubmitter - Successfully uploaded topology jar to assigned location: /var/storm/nimbus/inbox/stormjar-84efb0f9-e1a4-4d54-a55f-a4fe68adfd62.jar 2837 [main] INFO o.a.s.StormSubmitter - Submitting topology WordCountTopology in distributed mode with conf {\"storm.zookeeper.topology.auth.scheme\":\"digest\",\"storm.zookeeper.topology.auth.payload\":\"-6684020447691510709:-7027356525674661306\",\"topology.workers\":3,\"topology.debug\":false} 3636 [main] INFO o.a.s.StormSubmitter - Finished submitting topology: WordCountTopology ui 查看 访问 http://eshop-cache01:8080/index.html 进入 WordCountTopology 这个拓扑页面之后，点击端口号，可以看到这台机器上面的日志信息 kill 掉某个 storm 作业 语法：storm kill topology-name [root@eshop-cache01 storm]# storm kill WordCountTopology Running: /usr/java/latest/bin/java -client -Ddaemon.name= -Dstorm.options= -Dstorm.home=/usr/local/storm -Dstorm.log.dir=/usr/local/storm/logs -Djava.library.path=/usr/local/lib:/opt/local/lib:/usr/lib -Dstorm.conf.file= -cp /usr/local/storm/storm-helloword-1.0.0.jar:/usr/local/storm/lib/log4j-over-slf4j-1.6.6.jar:/usr/local/storm/lib/reflectasm-1.10.1.jar:/usr/local/storm/lib/storm-core-1.1.0.jar:/usr/local/storm/lib/storm-rename-hack-1.1.0.jar:/usr/local/storm/lib/log4j-api-2.8.jar:/usr/local/storm/lib/ring-cors-0.1.5.jar:/usr/local/storm/lib/log4j-slf4j-impl-2.8.jar:/usr/local/storm/lib/clojure-1.7.0.jar:/usr/local/storm/lib/servlet-api-2.5.jar:/usr/local/storm/lib/asm-5.0.3.jar:/usr/local/storm/lib/objenesis-2.1.jar:/usr/local/storm/lib/disruptor-3.3.2.jar:/usr/local/storm/lib/minlog-1.3.0.jar:/usr/local/storm/lib/log4j-core-2.8.jar:/usr/local/storm/lib/slf4j-api-1.7.21.jar:/usr/local/storm/lib/kryo-3.0.3.jar:/usr/local/storm/conf:/usr/local/storm/bin org.apache.storm.command.kill_topology WordCountTopology 13712 [main] INFO o.a.s.u.NimbusClient - Found leader nimbus : eshop-cache01:6627 16632 [main] INFO o.a.s.c.kill-topology - Killed topology: WordCountTopology 在 ui 上也可以操作 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"068.html":{"url":"068.html","title":"068. 缓存冷启动问题：新系统上线 redis 彻底崩溃导致数据无法恢复","keywords":"","body":"068. 缓存冷启动问题：新系统上线 redis 彻底崩溃导致数据无法恢复 什么是缓存冷启动？简单说就是缓存中没有数据，考虑下面两个场景 新系统第一次上线，此时在缓存里可能是没有数据的 系统在线上稳定运行着，但是突然间重要的 redis 缓存全盘崩溃了，而且不幸的是，数据全都无法找回来 系统第一次上线启动，系统在 redis 故障的情况下重新启动，在高并发的场景下就会出现所有的流量 都会打到 mysql（原始数据库） 上去。可能导致 mysql 崩溃 以下是图示： 本章要主题的点是 「冷启动」，是说缓存中没有数据但是缓存短时间又恢复正常后的流量被大量打到 mysql。 那么还有一种情况是「缓存雪崩」，可能是缓存失效、redis 挂了等，流量被大量打到 mysql 中 注意这两个场景的关注点是不同的。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"069.html":{"url":"069.html","title":"069. 缓存预热解决方案：基于 storm 实时热点统计的分布式并行缓存预热","keywords":"","body":" 069. 缓存预热解决方案：基于 storm 实时热点统计的分布式并行缓存预热 缓存预热基本思路 缓存预热具体实现思路 小结 069. 缓存预热解决方案：基于 storm 实时热点统计的分布式并行缓存预热 缓存预热基本思路 由于缓存冷启动问题，redis 启动后，一点数据都没有，直接就对外提供服务了，mysql 裸奔 提前给 redis 中灌入部分数据，再提供服务 数据量太大的话，无法将所有数据放入 redis 中 耗费时间过长 或 redis 根本无法容纳下所有的数据 需要根据当天的具体访问情况，实时统计出访问频率较高的热数据 然后将访问频率较高的热数据写入 redis 中，肯定数据也比较多， 我们也得多个服务并行读取数据去写，并行的分布式缓存预热 都准备好后，在对外服务，就不至于冷启动，让数据库裸奔了 缓存预热具体实现思路 nginx +lua 将访问流量上报到 kafka 中 要统计出当前最新的实时的热数据是那些，我们就得将商品详情页访问的请求对应的流量 日志实时上报到 kafka 中 storm 从 kafka 中消费数据，实时统计访问次数 访问次数基于 LRU 内存数据结构的存储方案； 为什么要基于 LRU 内存方案？ storm 中读写数据频繁 数据量大 所以不适合依赖 redis 或者 mysql： redis 可能出现故障，会导致 storm 的稳定性 mysql：扛不住高并发读写 hbase：hadoop 生态组合还是不错的，但是对于非专业大数据方向来说，维护太重了 我之前做过的一些项目，一些广告计费类的系统也是用这种方案，有人就直接往 mysql 中去写， 流量上来之后 mysql 直接被打死了 其实我们的需求就是：统计出最近一段时间访问最频繁的商品，进行访问计数， 同时维护出一个前 N 个访问最多的商品 list 即可 也就是热数据：最近一段时间（如最近 1 小时、5 分钟），1 万个商品请求， 统计这段时间内每个商品的访问次数，排序后做出一个 top n 列表 计算好每个 task 大致要存放的商品访问次数的数量，计算出大小， 然后构建一个 LRU MAP，它能够给你一个剩下访问次数最多的商品列表，访问高的才能存活 LRU MAP 有开源的实现，apach commons collections 中有提供，设置好 map 的最大大小， 就会自动根据 LRU 算法去剔除多余的数据，保证内存使用限制， 即时有部分数据被干掉了，下次会从 0 开始统计，也没有关系，因为被 LRU 算法干掉了， 就表示它不是热数据，说明最近一段时间都很少访问了，热度下降了 每个 Storm task 启动时，基于 zk 分布式锁，将自己的 ID 写入 zk 同一个节点中 这个 id 写到一个固定节点中，形成一个 task id 列表， 后续可以通过这个 id 列表去拿到对于 task 存储在 zk node 上的 topn 列表 每个 Storm task 负责完成自己这里的热数据统计 比如每隔一段时间，就遍历下这个 map，维护并更新一个前 n 个商品的 list 定时同步到 zk 中去 写一个后台线程，每隔一段时间，比如 1 分钟，将这个 task 所有的商品排名算一次 将排名前 n 的热数据 list 同步到 zk 中去 需要一个服务，根据 top n 列表在 mysql 中获取数据往 redis 中存 这个服务有会部署多个实例，在启动时会拉取 storm task id 列表， 然后通过 zk 分布式锁，基于 id 去加锁，获取到这个 task id 节点中存储的 topn 列表， 然后读取 mysql 中的数据，存储在 redis 中 这个服务可以是单独的服务，本课程为了方便会放在缓存服务中 这整个方案就是分布式并行缓存预热 小结 思路总结： 使用 stom 实时计算出最近一段时间内的 n 个 topn 列表，并存储在 zk task id 节点上 多服务通过 task id 进行分布式锁，获取 topn 列表，去 mysql 拉取数据放入 redis 中 由于对 storm 不熟悉，这里的思路看来，只是利用了 storm 能创建大量并行的 task 和数据分组策略， 来让大量的访问日志分发到 n 个 task 中，让 storm 这种抗住大量并发访问量的计算能力， 注意这里是计算出 n 个 topn 列表，也就是大量的热数据。而不是唯一的一份 topn 列表， 而且是最近一段时间内的（之前一直想不通 storm 怎么能达到实时计算？原来是通过这种分而治之方式 + 分段时间来重复计算自己负责的部分结果数据实现的，就是不知道 storm 其他的使用场景也是这样的吗？） 也想知道如果想维护一个全局的排行榜名单的话，用 storm 应该怎么做？这个数据量就很大了， 比如淘宝的双 11 的秒级统计成交金额 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"070.html":{"url":"070.html","title":"070. 基于 nginx+lua 完成商品详情页访问流量实时上报 kafka 的开发","keywords":"","body":" 070. 基于 nginx+lua 完成商品详情页访问流量实时上报 kafka 的开发 安装 lua-resty-kafka 脚本编写 kafka topic 创建与消费显示 测试脚本是否达到正常效果 070. 基于 nginx+lua 完成商品详情页访问流量实时上报 kafka 的开发 本章节要实现的就是：在 nginx 这一层，接收到访问请求的时候，就把请求的流量上报发送给 kafka， 这样的话，storm 才能去消费 kafka 中的实时的访问日志，然后去进行缓存热数据的统计 使用到的 lua 工具包：lua-resty-kafka 安装 lua-resty-kafka nginx 三台的作用： eshop-01：应用层 eshop-02：应用层 eshop-03：分发层 我们需要在 01 和 02 应用层上装上该依赖，并编写上报脚本 cd /usr/local/ wget https://github.com/doujiang24/lua-resty-kafka/archive/master.zip yum install -y unzip unzip master.zip # resty 目录下是 kafka 目录，其实就是讲 kafka 目录放到 lualib 中去 cp -rf /usr/local/lua-resty-kafka-master/lib/resty/ /usr/hello/lualib # 加载依赖包，其实后续写完脚本之后也需要 reload 的 /usr/servers/nginx/sbin/nginx -s reload 脚本编写 在 /usr/hello/lua/product.lua 中增加这段逻辑 提示：这种工具类的核心写法，在该工具官网 github 中有示例 该段逻辑由于比较独立，可以放在 product.lua 顶部。 local cjson = require(\"cjson\") -- 引用之前安装的工具包 local producer = require(\"resty.kafka.producer\") local broker_list = { { host = \"192.168.99.170\", port = 9092 }, { host = \"192.168.99.171\", port = 9092 }, { host = \"192.168.99.172\", port = 9092 } } -- 定义日志信息 local log_json = {} log_json[\"headers\"] = ngx.req.get_headers() log_json[\"uri_args\"] = ngx.req.get_uri_args() log_json[\"body\"] = ngx.req.read_body() log_json[\"http_version\"] = ngx.req.http_version() log_json[\"method\"] =ngx.req.get_method() log_json[\"raw_reader\"] = ngx.req.raw_header() log_json[\"body_data\"] = ngx.req.get_body_data() -- 序列化为一个字符串 local message = cjson.encode(log_json); -- local offset, err = p:send(\"test\", key, message) -- 这里的 key 只是作为消息路由分区使用，kafka 中的概念 local productId = ngx.req.get_uri_args()[\"productId\"] -- 异步发送信息 local async_producer = producer:new(broker_list, { producer_type = \"async\" }) local ok, err = async_producer:send(\"access-log\", productId, message) if not ok then ngx.log(ngx.ERR, \"kafka send err:\", err) return end 记得需要 /usr/servers/nginx/sbin/nginx -s reload kafka topic 创建与消费显示 详细内容可参考之前的内容 cd /usr/local/kafka_2.9.2-0.8.1.1 # 创建测试的 topic，名称为 access-log bin/kafka-topics.sh --zookeeper 192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181 --topic access-log --replication-factor 1 --partitions 1 --create # 创建一个消费者 bin/kafka-console-consumer.sh --zookeeper 192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181 --topic access-log --from-beginning 测试脚本是否达到正常效果 记得后端缓存服务需要启动，nginx 本地缓存是有过期时间的，过期后就会去请求后端服务了 访问地址：http://eshop-cache03/product?method=product&productId=1&shopId=1 页面能正常看到商品信息，但是 kafka consumer 无信息 # 查看 nginx 的错误日志发现 tail -f /usr/servers/nginx/logs/error.log 2019/05/07 20:14:49 [error] 9888#0: [lua] producer.lua:258: buffered messages send to kafka err: no resolver defined to resolve \"eshop-cache01\", retryable: true, topic: access-log, partition_id: 0, length: 1, context: ngx.timer, client: 192.168.99.172, server: 0.0.0.0:80 ::: tip 经过实战排错，resolver 8.8.8.8; 可以不配置，只需要修改 kafka 配置文件配置项 advertised.host.name = 对应机器 ip 即可 ::: 解决方法： vi /usr/servers/nginx/conf/nginx.conf 在 http 部分，加入 resolver 8.8.8.8; 再次尝试发现日志变更了 2019/05/07 20:20:55 [error] 9891#0: [lua] producer.lua:258: buffered messages send to kafka err: eshop-cache01 could not be resolved (3: Host not found), retryable: true, topic: access-log, partition_id: 0, length: 1, context: ngx.timer, client: 192.168.99.172, server: 0.0.0.0:80 可以看到日志，的确是去解析了，但是这个是我们本地自定义的肯定解析不到，那么这个问题是哪里的问题呢？ 我懒一点，视频中说到，需要更改 kafka 的配置文件，让用本机 ip 而不是 hostName vi /usr/local/kafka_2.9.2-0.8.1.1/config/server.properties # 默认是 hostname，更改为自己机器的 ip 地址 #advertised.host.name= advertised.host.name = 192.168.99.170 再重启 kafka [root@eshop-cache01 lua]# jps 12698 Jps 12310 logviewer 1576 Kafka kill -9 1576 cd /usr/local/kafka_2.9.2-0.8.1.1 nohup bin/kafka-server-start.sh config/server.properties & # 查看是否启动是否报错 cat nohup.out 再次访问，发现能接受到信息了 [root@eshop-cache01 kafka_2.9.2-0.8.1.1]# bin/kafka-console-consumer.sh --zookeeper 192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181 --topic access-log --from-beginning {\"method\":\"GET\",\"http_version\":1.1,\"raw_reader\":\"GET \\/product?productId=1&shopId=1 HTTP\\/1.1\\r\\nHost: 192.168.99.171\\r\\nUser-Agent: lua-resty-http\\/0.13 (Lua) ngx_lua\\/9014\\r\\n\\r\\n\",\"uri_args\":{\"productId\":\"1\",\"shopId\":\"1\"},\"headers\":{\"host\":\"192.168.99.171\",\"user-agent\":\"lua-resty-http\\/0.13 (Lua) ngx_lua\\/9014\"}} 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"071.html":{"url":"071.html","title":"071. 基于 storm+kafka 完成商品访问次数实时统计拓扑的开发","keywords":"","body":" 071. 基于 storm+kafka 完成商品访问次数实时统计拓扑的开发 项目 build.gradle HotProductTopology 编码 代码测试 071. 基于 storm+kafka 完成商品访问次数实时统计拓扑的开发 本节的代码思路如下： 之前已经完成过 storm hellowd 了，在这模板基础上添加业务代码。 编写消费 kafka 的 spout 编写解析日志的 bolt，获取到商品 id 编写统计商品次数的 bolt 项目 build.gradle 需要重新写一个项目，因为是业务代码了。需要依赖 kafka 等库 //指定编译的编码 tasks.withType(JavaCompile) { options.encoding = \"UTF-8\" } version = '1.0.0' sourceCompatibility = '1.7' dependencies { // 打包的时候不打包该依赖 // 注意：直接在 idea 中运行的话使用 compileOnly 会报错找不到依赖 // 打包的时候，使用 compile ，提交到 storm 中又会报错，所以打包和开发注意下依赖问题 // compileOnly 'org.apache.storm:storm-core:1.1.0' compile 'org.apache.storm:storm-core:1.1.0' compile 'commons-collections:commons-collections:3.2.1' compile 'com.alibaba:fastjson:1.1.43' compile ('org.apache.kafka:kafka_2.9.2:0.8.1.1'){ exclude group: 'org.slf4j', module: 'slf4j-log4j12' } compile ('org.apache.zookeeper:zookeeper:3.4.5'){ exclude group: 'org.slf4j', module: 'slf4j-log4j12' } } jar { manifestContentCharset 'utf-8' metadataCharset 'utf-8' manifest { attributes( \"Manifest-Version\": 1.0, \"Main-Class\": \"cn.mrcode.cachepdp.eshop.storm.HotProductTopology\") } from { configurations.compile.collect { it.isDirectory() ? it : zipTree(it) } } } HotProductTopology 编码 package cn.mrcode.cachepdp.eshop.storm; import org.apache.storm.spout.SpoutOutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichSpout; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Values; import java.util.HashMap; import java.util.List; import java.util.Map; import java.util.Properties; import java.util.concurrent.LinkedTransferQueue; import kafka.consumer.Consumer; import kafka.consumer.ConsumerConfig; import kafka.consumer.ConsumerIterator; import kafka.consumer.KafkaStream; import kafka.javaapi.consumer.ConsumerConnector; import kafka.message.MessageAndMetadata; /** * 消费 kafka 数据的 spout * * @author : zhuqiang * @date : 2019/5/22 23:01 */ public class AccessLogConsumerSpout extends BaseRichSpout { private LinkedTransferQueue queue; private SpoutOutputCollector collector; @Override public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) { queue = new LinkedTransferQueue(); this.collector = collector; startKafka(); } @Override public void nextTuple() { try { // 使用 LinkedTransferQueue 的目的是： // kafka put 会一直阻塞，直到有一个 take 执行，才会返回 // 这里能真实的反应客户端消费 kafka 的能力 // 而不是无限消费，存在内存中 String message = queue.take(); collector.emit(new Values(message)); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(\"message\")); } private ConsumerConnector consumer; private String topic; private void startKafka() { consumer = Consumer.createJavaConsumerConnector(createConsumerConfig( \"192.168.99.170:2181,\" + \"192.168.99.171:2181,\" + \"192.168.99.172:2181\", \"eshop-cache-group\")); this.topic = \"access-log\"; new Thread(new Runnable() { @Override public void run() { Map topicCountMap = new HashMap<>(); topicCountMap.put(topic, 1); Map>> consumerMap = consumer.createMessageStreams(topicCountMap); List> streams = consumerMap.get(topic); for (final KafkaStream stream : streams) { ConsumerIterator it = stream.iterator(); while (it.hasNext()) { MessageAndMetadata next = it.next(); String message = new String(next.message()); try { queue.transfer(message); } catch (InterruptedException e) { e.printStackTrace(); } } } } }).start(); } private ConsumerConfig createConsumerConfig(String a_zookeeper, String a_groupId) { Properties props = new Properties(); props.put(\"zookeeper.connect\", a_zookeeper); props.put(\"group.id\", a_groupId); props.put(\"zookeeper.session.timeout.ms\", \"40000\"); props.put(\"zookeeper.sync.time.ms\", \"200\"); props.put(\"auto.commit.interval.ms\", \"1000\"); return new ConsumerConfig(props); } } package cn.mrcode.cachepdp.eshop.storm; import com.alibaba.fastjson.JSONObject; import org.apache.storm.task.OutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichBolt; import org.apache.storm.tuple.Fields; import org.apache.storm.tuple.Tuple; import org.apache.storm.tuple.Values; import java.util.Map; /** * ${todo} * * @author : zhuqiang * @date : 2019/5/22 23:26 */ public class LogParseBolt extends BaseRichBolt { private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { this.collector = collector; } @Override public void execute(Tuple input) { String message = input.getStringByField(\"message\"); JSONObject jsonObject = JSONObject.parseObject(message); // \"uri_args\":{\"productId\":\"1\",\"shopId\":\"1\"} JSONObject uri_args = jsonObject.getJSONObject(\"uri_args\"); Long productId = uri_args.getLong(\"productId\"); if (productId != null) { collector.emit(new Values(productId)); } } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { declarer.declare(new Fields(\"productId\")); } } package cn.mrcode.cachepdp.eshop.storm; import org.apache.storm.task.OutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichBolt; import org.apache.storm.trident.util.LRUMap; import org.apache.storm.tuple.Tuple; import java.util.Map; /** * ${todo} * * @author : zhuqiang * @date : 2019/5/22 23:29 */ public class ProductCountBolt extends BaseRichBolt { private final LRUMap countMap = new LRUMap(100); @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { } @Override public void execute(Tuple input) { Long productId = input.getLongByField(\"productId\"); Long count = countMap.get(productId); if (count == null) { count = 0L; } countMap.put(productId, ++count); System.out.println(\"商品 \" + productId + \",次数 \" + countMap.get(productId)); } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { } } package cn.mrcode.cachepdp.eshop.storm; import org.apache.storm.Config; import org.apache.storm.LocalCluster; import org.apache.storm.StormSubmitter; import org.apache.storm.generated.AlreadyAliveException; import org.apache.storm.generated.AuthorizationException; import org.apache.storm.generated.InvalidTopologyException; import org.apache.storm.topology.TopologyBuilder; import org.apache.storm.tuple.Fields; import java.util.concurrent.TimeUnit; /** * ${todo} * * @author : zhuqiang * @date : 2019/5/22 22:58 */ public class HotProductTopology { public static void main(String[] args) throws InvalidTopologyException, AuthorizationException, AlreadyAliveException, InterruptedException { // 构建拓扑，也就是手动定义业务流程 // 其他的提交到 storm 集群后，由 storm 去调度在哪些机器上启动你所定义的 拓扑 TopologyBuilder builder = new TopologyBuilder(); // id、spout、并发数量 builder.setSpout(AccessLogConsumerSpout.class.getSimpleName(), new AccessLogConsumerSpout(), 2); builder.setBolt(LogParseBolt.class.getSimpleName(), new LogParseBolt(), 5) .setNumTasks(5) .shuffleGrouping(AccessLogConsumerSpout.class.getSimpleName()); builder.setBolt(ProductCountBolt.class.getSimpleName(), new ProductCountBolt(), 5) .setNumTasks(5) .fieldsGrouping(LogParseBolt.class.getSimpleName(), new Fields(\"productId\")); Config conf = new Config(); conf.setDebug(false); if (args != null && args.length > 0) { // 表示在命令行中运行的，需要提交的 storm 集群中去 conf.setNumWorkers(3); StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology()); } else { conf.setMaxTaskParallelism(3); LocalCluster cluster = new LocalCluster(); cluster.submitTopology(\"HotProductTopology\", conf, builder.createTopology()); TimeUnit.SECONDS.sleep(60); cluster.shutdown(); } } } 代码测试 先本地运行 HotProductTopology 访问地址：http://eshop-cache03/product?method=product&productId=1&shopId=1 这个时候回分发到两个 应用层 ningx 上，就会上报到 kafka。 观察打印的日志信息，访问一次就会打印一次 商品 1,次数 1 商品 1,次数 2 商品 1,次数 3 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"072.html":{"url":"072.html","title":"072. 基于 storm 完成 LRUMap 中 topn 热门商品列表的算法讲解与编写","keywords":"","body":" 072. 基于 storm 完成 LRUMap 中 topn 热门商品列表的算法讲解与编写 top n 简易算法 商品列表计算 top n 另外一种简单的 top n 算法 072. 基于 storm 完成 LRUMap 中 topn 热门商品列表的算法讲解与编写 top n 简易算法 public static void main(String[] args) { /** * top n 简易算法：手写思路 * top 3 列表： 5、3、1 * 比如来一个 6，那么比 5 大，把 5 所在位置往后移位。最后把 6 放到 第一位， * 变成：6、5、3 */ int n = 10; int[] topn = new int[n]; // 循环 n 次，模拟有这么多数据需要计算 for (int i = 0; i target) { // 从当前位置往后移动一位 System.arraycopy(topn, j, topn, j + 1, n - (j + 1)); topn[j] = randomNum; break; } } } // 某一次的输出结果 [99, 99, 99, 99, 99, 96, 93, 93, 91, 91] System.out.println(Arrays.toString(topn)); } 商品列表计算 top n public class ProductCountBolt extends BaseRichBolt { private final LRUMap countMap = new LRUMap(100); @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { // 启动一个线程，1 分钟计算一次 new Thread(new Runnable() { @Override public void run() { int n = 3; Map.Entry[] top = new Map.Entry[n]; while (true) { Arrays.fill(top, null); Utils.sleep(6000); for (Map.Entry entry : countMap.entrySet()) { long value = entry.getValue(); for (int i = 0; i targetObj = top[i]; if (targetObj == null) { top[i] = entry; break; } long target = targetObj.getValue(); if (value > target) { // 使用数组 + 系统原生 copy ，性能很棒 // 而且 top n 不大的话，更快 System.arraycopy(top, i, top, i + 1, n - (i + 1)); top[i] = entry; break; } } } System.out.println(Thread.currentThread().getName() + \"：\" + Arrays.toString(top)); } } }).start(); } } 启动 storm 后，来测试统计是否正确 访问：http://eshop-cache03/product?method=product&productId=11&shopId=1 这里为了能让商品 id 能落到同一个 task 上，选择了商品 id：2、5、8、11 总共 4个进行访问次数测试 统计输出如下 商品 5,次数 1 商品 2,次数 1 Thread-41：[null, null, null] Thread-40：[null, null, null] Thread-39：[5=1, 2=1, null] // 后面的非 39 线程的我就删除了，为了看着清晰一点 商品 2,次数 2 Thread-39：[2=2, 5=1, null] Thread-39：[2=2, 5=1, null] Thread-39：[2=2, 5=1, null] Thread-39：[2=2, 5=1, null] Thread-39：[2=2, 5=1, null] Thread-39：[2=2, 5=1, null] 商品 8,次数 1 Thread-39：[2=2, 5=1, 8=1] // top 列表中 3 个都满了 Thread-39：[2=2, 5=1, 8=1] 商品 11,次数 1 // 再来一个 11 ，看看效果 Thread-39：[2=2, 5=1, 8=1] // 结果没有看到 11，这个很正常，访问次数都不比现在的大，所以没有入围 商品 11,次数 2 Thread-39：[2=2, 11=2, 5=1] // 当访问两次后，商品 8 被挤掉了 商品 11,次数 3 Thread-39：[11=3, 2=2, 5=1] Thread-39：[11=3, 2=2, 5=1] 从日志测试来看，这个算法是没有问题的 另外一种简单的 top n 算法 这种 topn 算法没有那么麻烦，思路也很清晰 public static void topn() { int n = 10; int[] topn = new int[n]; // 循环 n 次，模拟有这么多数据需要计算 for (int i = 0; i = target) { topn[j] = randomNum; randomNum = target; } } } System.out.println(Arrays.toString(topn)); } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"073.html":{"url":"073.html","title":"073. 基于 storm+zookeeper 完成热门商品列表的分段存储","keywords":"","body":" 073. 基于 storm+zookeeper 完成热门商品列表的分段存储 改造 zk 工具类 实现分段存储后的 ProductCountBolt 073. 基于 storm+zookeeper 完成热门商品列表的分段存储 分段存储的思路： 每个 task 启动时，将自己的 task id 存储至 zk 中的 hot-product-task-list 节点 每个 task 在计算完一次 top n 时，将自己的 列表存储在 hot-product-task-task id 节点中 改造 zk 工具类 改造了 分布式锁的获取与释放，path 传递，而不再写死代码中了 新增了获取/写入节点数据 public class ZooKeeperSession { private final ZooKeeper zookeeper; private final CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeperSession() { String connectString = \"192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181\"; int sessionTimeout = 5000; try { // 异步连接，所以需要一个 org.apache.zookeeper.Watcher 来通知 // 由于是异步，利用 CountDownLatch 来让构造函数等待 zookeeper = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { Watcher.Event.KeeperState state = event.getState(); System.out.println(\"watch event：\" + state); if (state == Watcher.Event.KeeperState.SyncConnected) { System.out.println(\"zookeeper 已连接\"); connectedSemaphore.countDown(); } } }); } catch (IOException e) { e.printStackTrace(); } try { connectedSemaphore.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"zookeeper 初始化成功\"); } /** * 获取分布式锁 */ public void acquireDistributedLock(String path) { byte[] data = \"\".getBytes(); try { // 创建一个临时节点，后面两个参数一个安全策略，一个临时节点类型 // EPHEMERAL：客户端被断开时，该节点自动被删除 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(\"获取锁成功 [path=\" + path + \"]\"); } catch (Exception e) { e.printStackTrace(); // 如果锁已经被创建，那么将异常 // 循环等待锁的释放 int count = 0; while (true) { try { TimeUnit.MILLISECONDS.sleep(20); // 休眠 20 毫秒后再次尝试创建 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); } catch (Exception e1) { // e1.printStackTrace(); count++; continue; } System.out.println(\"获取锁成功 [path=\" + path + \"] 尝试了 \" + count + \" 次.\"); break; } } } /** * 释放分布式锁 */ public void releaseDistributedLock(String path) { try { zookeeper.delete(path, -1); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } /** * 写节点数据 */ public void setNodeData(String path, String data) { try { Stat exists = zookeeper.exists(path, false); if (exists == null) { // 节点不存在，先创建 PERSISTENT 持久连接 zookeeper.create(path, data.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); return; } zookeeper.setData(path, data.getBytes(), -1); } catch (KeeperException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } } public String getNodeData(String path) { try { return new String(zookeeper.getData(path, false, new Stat())); } catch (KeeperException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } return null; } private static final ZooKeeperSession instance = new ZooKeeperSession(); public static ZooKeeperSession getInstance() { return instance; } } 实现分段存储后的 ProductCountBolt package cn.mrcode.cachepdp.eshop.storm; import com.alibaba.fastjson.JSON; import org.apache.storm.task.OutputCollector; import org.apache.storm.task.TopologyContext; import org.apache.storm.topology.OutputFieldsDeclarer; import org.apache.storm.topology.base.BaseRichBolt; import org.apache.storm.trident.util.LRUMap; import org.apache.storm.tuple.Tuple; import org.apache.storm.utils.Utils; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import java.util.Map; public class ProductCountBolt extends BaseRichBolt { private final LRUMap countMap = new LRUMap(100); private ZooKeeperSession zooKeeperSession; private int taskId = -1; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) { taskId = context.getThisTaskId(); zooKeeperSession = ZooKeeperSession.getInstance(); // 启动一个线程，1 分钟计算一次 topnStart(); // 上报自己的节点 id 到列表中 writeTaskPathToZk(); } private void topnStart() { new Thread(new Runnable() { @Override public void run() { int n = 3; Map.Entry[] top = new Map.Entry[n]; while (true) { Arrays.fill(top, null); Utils.sleep(6000); for (Map.Entry entry : countMap.entrySet()) { long value = entry.getValue(); for (int i = 0; i targetObj = top[i]; if (targetObj == null) { top[i] = entry; break; } long target = targetObj.getValue(); if (value > target) { System.arraycopy(top, i, top, i + 1, n - (i + 1)); top[i] = entry; break; } } } System.out.println(Thread.currentThread().getName() + \"：\" + Arrays.toString(top)); // 把结果接入到 zk 上 writeTopnToZk(top); } } }).start(); } private void writeTaskPathToZk() { // 由于该操作是并发操作，需要通过分布式锁来写入 final String lockPath = \"/hot-product-task-list-lock\"; final String taskListNode = \"/hot-product-task-list\"; zooKeeperSession.acquireDistributedLock(lockPath); String nodeData = zooKeeperSession.getNodeData(taskListNode); // 已经存在数据的话，把自己的 task id 追加到尾部 if (nodeData != null && !\"\".equals(nodeData)) { nodeData += \",\" + taskId; } else { nodeData = taskId + \"\"; } zooKeeperSession.setNodeData(taskListNode, nodeData); zooKeeperSession.releaseDistributedLock(lockPath); } private void writeTopnToZk(Map.Entry[] topn) { List proudcts = new ArrayList<>(); for (Map.Entry t : topn) { if (t == null) { continue; } proudcts.add(t.getKey()); } final String taskNodePath = \"/hot-product-task-\" + taskId; zooKeeperSession.setNodeData(taskNodePath, JSON.toJSONString(proudcts)); } @Override public void execute(Tuple input) { Long productId = input.getLongByField(\"productId\"); Long count = countMap.get(productId); if (count == null) { count = 0L; } countMap.put(productId, ++count); System.out.println(\"商品 \" + productId + \",次数 \" + countMap.get(productId)); } @Override public void declareOutputFields(OutputFieldsDeclarer declarer) { } } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"074.html":{"url":"074.html","title":"074. 基于双重 zookeeper 分布式锁完成分布式并行缓存预热的代码开发","keywords":"","body":" 074. 基于双重 zookeeper 分布式锁完成分布式并行缓存预热的代码开发 并行缓存预热思路 服务启动启动预热与 spring 实例工具类封装 zk 工具类的改造 缓存预热逻辑编写 074. 基于双重 zookeeper 分布式锁完成分布式并行缓存预热的代码开发 并行缓存预热思路 服务启动的时候，进行缓存预热 从 zk 中读取 taskid 列表 依次遍历每个 taskid，尝试获取分布式锁，如果获取不到，快速报错，不要等待，因为说明已经有其他服务实例在预热了 直接尝试获取下一个 taskid 的分布式锁 即使获取到了分布式锁，也要检查一下这个 taskid 的预热状态，如果已经被预热过了，就不再预热了 预热状态，也是一个 node path 来存储的，每个 task 一个状态节点 执行预热操作，遍历 productid 列表，查询数据，然后写 ehcache 和 redis 预热完成后，设置 taskid 对应的预热状态 服务启动启动预热与 spring 实例工具类封装 由于需要在缓存预热的线程中使用缓存服务进行存储，这里需要封装一个 spring bean 获取工具类 package cn.mrcode.cachepdp.eshop.cache; import org.springframework.web.context.WebApplicationContext; import org.springframework.web.context.support.WebApplicationContextUtils; import javax.servlet.ServletContext; import javax.servlet.ServletContextEvent; import javax.servlet.ServletContextListener; import cn.mrcode.cachepdp.eshop.cache.prewarm.CachePrewarmThread; /** * @author : zhuqiang * @date : 2019/5/25 15:58 */ @Component public class InitListener implements ServletContextListener { @Override public void contextInitialized(ServletContextEvent sce) { ServletContext servletContext = sce.getServletContext(); WebApplicationContext webApplicationContext = WebApplicationContextUtils.getWebApplicationContext(servletContext); SpringContextUtil.setWebApplicationContext(webApplicationContext); new CachePrewarmThread().start(); } } public class SpringContextUtil { private static WebApplicationContext context; public static WebApplicationContext getWebApplicationContext() { return context; } public static void setWebApplicationContext(WebApplicationContext webApplicationContext) { context = webApplicationContext; } } zk 工具类的改造 在思路里面提到了，需要快速失败的一个加锁方式，还有写入/获取数据的方法。 在缓存服务里面的 zk 工具类还没有这样的功能，对这个进行改造 由于之前写过，这些代码都是体力活了，不想贴上来了 public class ZooKeeperSession { private final ZooKeeper zookeeper; private final CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeperSession() { String connectString = \"192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181\"; int sessionTimeout = 5000; try { // 异步连接，所以需要一个 org.apache.zookeeper.Watcher 来通知 // 由于是异步，利用 CountDownLatch 来让构造函数等待 zookeeper = new ZooKeeper(connectString, sessionTimeout, event -> { Watcher.Event.KeeperState state = event.getState(); System.out.println(\"watch event：\" + state); if (state == Watcher.Event.KeeperState.SyncConnected) { System.out.println(\"zookeeper 已连接\"); connectedSemaphore.countDown(); } }); } catch (IOException e) { e.printStackTrace(); } try { connectedSemaphore.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"zookeeper 初始化成功\"); } /** * 获取分布式锁 */ public void acquireDistributedLock(Long productId) { String path = \"/product-lock-\" + productId; byte[] data = \"\".getBytes(); try { // 创建一个临时节点，后面两个参数一个安全策略，一个临时节点类型 // EPHEMERAL：客户端被断开时，该节点自动被删除 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(\"获取锁成功 product[id=\" + productId + \"]\"); } catch (Exception e) { e.printStackTrace(); // 如果锁已经被创建，那么将异常 // 循环等待锁的释放 int count = 0; while (true) { try { TimeUnit.MILLISECONDS.sleep(20); // 休眠 20 毫秒后再次尝试创建 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); } catch (Exception e1) { // e1.printStackTrace(); count++; continue; } System.out.println(\"获取锁成功 product[id=\" + productId + \"] 尝试了 \" + count + \" 次.\"); break; } } } /** * 释放分布式锁 */ public void releaseDistributedLock(Long productId) { try { String path = \"/product-lock-\" + productId; zookeeper.delete(path, -1); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } /** * 获取分布式锁 */ public void acquireDistributedLock(String path) { byte[] data = \"\".getBytes(); try { // 创建一个临时节点，后面两个参数一个安全策略，一个临时节点类型 // EPHEMERAL：客户端被断开时，该节点自动被删除 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(\"获取锁成功 [path=\" + path + \"]\"); } catch (Exception e) { e.printStackTrace(); // 如果锁已经被创建，那么将异常 // 循环等待锁的释放 int count = 0; while (true) { try { TimeUnit.MILLISECONDS.sleep(20); // 休眠 20 毫秒后再次尝试创建 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); } catch (Exception e1) { // e1.printStackTrace(); count++; continue; } System.out.println(\"获取锁成功 [path=\" + path + \"] 尝试了 \" + count + \" 次.\"); break; } } } /** * 获取分布式锁；快速失败，不等待 */ public boolean acquireFastFailDistributedLock(String path) { byte[] data = \"\".getBytes(); try { // 创建一个临时节点，后面两个参数一个安全策略，一个临时节点类型 // EPHEMERAL：客户端被断开时，该节点自动被删除 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(\"获取锁成功 [path=\" + path + \"]\"); return true; } catch (Exception e) { System.out.println(\"获取锁失败 [path=\" + path + \"]\"); return false; } } /** * 释放分布式锁 */ public void releaseDistributedLock(String path) { try { zookeeper.delete(path, -1); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } /** * 写节点数据 */ public void setNodeData(String path, String data) { try { Stat exists = zookeeper.exists(path, false); if (exists == null) { // 节点不存在，先创建 PERSISTENT 持久连接 zookeeper.create(path, data.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); return; } zookeeper.setData(path, data.getBytes(), -1); } catch (KeeperException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } } public String getNodeData(String path) { try { return new String(zookeeper.getData(path, false, new Stat())); } catch (KeeperException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } return null; } private static final ZooKeeperSession instance = new ZooKeeperSession(); public static ZooKeeperSession getInstance() { return instance; } } 缓存预热逻辑编写 public class CachePrewarmThread extends Thread { @Override public void run() { // 1. 获取 task id 列表 ZooKeeperSession zk = ZooKeeperSession.getInstance(); final String taskListNode = \"/hot-product-task-list\"; String taskListNdeData = zk.getNodeData(taskListNode); if (taskListNode == null || \"\".equals(taskListNdeData)) { System.err.println(\"task list 为空\"); return; } CacheService cacheService = SpringContextUtil.getWebApplicationContext().getBean(CacheService.class); String[] taskList = taskListNdeData.split(\",\"); for (String taskId : taskList) { final String taskNodeLockPath = \"/hot-product-task-lock-\" + taskId; // 尝试获取该节点的锁，如果获取失败，说明被其他服务预热了 if (!zk.acquireFastFailDistributedLock(taskNodeLockPath)) { continue; } // 疑问：视频中为什么需要在这里对 预热数据节点加锁？ // 获取 检查预热状态 final String taskNodePrewarmStatePath = \"/hot-product-task-prewarm-state\" + taskId; String taskNodePrewarmState = zk.getNodeData(taskNodePrewarmStatePath); // 已经被预热过了 if (taskNodePrewarmState != null && !\"\".equals(taskNodePrewarmState)) { zk.releaseDistributedLock(taskNodeLockPath); continue; } // 还未被预热过，读取 topn 列表，并从数据库中获取商品信息，存入缓存中 final String taskNodePath = \"/hot-product-task-\" + taskId; String nodeData = zk.getNodeData(taskNodePath); if (nodeData == null && \"\".equals(nodeData)) { // 如果没有数据则不处理 zk.releaseDistributedLock(taskNodeLockPath); continue; } List pids = JSON.parseArray(nodeData, Long.class); // 假设这里是从数据库中获取的数据 pids.forEach(pid -> { ProductInfo productInfo = getProduct(pid); System.out.println(\"预热缓存信息：\" + productInfo); cacheService.saveProductInfo2LocalCache(productInfo); cacheService.saveProductInfo2ReidsCache(productInfo); }); // 修改预热状态 zk.setNodeData(taskNodePrewarmStatePath, \"success\"); // 释放该 task 节点的锁 zk.releaseDistributedLock(taskNodeLockPath); } } private ProductInfo getProduct(Long pid) { String productInfoJSON = \"{\\\"id\\\": \" + pid + \", \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; return JSON.parseObject(productInfoJSON, ProductInfo.class); } } 针对这个逻辑，我有几个疑问： 疑问：视频中为什么需要在这里对 预热数据节点加锁？ 在代码逻辑上来看，对该 task 节点加锁之后，也就只能是加锁的机器才能访问本任务对应的预热节点数据，为什么还需要加锁？ 疑问：这里缓存存入会存在数据竞争吗？ 换句话说，在 storm 中统计中，同一个商品 id 始终只会路由到一个 task 中吗？ 有一点没有搞明白，之前课程已经说过同一个商品 id 始终只会路由到一个 task 中， 如果这个能保证，那么这里就不会存在数据竞争问题。 相当于每个 task 中的 topn 的商品 id 都是独有的 疑问：缓存预热场景到现在也还没有明白 缓存预热只适合在已有系统？不然预热的访问数据从何而来？ 疑问：视频中把预热启动缓存的放在了 controller 中，不会与缓存重建产生冲突吗？ 这章节的思路，是项目重启的时候去异步预热缓存，如果这个时候对外开放了服务， 那么触发了缓存重建相关的操作，就会出现数据入缓存冲突的问题； 但是我记得之前说缓存预热的场景是，先预热，预热完成后，再对外开放服务。 如果是这样的话，那么这里是没有问题的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"075.html":{"url":"075.html","title":"075. 将缓存预热解决方案的代码运行后观察效果以及调试和修复所有的 bug","keywords":"","body":" 075. 将缓存预热解决方案的代码运行后观察效果以及调试和修复所有的 bug 运行 HotProductTopology 并观察日志 测试流程 storm 进程老是消失问题 本节总结 075. 将缓存预热解决方案的代码运行后观察效果以及调试和修复所有的 bug 本章节，视频中花费了一小时的时间调流程，出现 bug 的地方是 topn 算法有问题 和 zk 写数据没有先创建节点导致的。还有一部分是日志打的不如意； 这些 bug 导致该视频耗时 50 多分钟，幸好，本人前面的笔记，topn 写完是调试过的，所以问题不大 本章就是来测试之前的整个流程： 提交 HotProductTopology 到 storm 集群中 访问 ngxin 分发服务，让访问日志到达 kafka 中 观察 HotProductTopology 统计日志 通过 zkCli.sh 查看对应节点数据是否正常 重启动缓存服务，触发缓存预热，查看缓存预热日志是否正常 由于 nginx 分发层的逻辑是需要调用缓存服务的， 而之前缓存预热的是放在了项目启动时，所以这里还是按照视频中把缓存预热入口， 卸载一个 controller 中，方便这里测试 运行 HotProductTopology 并观察日志 打包的时候报错 Errors occurred while build effective model from E:\\SoftwareDevelop\\Repository\\caches\\modules-2\\files-2.1\\log4j\\log4j\\1.2.15\\a09f05bb79a0acabbe979f67ed4fbbbc07a368c1\\log4j-1.2.15.pom: 'build.plugins.plugin[io.spring.gradle.dependencymanagement.org.apache.maven.plugins:maven-antrun-plugin].dependencies.dependency.scope' for junit:junit:jar must be one of [compile, runtime, system] but is 'test'. in log4j:log4j:1.2.15 运行依赖检查 H:\\dev\\project\\mrcode\\cache-pdp\\eshop-storm>gradle dependencyInsight --dependency jmxtools \\--- log4j:log4j:1.2.15 +--- com.101tec:zkclient:0.3 | \\--- org.apache.kafka:kafka_2.9.2:0.8.1.1 | \\--- compileClasspath \\--- org.apache.zookeeper:zookeeper:3.4.5 +--- compileClasspath +--- org.apache.kafka:kafka_2.9.2:0.8.1.1 (*) \\--- com.101tec:zkclient:0.3 (*) 发现还依赖了 log4j，之前排除了一个，看来是引入了 zk 引起的传递依赖，需要再排除下 compile ('org.apache.kafka:kafka_2.9.2:0.8.1.1'){ exclude group: 'org.slf4j', module: 'slf4j-log4j12' exclude group: 'log4j', module: 'log4j' } compile ('org.apache.zookeeper:zookeeper:3.4.5'){ exclude group: 'org.slf4j', module: 'slf4j-log4j12' exclude group: 'log4j', module: 'log4j' } storm jar eshop-storm-1.0.0.jar cn.mrcode.cachepdp.eshop.storm.HotProductTopology HotProductTopology 启动后发现卡主了 [root@eshop-cache01 storm]# storm jar eshop-storm-1.0.0.jar cn.mrcode.cachepdp.eshop.storm.HotProductTopology HotProductTopologyxxxxxx-Dstorm.dependency.jars= -Dstorm.dependency.artifacts={} cn.mrcode.cachepdp.eshop.storm.HotProductTopology HotProductTopology watch event：SyncConnected zookeeper 已连接 就卡在这个地方了 在本地启动发现同样被卡在这个地方；最后发现只要 new ZooKeeper 的时候使用 内部类 匿名内部类 new Watcher()，就会卡主，而使用拉姆达表达式就不会，不知道是怎么回事；看了视频中的代码之后，最终测试出，问题出在单例的获取上，只能使用静态内部类的方式 延迟获取实例，直接一上来就 new 就会卡主；最终修改为下面这样了 package cn.mrcode.cachepdp.eshop.storm; import org.apache.zookeeper.CreateMode; import org.apache.zookeeper.KeeperException; import org.apache.zookeeper.WatchedEvent; import org.apache.zookeeper.Watcher; import org.apache.zookeeper.ZooDefs; import org.apache.zookeeper.ZooKeeper; import org.apache.zookeeper.data.Stat; import java.io.IOException; import java.util.concurrent.CountDownLatch; import java.util.concurrent.TimeUnit; /** * ${todo} * * @author : zhuqiang * @date : 2019/5/7 22:37 */ public class ZooKeeperSession { private final ZooKeeper zookeeper; private final CountDownLatch connectedSemaphore = new CountDownLatch(1); private ZooKeeperSession() { String connectString = \"192.168.99.170:2181,192.168.99.171:2181,192.168.99.172:2181\"; int sessionTimeout = 5000; try { // 异步连接，所以需要一个 org.apache.zookeeper.Watcher 来通知 // 由于是异步，利用 CountDownLatch 来让构造函数等待 zookeeper = new ZooKeeper(connectString, sessionTimeout, new Watcher() { @Override public void process(WatchedEvent event) { Watcher.Event.KeeperState state = event.getState(); System.out.println(\"watch event：\" + state); if (state == Watcher.Event.KeeperState.SyncConnected) { System.out.println(\"zookeeper 已连接\"); connectedSemaphore.countDown(); } } }); } catch (IOException e) { e.printStackTrace(); } try { connectedSemaphore.await(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"zookeeper 初始化成功\"); } /** * 获取分布式锁 */ public void acquireDistributedLock(String path) { byte[] data = \"\".getBytes(); try { // 创建一个临时节点，后面两个参数一个安全策略，一个临时节点类型 // EPHEMERAL：客户端被断开时，该节点自动被删除 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(\"获取锁成功 [path=\" + path + \"]\"); } catch (Exception e) { e.printStackTrace(); // 如果锁已经被创建，那么将异常 // 循环等待锁的释放 int count = 0; while (true) { try { TimeUnit.MILLISECONDS.sleep(20); // 休眠 20 毫秒后再次尝试创建 zookeeper.create(path, data, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); } catch (Exception e1) { // e1.printStackTrace(); count++; continue; } System.out.println(\"获取锁成功 [path=\" + path + \"] 尝试了 \" + count + \" 次.\"); break; } } } /** * 释放分布式锁 */ public void releaseDistributedLock(String path) { try { zookeeper.delete(path, -1); } catch (InterruptedException e) { e.printStackTrace(); } catch (KeeperException e) { e.printStackTrace(); } } /** * 写节点数据 */ public void setNodeData(String path, String data) { try { Stat exists = zookeeper.exists(path, false); if (exists == null) { // 节点不存，先创建 PERSISTENT 持久连接 zookeeper.create(path, data.getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); return; } zookeeper.setData(path, data.getBytes(), -1); } catch (KeeperException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } } public String getNodeData(String path) { try { return new String(zookeeper.getData(path, false, new Stat())); } catch (KeeperException e) { e.printStackTrace(); } catch (InterruptedException e) { e.printStackTrace(); } return null; } /** * 封装单例的静态内部类 * * @author Administrator */ private static class Singleton { private static final ZooKeeperSession instance; static { instance = new ZooKeeperSession(); } public static ZooKeeperSession getInstance() { return instance; } } /** * 获取单例 */ public static ZooKeeperSession getInstance() { return Singleton.getInstance(); } public static void main(String[] args) throws InterruptedException { ZooKeeperSession instance = ZooKeeperSession.getInstance(); } } 测试流程 经过几个小时的折腾，代码层面的问题解决之后，提交到了 storm 中； 在后端服务未启动的情况下访问 :http://eshop-cache03/product?method=product&productId=1&shopId=1 浏览器中会显示 500 错误，是因为 kafka 访问日志上报了，但是后面的逻辑去 缓存服务获取 缓存连接不上缓存服务，导致 500 错误。 这个时候查看一下 storm 日志，发现正常在统计了 然后启动缓存服务，触发缓存预热任务 获取锁成功 [path=/hot-product-task-lock-5] at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1151) at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1180) 预热缓存信息：ProductInfo{id=1, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} at cn.mrcode.cachepdp.eshop.cache.ZooKeeperSession.getNodeData(ZooKeeperSession.java:178) at cn.mrcode.cachepdp.eshop.cache.prewarm.CachePrewarmThread.run(CachePrewarmThread.java:42) 预热缓存信息：ProductInfo{id=3, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} 获取锁成功 [path=/hot-product-task-lock-5] 从日志看来，的确从 zk 获取到了日志信息， storm 进程老是消失问题 # 重启发现一会就进程又没有了 storm supervisor >/dev/null 2>&1 & # 最后通过 storm supervisor 启动，查看打印的日志 # 发现是加载了 storm-core-1.1.0.jar 的 jar 包，报错 log4j 日志问题 # 可能这个包是之前的，不知道为什么没有更新到 # 手动删除该 jar 包 rm -rf /usr/local/storm/lib/storm-core-1.1.0.jar # 重启 storm 相关进程 storm supervisor >/dev/null 2>&1 & storm logviewer > /dev/null 2>&1 & 能正常启动了；特别注意一件事情，在分布式系统中，记得同步时间， 因为学习这个课程是断断续续的，休眠之后，时间就停止了。 本节总结 血的教训，视频中花费了 1 小时时间调试 bug。 而我则花费了 4 个小时。问题总结如下： topn 算法部分写完自己本地测试过了 但是后续的 zk 分段存储和商品预热写完没有在本地测试，导致 zk 初始化在不同 jdk 版本下出现问题。 storm 打包排除日志文件有点变态，很不方便 storm 服务器时间不同步 zk 操作在这个代码中很粗糙，哪里都在打印异常。所以这里主要还是学习思想，而不是代码 最后还好，自己坚持把这个流程跑通了。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"076.html":{"url":"076.html","title":"076. 热点缓存问题：促销抢购时的超级热门商品可能导致系统全盘崩溃的场景","keywords":"","body":"076. 热点缓存问题：促销抢购时的超级热门商品可能导致系统全盘崩溃的场景 热数据 -> 热数据的统计 -> redis 中缓存的预热要解决的场景是：避免新系统刚上线，或者是 redis 崩溃数据丢失后重启， redis 中没有数据，redis 冷启动 -> 大量流量直接到数据库； redis 启动前，必须确保其中是有部分热数据的缓存的？什么意思？不是缓存预热就是为了存入热数据到 redis 中吗？ 疑问：但是这个场景貌似缺少前置条件？访问记录从何而来？而且是实时的？没有搞明白 总的来说，缓存预热的课程不是完整的，一些上下文没有交代清楚，让人无法串联知识点 现在这个热点缓存问题，是瞬间的缓存热点，如秒杀，简单说就是负载均衡的特点问题，导致大量访问瞬间被被路由到同一台机器 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"077.html":{"url":"077.html","title":"077. 基于 nginx+lua+storm 的热点缓存的流量分发策略自动降级解决方案","keywords":"","body":" 077. 基于 nginx+lua+storm 的热点缓存的流量分发策略自动降级解决方案 热点缓存的根本原因 自动降级路由策略要点 使用 storm 实时计算出瞬时出现的热点 热点数据上报分发层 nginx 分发层 nginx 降级（改变）流量分发策略 热点商品生命周期管理 总结 077. 基于 nginx+lua+storm 的热点缓存的流量分发策略自动降级解决方案 热点缓存的根本原因 根本原因是，在获取商品信息的时候，分发层 nginx 是根据 hash 策略路由到 应用层 nginx 中的， 包括一系列的缓存重建（之前的课程中我也是有很多疑问的，这里假定说方案是完美的）。 由于路由策略问题，导致某一个商品（如秒杀）全部分发都相同的机器上了，导致机器扛不住。 简而言之：路由策略路由到某一台机器，导致该机器扛不住这么多的访问量 那么这里的解决方案是：hash 单一路由降级为随机路由 自动降级路由策略要点 热点商品依据是什么？ 什么时机进行降级？ 下面解决这两个要点来探讨 使用 storm 实时计算出瞬时出现的热点 有多重算法，这里介绍一种简单的。 我们的商品访问次数存放在 LRUMAP 中，并且定时计算 topn 上传到 zk 中。 那么对于瞬时热点数据来说，统计频率需要变得再高一点，比如 5 秒一次统计。 核心思路： 对 LRUMAP 中数据进行排序，计算出后 95% 的商品平均访问值 设置热点商品阈值 前百分之 5 的商品访问次数对于这个平均访问值倍数差， 如：平均访问值是 100，阈值是 5倍，那么超过 500 访问量的商品则视为热点商品 该商品可能在短时间内继续扩大访问量，超过阈值 n 倍 热点数据和热数据不是一个概念： 热数据：之前做的 topn ，视为热数据 热点数据：热数据中的某个商品访问量，瞬间超出了普通商品的 n 倍，视为热点数据 热点数据上报分发层 nginx 热点数据计算出来后，也就是被 storm 实时计算感知到了，那么就可以上报到 nginx 了 storm 发送 htpp 请求到 nginx 上，用 lua 脚本处理请求，拉取缓存热点商品缓存数据 缓存到 nginx 上。 分发层 nginx 降级（改变）流量分发策略 现在所有 nginx 上都缓存了热点商品缓存，但是由于 hash 流量分发策略，并不能让所有 nginx 为之服务，所以需要对热点商品改变流量分发策略，让所有请求均衡的分发到所有 nginx 上 本列中使用随机分发策略。自己可以扩展一下均衡负载相关算法。 热点商品生命周期管理 不能说长期的把这些商品缓存和流量降级策略长期保持，还需要 storm 进行计算感知， 如秒杀过后，该商品的访问量其实就降低到了普通商品访问量了， 这个时候就需要通知 nginx 取消降级和清楚该商品缓存 实现思路可以是： 下次去识别的时候，这次的热点 list 跟上次的热点 list 做一下 diff，看看可能有的商品已经不是热点了； 很重要的一点，有一个前置条件一定要记得：我们使用了 LRUMap，最近最少使用在达到 Map 最大条目时， 最近未被使用的会被驱逐，这个也能反映某一段时间内该商品不是热点数据了。最少的更加不是热点商品数据了 所以这里可以保存上一次的热点列表，做 diff，然后执行取消热点逻辑 热点的取消的逻辑，发送 http 请求到流量分发的 nginx 上去，取消掉对应的热点数据，从 nginx 本地缓存中删除 具体逻辑细节只能代码中见分晓了 总结 热点商品和热点数据概念不一致，热点是瞬时访问量达到一定阈值 热点问题是由于路由分发策略导致几种在一台服务器，服务器承受不住访问，导致崩溃 解决方案：实时感知热点，自动降级策略，征用其他 nginx 本地缓存，其他 nginx 为之服务 热点商品解决方案示意图： 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"078.html":{"url":"078.html","title":"078. 在 storm 拓扑中加入热点缓存实时自动识别和感知的代码逻辑","keywords":"","body":"078. 在 storm 拓扑中加入热点缓存实时自动识别和感知的代码逻辑 思路： 对商品访问次数 LRUMap 中的所有数据进行排序 计算后 95% 商品平均访值 对前 5% 的商品进行热点阈值评估，大于 n 倍的视为热点商品，存储在热点商品列表中 下面用代码来实现这个逻辑 /** * 热点商品感知 */ private static class HotProductFindThread extends Thread { private final Logger logger = LoggerFactory.getLogger(getClass()); private final LRUMap countMap; public HotProductFindThread(LRUMap countMap) { this.countMap = countMap; } @Override public void run() { List> countList = new ArrayList<>(); List hotPidList = new ArrayList<>(); while (true) { Utils.sleep(5000); if (countMap.size() entry : countMap.entrySet()) { countList.add(entry); } Collections.sort(countList, new Comparator>() { @Override public int compare(Map.Entry o1, Map.Entry o2) { // 取反结果，是降序排列 return ~Long.compare(o1.getValue(), o2.getValue()); } }); // 2.计算后 95% 商品平均访值 int avg95Count = (int) (countList.size() * 0.95); int avg95Total = 0; // 从列表尾部开始循环 avg95Count 次 for (int i = countList.size() - 1; i >= countList.size() - avg95Count; i--) { avg95Total += countList.get(i).getValue(); } // 后百分之 95 商品的平均访问值 int avg95Avg = avg95Total / avg95Count; int threshold = 5; // 阈值 // 3. 计算热点商品 for (int i = 0; i entry = countList.get(i); if (entry.getValue() > avg95Avg * threshold) { logger.info(\"热点商品：\" + entry); hotPidList.add(entry.getKey()); } } logger.info(\"热点商品列表：\" + hotPidList); } } public static void main(String[] args) { LRUMap countMap = new LRUMap<>(5); countMap.put(1L, 2L); countMap.put(2L, 1L); countMap.put(3L, 3L); countMap.put(4L, 30L); // 最后打印 4L 是热点商品，这个结果是对的 new HotProductFindThread(countMap).run(); } } 吸取上一章节的教训，这次写完一个小模块，就测试下逻辑是否正常。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"079.html":{"url":"079.html","title":"079. 在 storm 拓扑中加入 nginx 反向推送缓存热点与缓存数据的代码逻辑","keywords":"","body":" 079. 在 storm 拓扑中加入 nginx 反向推送缓存热点与缓存数据的代码逻辑 httpClient 工具封装 热点商品推送 nginx 代码逻辑 079. 在 storm 拓扑中加入 nginx 反向推送缓存热点与缓存数据的代码逻辑 httpClient 工具封装 compile 'org.apache.httpcomponents:httpclient:4.4' 这个工具应该都会的 package cn.mrcode.cachepdp.eshop.storm.http; import org.apache.http.HttpEntity; import org.apache.http.HttpResponse; import org.apache.http.NameValuePair; import org.apache.http.client.HttpClient; import org.apache.http.client.entity.UrlEncodedFormEntity; import org.apache.http.client.methods.HttpGet; import org.apache.http.client.methods.HttpPost; import org.apache.http.impl.client.DefaultHttpClient; import org.apache.http.message.BasicNameValuePair; import org.apache.http.util.EntityUtils; import java.io.BufferedReader; import java.io.InputStream; import java.io.InputStreamReader; import java.util.ArrayList; import java.util.Iterator; import java.util.List; import java.util.Map; /** * HttpClient工具类 * * @author lixuerui */ @SuppressWarnings(\"deprecation\") public class HttpClientUtils { /** * 发送GET请求 * * @param url 请求URL * @return 响应结果 */ @SuppressWarnings(\"resource\") public static String sendGetRequest(String url) { String httpResponse = null; HttpClient httpclient = null; InputStream is = null; BufferedReader br = null; try { // 发送GET请求 httpclient = new DefaultHttpClient(); HttpGet httpget = new HttpGet(url); HttpResponse response = httpclient.execute(httpget); // 处理响应 HttpEntity entity = response.getEntity(); if (entity != null) { is = entity.getContent(); br = new BufferedReader(new InputStreamReader(is)); StringBuffer buffer = new StringBuffer(); String line = null; while ((line = br.readLine()) != null) { buffer.append(line + \"\\n\"); } httpResponse = buffer.toString(); } } catch (Exception e) { e.printStackTrace(); } finally { try { if (br != null) { br.close(); } if (is != null) { is.close(); } } catch (Exception e2) { e2.printStackTrace(); } } return httpResponse; } /** * 发送post请求 * * @param url URL * @param map 参数Map */ @SuppressWarnings({\"rawtypes\", \"unchecked\", \"resource\"}) public static String sendPostRequest(String url, Map map) { HttpClient httpClient = null; HttpPost httpPost = null; String result = null; try { httpClient = new DefaultHttpClient(); httpPost = new HttpPost(url); //设置参数 List list = new ArrayList(); Iterator iterator = map.entrySet().iterator(); while (iterator.hasNext()) { Map.Entry elem = (Map.Entry) iterator.next(); list.add(new BasicNameValuePair(elem.getKey(), elem.getValue())); } if (list.size() > 0) { UrlEncodedFormEntity entity = new UrlEncodedFormEntity(list, \"utf-8\"); httpPost.setEntity(entity); } HttpResponse response = httpClient.execute(httpPost); if (response != null) { HttpEntity resEntity = response.getEntity(); if (resEntity != null) { result = EntityUtils.toString(resEntity, \"utf-8\"); } } } catch (Exception ex) { ex.printStackTrace(); } finally { } return result; } } 热点商品推送 nginx 代码逻辑 private void pushHotToNginx(Long pid) { // 降级策略推送到分发层 nginx String distributeNginxURL = \"http://eshop-cache03/hot?productId=\" + pid; HttpClientUtils.sendGetRequest(distributeNginxURL); // 获取商品信息 String cacheServiceURL = \"http://192.168.0.99:6002/getProductInfo?productId=\" + pid; String response = HttpClientUtils.sendGetRequest(cacheServiceURL); try { response = URLEncoder.encode(response, \"utf-8\"); } catch (UnsupportedEncodingException e) { e.printStackTrace(); } // 推送到应用层 nginx String[] appNginxURLs = new String[]{ \"http://eshop-cache01/hot?productId=\" + pid + \"&productInfo=\" + response, \"http://eshop-cache02/hot?productId=\" + pid + \"&productInfo=\" + response }; for (String appNginxURL : appNginxURLs) { HttpClientUtils.sendGetRequest(appNginxURL); } } public static void main(String[] args) throws UnsupportedEncodingException { // 获取商品信息 String cacheServiceURL = \"http://192.168.0.99:6002/getProductInfo?productId=\" + 1; String response = HttpClientUtils.sendGetRequest(cacheServiceURL); String url = \"http://192.168.0.99:6002/test?productId=\" + 1 + \"&productInfo=\" + URLEncoder.encode(response, \"UTF-8\"); HttpClientUtils.sendGetRequest(url); } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"080.html":{"url":"080.html","title":"080. 在流量分发+后端应用双层 nginx 中加入接收热点缓存数据的接口","keywords":"","body":" 080. 在流量分发+后端应用双层 nginx 中加入接收热点缓存数据的接口 分发层接收热点缓存逻辑 应用层接收热点缓存逻辑 080. 在流量分发+后端应用双层 nginx 中加入接收热点缓存数据的接口 分发层接收热点缓存逻辑 添加 hot 接口映射 vi /usr/hello/hello.conf server { listen 80; server_name _; location /lua { default_type 'text/html'; content_by_lua_file /usr/hello/lua/hello.lua; } location /product { default_type 'text/html'; content_by_lua_file /usr/hello/lua/distribute.lua; } # 建立接口映射 location /hot { default_type 'text/html'; content_by_lua_file /usr/hello/lua/hot.lua; } } 编写缓存逻辑 /usr/hello/lua/hot.lua local uri_args = ngx.req.get_uri_args() local product_id = uri_args[\"productId\"] local cache_ngx = ngx.shared.my_cache local hot_product_cache_key = \"hot_product_\"..product_id -- 存入缓存，时间可以设置长一点，1 小时 cache_ngx:set(hot_product_cache_key,\"true\",60 * 60) 应用层接收热点缓存逻辑 应用层都是一样的，也需要先建立接口映射，这里就不贴了 编写缓存逻辑 /usr/hello/lua/hot.lua local uri_args = ngx.req.get_uri_args() local product_id = uri_args[\"productId\"] local product_info = uri_args[\"productInfo\"] local product_cache_key = \"product_info_\"..product_id local cache_ngx = ngx.shared.my_cache cache_ngx:set(product_cache_key,product_info,60 * 60) ::: tip product_info 发送的时候，编码的，这里不知道不解码是否有问题 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"081.html":{"url":"081.html","title":"081. 在 nginx+lua 中实现热点缓存自动降级为负载均衡流量分发策略的逻辑","keywords":"","body":"081. 在 nginx+lua 中实现热点缓存自动降级为负载均衡流量分发策略的逻辑 之前思路已经有了：通过 storm 反推过来的 /usr/hello/lua/distribute.lua local uri_args = ngx.req.get_uri_args() -- 获取参数 local productId = uri_args[\"productId\"] local shopId = uri_args[\"shopId\"] -- 定义后端应用 ip local host = {\"192.168.99.170\", \"192.168.99.171\"} local hot_product_key = \"hot_product_\"..productId local cache_ngx = ngx.shared.my_cache local hot_product_flag = cache_ngx:get(hot_product_key) local backend = \"\" if hot_product_flag == \"true\" then -- 设置随机数种子 math.randomseed(tostring(os.time()):reverse():sub(1, 7)) local index = math.random(1, 2) backend = \"http://\"..host[index] else -- 对商品 id 取模并计算 hash 值 local hash = ngx.crc32_long(productId) hash = (hash % 2) + 1 -- 拼接 http 前缀 backend = \"http://\"..host[hash] end -- 获取到参数中的路径，比如你要访问 /hello，这个例子中是需要传递访问路径的 local method = uri_args[\"method\"] -- 拼接具体的访问地址不带 host，如：/hello?productId=1 local requestBody = \"/\"..method..\"?productId=\"..productId..\"&shopId=\"..shopId -- 获取 http 包 local http = require(\"resty.http\") local httpc = http.new() -- 访问，这里有疑问：万一有 cooke 这些脚本支持吗？会很麻烦吗？ local resp, err = httpc:request_uri(backend, { method = \"GET\", path = requestBody, keepalive=false }) -- 如果没有响应则输出一个 err 信息 if not resp then ngx.say(\"request error :\", err) return end -- 有响应测输出响应信息 ngx.say(resp.body) -- 关闭 http 客户端实例 httpc:close() 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"082.html":{"url":"082.html","title":"082. 在 storm 拓扑中加入热点缓存消失的实时自动识别和感知的代码逻辑","keywords":"","body":" 082. 在 storm 拓扑中加入热点缓存消失的实时自动识别和感知的代码逻辑 storm 中增加热点消失感知逻辑 nginx 增加取消热点逻辑 082. 在 storm 拓扑中加入热点缓存消失的实时自动识别和感知的代码逻辑 storm 中增加热点消失感知逻辑 在 cn.mrcode.cachepdp.eshop.storm.ProductCountBolt.HotProductFindThread#run 中补充该逻辑 // 4. 热点商品消失，通知 nginx 取消热点缓存 if (lastTimeHotPids.size() > 0) { // 上一次有热点商品 for (long lastTimeHotPid : lastTimeHotPids) { // 但是不在这一次的热点中了，说明热点消失了 if (!hotPidList.contains(lastTimeHotPid)) { // 发送到分发层 String url = \"http://eshop-03/cancel_hot?productId=\" + lastTimeHotPid; HttpClientUtils.sendGetRequest(url); } } } lastTimeHotPids.clear(); for (Long pid : hotPidList) { lastTimeHotPids.add(pid); } nginx 增加取消热点逻辑 在 /usr/hello/hello.conf 中增加接口映射 location /cancel_hot { default_type 'text/html'; content_by_lua_file /usr/hello/lua/cancel_hot.lua; } /usr/hello/lua/cancel_hot.lua 逻辑 local uri_args = ngx.req.get_uri_args() local product_id = uri_args[\"productId\"] local cache_ngx = ngx.shared.my_cache local hot_product_cache_key = \"hot_product_\"..product_id -- 设置标志，并且过期时间为 60 秒，过期之后条件也不成立 cache_ngx:set(hot_product_cache_key,\"false\",60) 在 /usr/hello/hello.conf 中声明下 nginx 的缓存变量 my_cache lua_shared_dict my_cache 128m; 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"083.html":{"url":"083.html","title":"083. 将热点缓存自动降级解决方案的代码运行后观察效果以及调试和修复 bug","keywords":"","body":" 083. 将热点缓存自动降级解决方案的代码运行后观察效果以及调试和修复 bug 热点感知测试 热点消失感知测试 优化 小结 083. 将热点缓存自动降级解决方案的代码运行后观察效果以及调试和修复 bug 热点感知测试 测试： nginx 需要 /usr/servers/nginx/sbin/nginx -s reload 在一些关键处增加日志打印，方便查看调试结果 后面都在本地运行 storm 了，由于 gradle 打包太麻烦了。 HotProductTopology 修改应用层 nginx 上的模板 html 因为需要观察是否被降级为随机路由了，在模板上写上自己本机的 hostanme 即可 调试日志如下： 商品 1,次数 1 Thread-35：[1=1, null, null] Thread-36：[null, null, null] 商品 2,次数 1 Thread-36：[2=1, null, null] Thread-35：[1=1, null, null] 商品 3,次数 1 商品 4,次数 1 51677 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 1 个，平均访问值为 1 51677 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[] 51688 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 1 个，平均访问值为 1 51688 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[] Thread-36：[2=1, 4=1, null] Thread-35：[1=1, 3=1, null] 商品 5,次数 1 56678 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 1 56678 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[] 56689 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 1 个，平均访问值为 1 56689 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[] 商品 6,次数 1 Thread-36：[2=1, 4=1, 6=1] Thread-35：[1=1, 3=1, 5=1] 商品 6,次数 2 商品 6,次数 3 商品 6,次数 4 商品 6,次数 5 商品 6,次数 6 商品 6,次数 7 商品 6,次数 8 商品 6,次数 9 61679 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 1 61679 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[] 61690 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 1 61690 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 发现一个热点商品：6=9 // 倍数设置的 5 。9 满足条件称为热点了 商品 6,次数 10 商品 6,次数 11 商品 6,次数 12 商品 6,次数 13 商品 6,次数 14 商品 6,次数 15 商品 6,次数 16 62209 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[6] 商品 6,次数 17 商品 6,次数 18 Thread-36：[6=18, 2=1, 4=1] Thread-35：[1=1, 3=1, 5=1] 66680 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 1 66681 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[] 商品 6,次数 19 商品 6,次数 20 商品 6,次数 21 商品 6,次数 22 商品 6,次数 23 67211 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 1 67211 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 发现一个热点商品：6=23 67301 [Thread-38] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[6] 通过日志观察到该商品，当成为热点的时候，触发了往 nginx 上推送标志， 进行策略降级（这个可以通过）不停的访问 http://eshop-cache03/product?method=product&productId=6&shopId=1 商品 id 为 6 的这个商品，当不是热点商品的时候，只会被路由到指定机器上，当成为热点之后，就会随机路由了 热点消失感知测试 怎么测试热点消失呢？看上面的日志有一条很重要的信息 Thread-36：[6=18, 2=1, 4=1] 66680 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 1 商品 id 为 6 能被计算为瞬时热点商品是因为，后两个商品平均访问次数为 1，大于 5 倍的阈值， 那么让 商品 id 为 6 的取消热点的方案就出来了：选择商品 id 为 2 的狂刷，把 6 的顶下来 测试日志如下 Thread-36：[6=13, 2=2, 4=1] 85287 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 1 85287 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 发现一个热点商品：6=13 85364 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[6] ... Thread-36：[2=25, 6=13, 4=1] 90364 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 后 95% 商品数量 2 个，平均访问值为 7 90364 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 热点商品列表：[] 90364 [Thread-37] INFO c.m.c.e.s.ProductCountBolt$HotProductFindThread - 一个热点商品消失了：6 这个时候再访问 id=6 的商品，发现一直被路由到 eshop-eache01 上了 优化 根据日志来看，当商品 id=6 被定为热点的时候，如果没有其他热点商品进来，那么按照现在休眠 5 秒的时间， 每 5 秒就会获取一次缓存并推送到 nginx 上。可以针对这一点进行优化 // 3. 计算热点商品 for (int i = 0; i entry = countList.get(i); if (entry.getValue() > avg95Avg * threshold) { logger.info(\"发现一个热点商品：\" + entry); hotPidList.add(entry.getKey()); if (!lastTimeHotPids.contains(entry.getKey())) { // 如果该商品已经是热点商品了，则不推送，新热点商品才推送 // 这里根据具体业务要求进行定制 // 推送热点商品信息到 所有 nginx 上 pushHotToNginx(entry.getKey()); } } } 小结 本小结是为了解决：热点商品在路由 hash 策略下，大流量打到同一台机器上扛不住 方案思路如下： 通过 storm 实时统计访问次数 热点商品感知：对每个 task 中的访问列表排序，前 5% 的商品与后 95% 商品平均访问值进行阀值比较，达到到具体设定倍数即认为是热点商品 热点商品消失感知：记录上一次的热点商品，当它跌出前 5% 时，被感知到，通过两次热点列表比较能得到 感知到热点商品时通知流量分发层 nginx 改变路由策略 分发到更多的 nginx 上去，同时 storm 需要反推该商品详情到更多的 nginx 上去， 本列是所有 nginx，随机分发策略 感知到热点消失时，通知流量分发层取消降级策略 之前推送到 nginx 上的缓存可以不用理睬，因为设置了缓存过期时间。 只需要再流量分发层上取消掉随机分发策略即可 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/084.html":{"url":"hystrix/084.html","title":"084. hystrix 与高可用系统架构：资源隔离+限流+熔断+降级+运维监控","keywords":"","body":" 084. hystrix 与高可用系统架构：资源隔离+限流+熔断+降级+运维监控 hystrix 是什么？ 高可用系统架构 如何讲解这块内容？ 084. hystrix 与高可用系统架构：资源隔离+限流+熔断+降级+运维监控 ::: tip Tip 高能预警：本 tip 是学习完 hystrix 章节后补充的，看每一小结的标题都会让你热血沸腾， 实际上在讲解 hystrix 的时候，我发现与 官网教程 雷同， 最重要的是说以商品缓存作为业务背景来解决一些问题，这个业务场景太少了， 在讲解 hystrix 功能时，只是把官网里面的例子换成了商品信息（不是绝对，只是用法）， 而没有一个比较合适的业务场景；比如页面在获取店铺信息时，遇到请求失败？在请求商品信息时获取失败？ 不知道是我没有注意到还是说根本没有这类似的背景讲解，只是在针对功能来用商品信息讲解用法 在框架的使用上，个人觉得应该是在场景上才能理解的，比如为什么手游「赤潮」中进入游戏时加载界面， 显示的头像偶尔是不正确的，这种是不是就是使用了降级机制呢？ ::: 前半部分，专注高并发这一块的缓存架构，怎么承载高并发？在各种高并发导致的令人崩溃的异常场景下，还能运行 接下来会花大部分章节讲解怎么保证高可用性：在各种系统的各个地方有乱七八糟的异常和故障的情况下，整套缓存系统还能继续健康的 run 着 网上会有一些将高可用的知识：HA、HAProxy 组件，主备服务间的切换，这就做到了高可用性， 主备实例，多冗余实例只是高可用最最基础的东西 接下来会讲解在什么样的情况下，可能会导致系统的崩溃？以及系统不可用，针对各种各样的一些情况， 然后我们用什么技术，去保护整个系统处于高可用的一个情况下 高可用有很多方式，本课程使用 hystrix hystrix 是什么？ netflix 美国流媒体巨头、世界最大的收费视频网站； 几年前整个网站经常出故障，可用性不太高，他们 api 团队为了提升高可用性，开发了一个框架 hystrix。 hystrix 提供了高可用相关的各种各样的功能，确保在 hystrix 的保护下，整个系统可以长期处于 高可用的状态，如 99.99%； 最理想的状态下，软件故障不应该导致整个系统的崩溃，服务器硬件故障可用通过服务的冗余来保证， 唯一有可能导致系统彻底崩溃，就是类似于机房停电，自然灾害等状况 不可用和产生的一些故障或者 bug 的区别： 不可用： 是完全不可用，整个系统完全崩溃 部分故障或 bug： 只是一小部分服务出问题 高可用系统架构 资源隔离、限流、熔断、降级、运维监控 而这些也是 hystrix 提供的功能 资源隔离：让某一刻东西在故障的情况下，不会耗尽系统所有资源，如线程资源 一个真实的遭遇，线上某块代码 bug，导致大量线程死循环，又创建大量线程， 最后系统资源被耗尽。崩溃 资源隔离的话，比如限制只能使用 10 个线程，那么这一块出问题，也不会影响整个系统 限流 高并发流量涌入，比如突然间一秒钟 100 完 QPS，系统完全承受不住，直接崩溃。 限流可以只对 10 万 QPS 进行服务，其他的都拒绝。这种情况下就是在你双 11 抢东西付款 的时候，老是告诉你系统繁忙的情况，但是偶尔又可以刷出来 熔断：连续故障，则在一段时间内直接拒绝服务 我自己最近遇到的就是 zuul 中的路由转发，当某一个服务连续转发失败（如那个服务根本没有启动）， 则在一分钟内直接返回异常信息，而不是继续转发，继续等待异常 降级： 如 mysql 挂了，系统发现了，自动降级，从内存里存的少量数据中，去提取一些数据出来。 疑问：这样的数据在什么场景下使用使用呢？ 运维监控： 监控 + 报警 + 优化，各种异常的情况，有问题就及时报警，然后对症下药 如何讲解这块内容？ 如何将 eshop-cache，核心的缓存服务改造成高可用的架构？ 由于合作方的一些小要求，打算吧 hystrix 中的一部分内容，单拉出来，做成一个免费的小课程，作为福利发放出去， 那么会重新写一个 eshop-cache-ha，和之前的业务场景衔接起来 eshop-cache，在各级缓存数据都失效的情况下，会重新从源系统中调用接口，依赖源系统去查询 mysql 数据库去重新获取数据， 如果你的各种依赖的服务有了故障，那么很可能会导致你的系统不可用 怎么使用 hystrix 对系统进行各种高可用性的系统加固，来应对各种不可用的情况； 所以会在 eshop-cache-ha 上去写代码，hystrix 做服务高可用这一块的内容， 还是基于商品详情页缓存架构这个业务背景，重新写代码，这样的话相对来说就比较独立，这章节就是高可用架构 简而言之，系统在 hystrix 的保护下，不会完全崩溃，就算所有依赖都失效了，那么也还能提供一些最最基础的简单服务； 上面的介绍很多地方现在以我本人的知识储备是完全想不到在什么场景下还能提供简单服务？是什么效果？这吸引力还是很大的 redis 挂掉，放在缓存雪崩那一章节讲解，雪崩，redis 必然挂，mysql 有较大概率挂掉，系统在风雨飘摇中 之前一个真实的项目，我们多个项目都用了公司里公用的缓存的存储，缓存彻底挂了、雪崩了， 导致各种业务系统全部崩溃，崩溃了好几个小时，导致公司损失了大量的资金的损失， 其中导致公司损失最大的负责人，受到了很大的处分 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/085.html":{"url":"hystrix/085.html","title":"085. hystrix 要解决的分布式系统可用性问题以及其设计原则","keywords":"","body":" 085. hystrix 要解决的分布式系统可用性问题以及其设计原则 hystrix 是什么？ Hystrix 的历史 初步看一看 Hystrix 的设计原则是什么？ Hystrix 要解决的问题是什么？ 再看 hystrix 的更加细节的设计原则是什么？ Hystrix 是如何实现它的目标的？ 《亿级流量电商详情页系统的大型高并发与高可用缓存架构实战》功能回顾 085. hystrix 要解决的分布式系统可用性问题以及其设计原则 我们的主题是：高可用性，会用几讲的时间来讲解一下如何用 hystrix 来构建高可用的服务的架构， 咱们会用一个真实的项目背景，作为业务场景，来带出来在这个特定的业务场景下， 可能会产生哪些各种各样的可用性的一些问题，针对这些问题，我们用 hystrix 的解决方案和原理是什么？ 带着大家，纯手工将所有的服务的高可用架构的代码，全部纯手工自己敲出来，形成高可用服务架构的项目实战的一个课程 hystrix 是什么？ 在分布式系统中，每个服务都可能会调用很多其他服务，被调用的那些服务就是依赖服务，有的时候某些依赖服务出现故障也是很正常的。 Hystrix 可以让我们在分布式系统中对服务间的调用进行控制，加入一些调用延迟或者依赖故障的容错机制。 Hystrix 通过将依赖服务进行资源隔离，进而阻止某个依赖服务出现故障的时候，这种故障在整个系统所有的依赖服务调用中进行蔓延， 同时 Hystrix 还提供故障时的 fallback 降级机制 总而言之，Hystrix 通过这些方法帮助我们提升分布式系统的可用性和稳定性 上面一段文字用下图示意 疑问：看完还是没有概念能打消想知道 b 服务不能使用时怎么能保证，a 服务功能完整性? Hystrix 的历史 hystrix 就是一种高可用保障的一个框架，类似于 spring（ioc、mvc）、mybatis、activiti、lucene 框架， 预先封装好的为了解决某个特定领域的特定问题的一套代码库，用了框架来解决这个领域的特定的问题， 就可以大大减少我们的工作量，提升我们的工作质量和工作效率 Netflix（网飞公司，美国流媒体巨头、世界最大的收费视频网站网飞），API 团队从 2011 年开始做一些提升系统可用性和稳定性的工作， Hystrix 就是从那时候开始发展出来的。在 2012 年的时候，Hystrix 就变得比较成熟和稳定了， Netflix 中，除了 API 团队以外，很多其他的团队都开始使用 Hystrix。 时至今日，Netflix 中每天都有数十亿次的服务间调用，通过 Hystrix 框架在进行， 而 Hystrix 也帮助 Netflix 网站提升了整体的可用性和稳定性 初步看一看 Hystrix 的设计原则是什么？ hystrix 为了实现高可用性的架构，设计 hystrix 的时候，一些设计原则是什么？ 对依赖服务调用时出现的调用延迟和调用失败进行控制和容错保护 在复杂的分布式系统中，阻止某一个依赖服务的故障在整个系统中蔓延 服务 A - 服务 B -> 服务 C，服务 C 故障了，服务 B 也故障了，服务 A 故障了，整套分布式系统全部故障，整体宕机 提供 fail-fast（快速失败）和快速恢复的支持 提供 fallback 优雅降级的支持 支持近实时的监控、报警以及运维操作 关键词总结： 调用延迟 + 失败，提供容错 阻止故障蔓延 快速失败 + 快速恢复 降级 监控 + 报警 + 运维 这里不是完全描述了 hystrix 的功能，简单来说是按照这些原则来设计 hystrix ，提供整个分布式系统的高可用的架构 Hystrix 要解决的问题是什么？ 在复杂的分布式系统架构中，每个服务都有很多的依赖服务，而每个依赖服务都可能会故障， 如果服务没有和自己的依赖服务进行隔离，那么可能某一个依赖服务的故障就会拖垮当前这个服务 举例来说：某个服务有 30 个依赖服务，每个依赖服务的可用性非常高，已经达到了 99.99% 的高可用性 那么该服务的可用性就是 99.99% - （100% - 99.99% * 30 = 0.3%）= 99.69%， 意味着 3% 的请求可能会失败，因为 3% 的时间内系统可能出现了故障不可用了 对于 1 亿次访问来说，3% 的请求失败也就意味着 300万 次请求会失败，也意味着每个月有 2个 小时的时间系统是不可用的， 在真实生产环境中，可能更加糟糕 上面的描述想表达的意思是：即使你每个依赖服务都是 99.99% 高可用性，但是一旦你有几十个依赖服务， 还是会导致你每个月都有几个小时是不可用的 下面画图分析说，当某一个依赖服务出现了调用延迟或者调用失败时，为什么会拖垮当前这个服务？ 以及在分布式系统中，故障是如何快速蔓延的？ 简而言之： 假设只有系统承受并发能力是 100 个线程， C 出问题的时候，耗时增加，将导致当前进入的 40 个线程得不到释放 后续大量的请求涌进来，也是先调用 c，然后又在这里了 最后 100 个线程都被卡在 c 了，资源耗尽，导致整个服务不能提供服务 那么其他依赖的服务也会出现上述问题，导致整个系统全盘崩溃 当时这个只能是在 高并发高流量的场景下会出现这种情况，其实我工作中也遇到过一次真实的案例， quartz 默认线程只有 25 个，当时定时任务接近 150 个左右，平时每个定时任务触发时间基本上上分散的， 而且基本上在 10 分钟左右会结束任务，当我们调用其他第三方服务时，没有加超时功能， 第三方服务可能出问题了，导致我们的请求被卡主，进而导致任务线程不能结束，最后整个任务调度系统完全崩溃， 完全不能提供服务。 这个场景在我所工作生涯中可能是记忆最深的一次了，因为当时在线上，根据日志打印完全看不出来问题， 就像系统假死一样，后来通过 jconsole 查看线程挂起情况，发现所有线程调用第三方服务后都被卡主了。 才顺藤摸瓜找到 quartz 的默认线程只有 25 个。最后加大了线程，也只是治标不治本，长时间运行还是会出问题 再看 hystrix 的更加细节的设计原则是什么？ 阻止任何一个依赖服务耗尽所有的资源，比如 tomcat 中的所有线程资源 避免请求排队和积压，采用限流和 fail fast 来控制故障 提供 fallback 降级机制来应对故障 使用资源隔离技术 隔离技术是为了实现第一条的功能 比如 bulkhead（舱壁隔离技术），swimlane（泳道技术），circuit breaker（短路技术）， 来限制任何一个依赖服务的故障的影响 通过近实时的统计/监控/报警功能，来提高故障发现的速度 通过近实时的属性和配置热修改功能，来提高故障处理和恢复的速度 保护依赖服务调用的所有故障情况，而不仅仅只是网络故障情况 调用这个依赖服务的时候，client 调用包有 bug、阻塞，等等 依赖服务的各种各样的 调用的故障，都可以处理 Hystrix 是如何实现它的目标的？ 通过 HystrixCommand 或者 HystrixObservableCommand 来封装对外部依赖的访问请求 d 这个访问请求一般会运行在独立的线程中，资源隔离 对于超出我们设定阈值的服务调用，直接进行超时，不允许其耗费过长时间阻塞住。 这个超时时间默认是 99.5% 的访问时间，但是一般我们可以自己设置一下 为每一个依赖服务维护一个独立的线程池，或者是 semaphore(信号量)，当线程池已满时，直接拒绝对这个服务的调用 对依赖服务的调用的成功次数、失败次数、拒绝次数、超时次数，进行统计 如果对一个依赖服务的调用失败次数超过了一定的阈值，自动进行熔断 在一定时间内对该服务的调用直接降级，一段时间后再自动尝试恢复 当一个服务调用出现失败、被拒绝、超时、短路（熔断）等异常情况时，自动调用 fallback 降级机制 对属性和配置的修改提供近实时的支持 疑问：上图只是站在全局角度来看的？并非自己所想，当一个依赖故障的时候，怎么搞也拿不到正确数据了？关注点关注错了？意思是说，及时这个一个小功能点不能用了，但是该系统其它的功能点能正常使用。并且不会因为这个故障导致整个系统崩溃？ 《亿级流量电商详情页系统的大型高并发与高可用缓存架构实战》功能回顾 由于该章节稍微独立，所以需要照顾下这个独立同学的上下文知识点，回顾下这个主题的功能点 亿级流量的电商网站的商品详情页系统架构 大型的缓存架构，支撑高并发与高可用 几十万 QPS 的高并发 + 99.99% 高可用 + 1T 以上的海量数据 + 绝对数据安全的 redis 集群架构 高并发场景下的数据库 + 缓存双写一致性保障方案 大缓存的维度化拆分方案 基于双层 nginx 部署架构的缓存命中率提升方案 基于 kafka + spring boot + ehcache + redis + nginx + lua 的多级缓存架构 基于 zookeeper 的缓存并发更新安全保障方案 基于 storm + zookeeper 的大规模缓存预热解决方案 基于 storm + zookeeper + nginx + lua 的热点缓存自动降级与恢复解决方案 基于 hystrix 的高可用缓存服务架构 hystrix 的进阶高可用架构方案、架构性能优化以及监控运维 基于 hystrix 的大规模缓存雪崩解决方案 高并发场景下的缓存穿透解决方案 高并发场景下的缓存失效解决方案 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/086.html":{"url":"hystrix/086.html","title":"086. 电商网站的商品详情页缓存服务业务背景以及框架结构说明","keywords":"","body":" 086. 电商网站的商品详情页缓存服务业务背景以及框架结构说明 电商网站的商品详情页系统架构 缓存服务 框架结构 086. 电商网站的商品详情页缓存服务业务背景以及框架结构说明 ::: tip 《亿级流量电商详情页系统的大型高并发与高可用缓存架构实战》 本笔记中会忽略这种独立章节的解释内容，本章内容是整套教程中的一部分。后续不再解释 ::: 我们这个课程，基于 hystrix，如何来构建高可用的分布式系统的架构，项目实战 大背景：电商网站、首页、商品详情页、搜索结果页、广告页、促销活动、购物车、订单系统、库存系统、物流系统；电商里面系统太多了 小背景：商品详情页，如何用最快的结果将商品数据填充到一个页面中，然后将页面显示出来 分布式系统：商品详情页中的缓存服务，+ 底层源数据服务，商品信息服务，店铺信息服务，广告信息服务，推荐信息服务，综合起来组成一个分布式的系统 我们主要是讲解商品详情页中的缓存架构。在该背景下进行讲解； ::: tip 该章节的背景介绍在前面已经讲解过，本人不会写笔记，直接使用原始笔记润色 ::: 电商网站的商品详情页系统架构 小型电商网站的商品详情页系统架构（不是我们要讲解的） 大型电商网站的商品详情页系统架构 页面模板 举个例子 将数据动态填充/渲染到一个 html 模板中，是什么意思呢？ #{name}的页面 商品的价格是：#{price} 商品的介绍：#{description} 上面这个就可以认为是一个页面模板，里面的很多内容是不确定的，#{name}，#{price}，#{description}，这都是一些模板脚本，不确定里面的值是什么？ 将数据填充/渲染到 html 模板中，是什么意思呢？ { \"name\": \"iphone7 plus（玫瑰金+32G）\", \"price\": 5599.50 \"description\": \"这个手机特别好用。。。。。。\" } iphone7 plus（玫瑰金+32G）的页面 商品的价格是：5599.50 商品的介绍：这个手机特别好用。。。。。。 上面这个就是一份填充好数据的一个html页面 缓存服务 缓存服务，订阅一个 MQ 的消息变更，如果有消息变更的话，那么就会发送一个网络请求，调用一个底层的对应的源数据服务的接口，去获取变更后的数据 将获取到的变更后的数据填充到分布式的 redis 缓存中去 高可用这一块儿，最可能出现说可用性不高的情况，是什么呢？就是说，在接收到消息之后，可能在调用各种底层依赖服务的接口时，会遇到各种不稳定的情况 比如底层服务的接口调用超时，预计 200ms 内返回，但是 2s 都没有返回; 底层服务的接口调用失败，比如说卡了 500ms 之后，返回一个报错 在分布式系统中，对于这种大量的底层依赖服务的调用，就可能会出现各种可用性的问题，一旦没有处理好的话，可能就会导致缓存服务自己本身会挂掉，或者故障掉，就会导致什么呢？不可以对外提供服务，严重情况下，甚至会导致说整个商品详情页显示不出来 hystrix 的主题是：缓存服务接收到变更消息后，去调用各个底层依赖服务时的高可用架构的实现 框架结构 围绕着缓存服务去拉取各种底层的源数据服务的数据，调用其接口时，可能出现的系统不可用的问题 在框架上一切从简：动手搭建 2 个 spring boot 服务，缓存服务和商品服务，缓存服务依赖于商品服务 模拟各种商品服务可能接口调用时出现的各种问题，导致系统不可用的场景，然后用 hystrix 完整的各种技术点全部贯穿在里面，解决了一大堆设计业务背景下的系统不可用问题，hystrix 整个技术体系，知识体系，也就讲解完了 消息队列、redis 咱们都不搞了，只关注这个 hystrix 的场景 简化的分布式系统的架构：spring boot + http client + hystrix 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/087.html":{"url":"hystrix/087.html","title":"087. 基于 spring boot 快速构建缓存服务以及商品服务","keywords":"","body":"087. 基于 spring boot 快速构建缓存服务以及商品服务 本章搭建项目： eshop-cache-ha：缓存服务，端口 7001 eshop-product-ha：商品服务，端口 7000 两个服务都是建立在父模块下的子模块，项目源码参考 https://github.com/zq99299/cache-pdp.git 以下配置重新搞过一次，之前的老是依赖有问题，时不时的就编译不了 最终项目目录布局 搭建一个空项目，能连接到数据库的配置，两个项目都是一样的除了端口号不一样 公共配置 build.gradle allprojects { group = 'cn.mrcode.cachepdp' version = '0.0.1-SNAPSHOT' repositories { mavenLocal() maven { url 'https://repo.spring.io/libs-snapshot' } maven { url \"http://maven.aliyun.com/nexus/content/groups/public\" } maven { url \"https://maven.repository.redhat.com/ga/\" } maven { url \"http://maven.nuiton.org/nexus/content/groups/releases/\" } maven { url \"https://repository.cloudera.com/artifactory/cloudera-repos/\" } mavenCentral() } } subprojects { p -> apply plugin: 'java' apply plugin: 'idea' sourceCompatibility = '1.8' //跳过所有文件的编译测试；不是跳过compileTestJava task 而是在执行该task的时候，跳过所有的测试文件 test { exclude '**/*.class' } // 这里一定得要。在多模块下，不然编译失败， // bootJar 默认会关闭 jar 任务 jar { enabled = true } //指定编译的编码 tasks.withType(JavaCompile) { options.encoding = \"UTF-8\" } } 公共配置 settings.gradle pluginManagement { repositories { gradlePluginPortal() } } rootProject.name = 'cache-pdp' // 这个写在之前的父项目下了，所以还有其他的子项目 include 'eshop-inventory' include 'eshop-cache' include 'storm-helloword' include 'eshop-storm' include 'eshop-cache-ha' include 'eshop-product-ha' eshop-product-ha/build.gradle plugins { id 'org.springframework.boot' version '2.1.5.RELEASE' } apply plugin: 'io.spring.dependency-management' dependencies { compile 'org.springframework.boot:spring-boot-starter-web' compile 'org.springframework.boot:spring-boot-starter-jdbc' compile 'org.springframework.boot:spring-boot-starter-actuator' compile 'org.springframework.boot:spring-boot-starter-thymeleaf' compile 'org.mybatis.spring.boot:mybatis-spring-boot-starter:2.0.1' runtimeOnly 'mysql:mysql-connector-java:5.1.34' testImplementation 'org.springframework.boot:spring-boot-starter-test' compile 'com.alibaba:fastjson:1.1.43' } eshop-product-ha application.yml server: port: 7000 logging: level: root: info # 可以打印 sql cn.mrcode.cachepdp.eshop.product.ha: info org.springframework.web: TRACE # path: ./ spring: datasource: driver-class-name: com.mysql.jdbc.Driver # driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://192.168.99.173:3306/eshop?useUnicode=yes&characterEncoding=UTF-8&useSSL=false username: eshop password: eshop jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 mybatis: # type-aliases-package: cn.mrcode.cachepdp.eshop.product.ha.model mapper-locations: classpath*:mapper/*.xml 项目启动后，可访问到 http://localhost:7000/getUserInfo 即为成功 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/088.html":{"url":"hystrix/088.html","title":"088. 快速完成缓存服务接收数据变更消息以及调用商品服务接口的代码编写","keywords":"","body":"088. 快速完成缓存服务接收数据变更消息以及调用商品服务接口的代码编写 ::: tip 在 hystrix 章节中，服务之间本应该使用 mq 来通知缓存是否有修改等， 为了更好的讲解 hystrix，一切从简，使用 httpClient 等方式来模拟 httpClient 依赖如下 compile 'org.apache.httpcomponents:httpclient:4.4' 具体工具类就不贴了 ::: 本章要做的事情：通知缓存服务有商品更新，缓存服务调用商品服务完成更新 缓存服务提供一个 /change/product?productId=1 的接口 调用该接口表示触发了商品更新信息，该接口获取调用上服务获取商品详情，完成缓存更新 商品服务提供一个 /getProduct?productId=1 的接口 获取一个商品详情信息 开始编码 cn.mrcode.cachepdp.eshop.cache.ha.controller.CacheController @RestController public class CacheController { private final Logger log = LoggerFactory.getLogger(getClass()); @RequestMapping(\"/change/product\") public String changeProduct(Long productId) { String url = \"http://localhost:7000/getProduct?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); log.info(response); return \"success\"; } } cn.mrcode.cachepdp.eshop.product.ha.controller.ProductController @RestController public class ProductController { private final Logger log = LoggerFactory.getLogger(getClass()); @RequestMapping(\"/getProduct\") public String getProduct(Long productId) { String productInfoJSON = \"{\\\"id\\\": \" + productId + \", \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; return productInfoJSON; } } 访问地址：http://localhost:7001/change/product?productId=1 输出如下日志，即测试成功 2019-06-01 22:40:05.936 INFO 8536 --- [nio-7001-exec-1] c.m.c.e.c.ha.controller.CacheController : {\"id\": 1, \"name\": \"iphone7手机\", \"price\": 5599, \"pictureList\":\"a.jpg,b.jpg\", \"specification\": \"iphone7的规格\", \"service\": \"iphone7的售后服务\", \"color\": \"红色,白色,黑色\", \"size\": \"5.5\", \"shopId\": 1,\"modifyTime\":\"2019-05-13 22:00:00\"} 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"hystrix/089.html":{"url":"hystrix/089.html","title":"089. 商品服务接口故障导致的高并发访问耗尽缓存服务资源的场景分析","keywords":"","body":"089. 商品服务接口故障导致的高并发访问耗尽缓存服务资源的场景分析 本章讲解一下最基本的商品服务接口调用故障，导致缓存服务资源耗尽的场景 这里总结下上图的信息： 我们的缓存架构大体上上面这样，缓存架构简介 nginx 本地缓存，段实际的过期，过期之后去请求 redis 缓存 redis 哨兵集群，高可用，大数据量，高并发 nginx 在 redis 获取不到的时候，就去缓存服务获取 缓存服务会在本地缓存中获取，如果获取不到则去商品服务获取，并返回 nginx，同时更新 redis 缓存信息（通过一些手段保证数据不会并发冲突覆盖） 商品信息有更新，则通过消息队列通知缓存服务更新 redis 相关缓存 缓存故障的产生 当所有缓存都失效的时候，大量获取商品详情的请求会到达商品服务， 商品服务会去数据库获取信息（这里不考虑数据库是否能支撑住）， 这时当获取商品服务接口比平时耗时更长时，大量的请求会被阻塞 缓存服务的线程资源也被阻塞，nginx 的线程资源也被阻塞，这个时候就会出现， 大量的商品详情页请求失败，一个服务还有其他的接口，比如店铺接口，当线程资源被耗尽的时候，其他服务也不能正常提供服务了 这样一来所有服务不能对外提供服务，大量流量进来，系统崩溃 下一章讲解怎么使用 hystrix 在具体的业务场景，去开发高可用的架构 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/090.html":{"url":"hystrix/090.html","title":"090. 基于 hystrix 的线程池隔离技术进行商品服务接口的资源隔离","keywords":"","body":" 090. 基于 hystrix 的线程池隔离技术进行商品服务接口的资源隔离 HystrixCommand 将商品服务接口调用的逻辑进行封装 HystrixObservableCommand 批量获取商品数据封装 HystrixObservableCommand 的调用方式 Action1 方式 Observer 方式 同步调用方式 资源隔离效果 090. 基于 hystrix 的线程池隔离技术进行商品服务接口的资源隔离 // 默认也是 1.5.12 的版本 compile 'com.netflix.hystrix:hystrix-core' 官网解说 hystrix 原理 我到官网 git hub pages 中溜达了一下，前面课程中讲解的基本上都是官网中讲解的，只是全身英文的， 我看起来很吃力，机翻有些看不太懂 这里介绍 hystrix 最基本的资源隔离技术：线程池隔离技术 提供了一个抽象 Command，把某一个依赖服务所有的调用请求，都走同一个线程池中的线程， 而不会用其他的线程资源，这就叫做资源隔离 Command ：每次服务调用请求，都是使用线程池内的一个线程去执行 command 的， comman 里面是你的业务逻辑。 假设该组服务线程池是 3 个线程，同时发起了 1000 个请求， 最多也只会有 3 个线程去执行请求，那么就算这个服务故障了，也不会将所有资源耗尽 HystrixCommand 将商品服务接口调用的逻辑进行封装 是一个获取单条数据的抽象 package cn.mrcode.cachepdp.eshop.cache.ha.hystrix.command; import com.alibaba.fastjson.JSON; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixCommandProperties; import com.netflix.hystrix.HystrixThreadPoolProperties; import java.util.concurrent.TimeUnit; import cn.mrcode.cachepdp.eshop.cache.ha.http.HttpClientUtils; import cn.mrcode.cachepdp.eshop.cache.ha.model.ProductInfo; /** * ${todo} * * @author : zhuqiang * @date : 2019/6/1 23:45 */ public class GetProductCommand extends HystrixCommand { private final Long productId; public GetProductCommand(Long productId) { // super(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")); // 线程组名 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) // 超时时间 .andCommandPropertiesDefaults(HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(6000)) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 线程池大小，最多有多少个线程同时并发 .withCoreSize(2) // 排队，默认为 -1 ，假设 10 个请求，2 个执行，2 个排队，那么其他 6 个将直接返回错误 .withMaxQueueSize(2) ) ); this.productId = productId; } @Override protected ProductInfo run() throws Exception { String url = \"http://localhost:7000/getProduct?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(\"睡眠 5 秒，模拟\"); TimeUnit.SECONDS.sleep(5); return JSON.parseObject(response, ProductInfo.class); } } controller 调用 @RequestMapping(\"/getProduct\") public ProductInfo getProduct(Long productId) { GetProductCommand getProductCommand = new GetProductCommand(productId); // 同步执行 ProductInfo productInfo = getProductCommand.execute(); return productInfo; } 测试访问：http://localhost:7001/getProduct?productId=1 一共点击 6 次，只有 4 条被执行了，有两条直接报错 睡眠 5 秒，模拟 睡眠 5 秒，模拟 com.netflix.hystrix.exception.HystrixRuntimeException: GetProductCommand could not be queued for execution and no fallback available. at com.netflix.hystrix.AbstractCommand$22.call(AbstractCommand.java:819) ~[hystrix-core-1.5.12.jar:1.5.12] 睡眠 5 秒，模拟 睡眠 5 秒，模拟 上面的日志顺序，后面有两条请求，是因为后面的是前面 4 条数据，其中有两条在排队，所以前面两条请求完成后才会执行后面两条。 报错的两条被拒绝了，说不能排队也没有可用的 fallback（后面会讲解这个概念） HystrixObservableCommand 批量获取商品数据封装 官网解说 hystrix 的使用 本章的使用方式都是官网教程中有的 HelloWord 例子 package cn.mrcode.cachepdp.eshop.cache.ha.hystrix.command; import com.alibaba.fastjson.JSON; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixObservableCommand; import cn.mrcode.cachepdp.eshop.cache.ha.http.HttpClientUtils; import cn.mrcode.cachepdp.eshop.cache.ha.model.ProductInfo; import rx.Observable; import rx.schedulers.Schedulers; /** * ${todo} * * @author : zhuqiang * @date : 2019/6/2 15:41 */ public class GetProductsCommand extends HystrixObservableCommand { private final Long[] pids; public GetProductsCommand(Long[] pids) { super(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")); this.pids = pids; } @Override protected Observable construct() { // create OnSubscribe 方法已经过时 // 文档说改为了 unsafeCreate 方法 return Observable.unsafeCreate((Observable.OnSubscribe) onSubscribe -> { // for (Long pid : pids) { // String url = \"http://localhost:7000/getProduct?productId=\" + pid; // String response = HttpClientUtils.sendGetRequest(url); // onSubscribe.onNext(JSON.parseObject(response, ProductInfo.class)); // } // onSubscribe.onCompleted(); try { if (!onSubscribe.isUnsubscribed()) { for (Long pid : pids) { String url = \"http://localhost:7000/getProduct?productId=\" + pid; String response = HttpClientUtils.sendGetRequest(url); onSubscribe.onNext(JSON.parseObject(response, ProductInfo.class)); } onSubscribe.onCompleted(); } } catch (Exception e) { onSubscribe.onError(e); } }).subscribeOn(Schedulers.io()); } } HystrixObservableCommand 的调用方式 Action1 方式 拉姆达表达式的方式调用，订阅获取每一条结果 /** * @param productIds 英文逗号分隔 */ @RequestMapping(\"/getProducts\") public void getProduct(String productIds) { List pids = Arrays.stream(productIds.split(\",\")).map(Long::parseLong).collect(Collectors.toList()); GetProductsCommand getProductsCommand = new GetProductsCommand(pids.toArray(new Long[pids.size()])); // 第一种获取数据模式 getProductsCommand.observe().subscribe(productInfo -> { System.out.println(productInfo); }); System.out.println(\"方法已执行完成\"); } 访问 http://localhost:7001/getProducts?productIds=1,2,3 日志 方法已执行完成 ProductInfo{id=1, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} ProductInfo{id=2, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} ProductInfo{id=3, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} Observer 方式 // 第二种获取数据模式 // 注意不要多次在同一个 command 上订阅 // 否则报错 GetProductsCommand command executed multiple times - this is not permitted. getProductsCommand.observe().subscribe(new Observer() { @Override public void onCompleted() { System.out.println(\"Observer: onCompleted\"); } @Override public void onError(Throwable e) { System.out.println(\"Observer: onError:\" + e); } @Override public void onNext(ProductInfo productInfo) { System.out.println(\"Observer: onNext:\" + productInfo); } }); 方法已执行完成 Observer: onNext:ProductInfo{id=1, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} Observer: onNext:ProductInfo{id=2, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} Observer: onNext:ProductInfo{id=3, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} Observer: onCompleted 两种方式有什么不同，其实看对象方法就知道了，如：当异常时，可以通过方法回调获取异常，而 Action1 方式则没有这样的功能 Observer: onError:com.netflix.hystrix.exception.HystrixRuntimeException: GetProductsCommand timed-out and no fallback available. java.net.ConnectException: Connection refused: connect at java.net.DualStackPlainSocketImpl.connect0(Native Method) 同步调用方式 // 同步调用方式 Iterator iterator = getProductsCommand.observe().toBlocking().getIterator(); while (iterator.hasNext()) { System.out.println(iterator.next()); } 从日志看出来，同步方式的确能达到效果 ProductInfo{id=1, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} ProductInfo{id=2, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} ProductInfo{id=3, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} 方法已执行完成 资源隔离效果 如图，这样一来调用都用线程去调用，的确能起到资源隔离的效果 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"hystrix/091.html":{"url":"hystrix/091.html","title":"091. 基于 hystrix 的信号量技术对地理位置获取逻辑进行资源隔离与限流","keywords":"","body":" 091. 基于 hystrix 的信号量技术对地理位置获取逻辑进行资源隔离与限流 什么是信号量（Semaphore）? 线程池与信号量隔离技术的区别？ 信号量在代码中的使用 091. 基于 hystrix 的信号量技术对地理位置获取逻辑进行资源隔离与限流 什么是信号量（Semaphore）? 信号量（Semaphore）也称为计数器，在 jdk 线程知识中也提供了 信号量 线程池与信号量隔离技术的区别？ 在 hystrix 中的一个核心就是资源隔离，提供了线程池和信号量的方式，那么他们有什么区别呢? ::: tip 官网中详细讲解了 线程池与信号量的区别于优缺点 只是是英文文档，看起来有点吃力 ::: 简单来说： 线程池： 使用独立线程池去执行业务逻辑，与当前请求线程（tomcat）不是同一个 线程阻塞可中断，所以有超时功能 可异步执行 信号量 计数器方式，只能是当前请求线程去执行业务逻辑 由于使用了当前请求线程，无法实现超时功能（实际测试可以实现，具体不知道是什么原因） 由于使用了当前请求线程，无法异步执行 官网中说到线程池的优点有好长的列表。那么线程池主要缺点是它们增加了计算开销。每个命令执行都涉及在单独的线程上运行命令所涉及的排队，调度和上下文切换。 Netflix 在设计这个系统时决定接受这个缺点，以换取它提供的好处，并认为它足够小，不会产生重大的成本或性能影响。 所以信号量方式只是单纯的你觉得客户端不会有故障的情况下，丢掉线程池开销这点性能消耗时使用。 下图示意了线程池与信号量在线程上的区别于原理示意图 信号量在代码中的使用 在了解了信号量与线程池的区别情况下，课程中讲解的例子，我觉得在场景描述上有歧义，就不记录了 大体上的思路是：商品信息中包含了发货地址信息，地址信息是缓存在本地 map 中的，使用信号量方式来限流获取地址信息。 官网中已经讲得很明白了。所以，对于信号量的使用，这里只是演示下 使用信号量策略很简单，在构造 command 时，更改隔离策略为 SEMAPHORE package cn.mrcode.cachepdp.eshop.cache.ha.hystrix.command; import com.alibaba.fastjson.JSON; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixCommandProperties; import java.util.concurrent.TimeUnit; import cn.mrcode.cachepdp.eshop.cache.ha.http.HttpClientUtils; import cn.mrcode.cachepdp.eshop.cache.ha.model.ProductInfo; /** * 基于信号量方式资源隔离 * * @author : zhuqiang * @date : 2019/6/2 17:45 */ public class GetCityCommand extends HystrixCommand { private final Long productId; public GetCityCommand(Long productId) { // super(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")); // 线程组名 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) // 超时时间 .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() // 设置 4 秒超时，看是否有效果 .withExecutionTimeoutInMilliseconds(6000) .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE) // 信号量最大请求数量设置 .withExecutionIsolationSemaphoreMaxConcurrentRequests(2) ) ); this.productId = productId; } @Override protected ProductInfo run() throws Exception { System.out.println(Thread.currentThread().getName()); String url = \"http://localhost:7000/getProduct?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); System.out.println(\"睡眠 5 秒，模拟\"); TimeUnit.SECONDS.sleep(5); return JSON.parseObject(response, ProductInfo.class); } } 调用处代码 @RequestMapping(\"/semaphore/getProduct\") public ProductInfo semaphoreGetProduct(Long productId) { GetCityCommand getCityCommand = new GetCityCommand(productId); System.out.println(Thread.currentThread().getName()); ProductInfo productInfo = getCityCommand.execute(); return productInfo; } 访问：http://localhost:7001/semaphore/getProduct?productId=1 测试结果： 对于限流日志报错如下 com.netflix.hystrix.exception.HystrixRuntimeException: GetCityCommand could not acquire a semaphore for execution and no fallback available. 这里测试超时也是有效果的，但是不知道是怎么实现的，看了下源码，里面 jdk 多线程的代码很多， 看不明白； 应该是没有使用自己的线程池了，看日志打印的线程名称是 tomcat 的线程 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"hystrix/092.html":{"url":"hystrix/092.html","title":"092. hystrix 的线程池+服务+接口划分以及资源池的容量大小控制","keywords":"","body":" 092. hystrix 的线程池+服务+接口划分以及资源池的容量大小控制 execution.isolation.strategy 隔离策略配置 command 名称和 command 组 command 线程池 三种 key 含义 coreSize queueSizeRejectionThreshold execution.isolation.semaphore.maxConcurrentRequests 092. hystrix 的线程池+服务+接口划分以及资源池的容量大小控制 资源隔离的两种策略：线程池和信号量 本章对资源隔离进一步讲解，进行细粒度的控制配置 execution.isolation.strategy 隔离策略配置 THREAD 线程池机制：每个 command 运行在一个线程中，限流是通过线程池的大小来控制的 SEMAPHORE 信号量机制：command 是运行在调用线程中，但是通过信号量的容量来进行限流 如何在线程池和信号量之间做选择？ 默认的策略就是线程池 线程池 最大的好处就是对于网络访问请求，如果有超时的话，可以避免调用线程阻塞住 信号量 通常是针对超大并发量的场景下，每个服务实例每秒都几百的 QPS，那么此时你用线程池的话，线程一般不会太多，可能撑不住那么高的并发，如果要撑住，可能要耗费大量的线程资源，那么就是用信号量，来进行限流保护（这种理解也能说得通） 一般用信号量常见于那种基于纯内存的一些业务逻辑服务，而不涉及到任何网络访问请求 netflix 有 100+ 的 command 运行在 40+ 的线程池中，只有少数 command 是不运行在线程池中的，就是从纯内存中获取一些元数据，或者是对多个 command 包装起来的 facacde command，是用信号量限流的 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) .andCommandPropertiesDefaults( HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.THREAD) ) ); command 名称和 command 组 主要用来更细粒度的控制依赖服务接口线程池如何来划分 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"GetProductCommand\")) ); command group 是一个非常重要的概念，默认情况下，是通过 command group 来定义一个线程池的，而且还会通过 command group 来聚合一些监控和报警信息，同一个 command group 中的请求，都会进入同一个线程池中 command 线程池 threadpool key 代表了一个 HystrixThreadPool，用来进行统一监控、统计、缓存，默认的 threadpool key 就是 command group 名称；每个 command 都会跟它的 threadpool key 对应的 thread pool 绑定在一起 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"GetProductCommand\")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"HelloWorldPool\")) ); 三种 key 含义 command key：代表了一类 command，一般来说代表了底层的依赖服务的一个接口 group key：代表了某一个底层的依赖服务 一个依赖服务可能会暴露出来多个接口，每个接口就是一个 command key 在逻辑上去组织起来一堆 command key 的调用、统计信息、成功次数、timeout 超时次数、失败次数、可以看到某一个服务整体的一些访问情况 一般来说，推荐是根据一个服务去划分出一个线程池，command key 默认都是属于同一个线程池的 比如说你以一个服务为粒度，估算出来这个服务每秒的所有接口加起来的整体 QPS 在 100 左右， 该服务部署了 10 个实例。每个实例该 group key 服务给 10 个左右线程就可以了， 整个集群服务的 QPS 就是 10 * 10 = 100 一般来说 group key 是用来在逻辑上组合一堆 command 的； 举个例子，对于一个服务中的某个功能模块来说，希望将这个功能模块内的所有 command 放在一个 group 中，那么在监控和报警的时候可以放一起看 group key 对应了一个服务，但是这个服务暴露出来的几个接口，访问量很不一样，差异非常之大， 你可能就希望在这个服务 group key 内部的多个 command key，做一些细粒度的资源隔离， 那么久需要用到 threadpool key 了 threadpool key threadpool key 是 command key 的上层；层级结构是这样的：group key -> threadpool key -> command key 逻辑上来说，多个 command key 属于一个 command group，在做统计的时候会放在一起统计 每个 command key 有自己的线程池，每个接口有自己的线程池，去做资源隔离和限流 但是对于 thread pool 资源隔离来说，可能是希望能够拆分的更加一致一些，比如在一个功能模块内，对不同的请求可以使用不同的 thread pool command group 一般来说，可以是对应一个服务，多个 command key 对应这个服务的多个接口，多个接口的调用共享同一个线程池 coreSize 这是线程池的大小，默认是 10 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) .andThreadPoolPropertiesDefaults( HystrixThreadPoolProperties.Setter() .withCoreSize(10) ) ); 一般来说，用这个默认的10个线程大小就够了 queueSizeRejectionThreshold 该配置涉及到一个工作原理，这里画图简单讲解下 控制 queue 满后 reject(拒绝) 的 threshold，因为 maxQueueSize 不允许热修改，因此提供这个参数可以热修改，控制队列的最大大小 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) .andThreadPoolPropertiesDefaults( HystrixThreadPoolProperties.Setter() .withCoreSize(10) // 不能热修改 // com.netflix.hystrix.HystrixThreadPoolProperties.maxQueueSize 源码中有注释说明 .withMaxQueueSize(20) // 可以热修改,默认值是 5 .withQueueSizeRejectionThreshold(10) ) ); execution.isolation.semaphore.maxConcurrentRequests 设置使用 SEMAPHORE 隔离策略的时候，允许访问的最大并发量，超过这个最大并发量，请求直接被 reject 这个并发量的设置，跟线程池大小的设置，应该是类似的，但是基于信号量的话，性能会好很多，而且 hystrix 框架本身的开销会小很多 默认值是 10，设置的小一些，否则因为信号量是基于调用线程去执行 command 的，而且不能从 timeout 中抽离，因此一旦设置的太大，而且有延时发生，可能瞬间导致 tomcat 本身的线程资源本占满 不能从 timeout 中抽离是什么意思（前面章节私自提前了解了一点配置，实测有超时效果）？ super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) // 超时时间 .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionTimeoutInMilliseconds(6000) .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE) // 信号量最大请求数量设置 .withExecutionIsolationSemaphoreMaxConcurrentRequests(2) ) ); 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/093.html":{"url":"hystrix/093.html","title":"093. 深入分析 hystrix 执行时的 8 大流程步骤以及内部原理","keywords":"","body":" 093. 深入分析 hystrix 执行时的 8 大流程步骤以及内部原理 1. 构建一个 HystrixCommand 或者 HystrixObservableCommand 2. 调用 command 的执行方法 3. 检查是否开启缓存 4. 检查是否开启了短路器 5. 检查线程池/队列/ semaphore 是否已经满了 6. 执行 command 7. 短路健康检查 8. 调用 fallback 降级机制 不同的执行方式 093. 深入分析 hystrix 执行时的 8 大流程步骤以及内部原理 之前几讲，我们用实际的业务背景给了一些可用性的问题; 然后借着那些最最基础的可用性的问题，然后讲解了 hystrix 最基本的支持高可用的技术：资源隔离 + 限流 创建 command，执行这个 command，配置这个 command 对应的 group 和线程池，以及线程池/信号量的容量和大小 我们要去讲解一下，你开始执行这个 command，调用了这个 command 的 execute() 方法以后， hystrix 内部的底层的执行流程和步骤以及原理是什么呢？ 在讲解这个流程的过程中，我们会带出来 hystrix 其他的一些核心以及重要的功能 画图分析整个 8 大步骤的流程，然后再对每个步骤进行细致的讲解； 由于是笔记，先贴上完整的图 视频讲解图 官方图 - 英文不好进行了翻译 汉化语句: available in cache？ 是否有缓存？ circuit breaker open？ 断路器是否打开？ Semaphore / Thread pool rejected? 信号量/线程池被拒绝? execution fails? 执行失败？ fallback successful? fallback 是否执行成? no;failed or not implemented 没有;失败或没有实现 report metrics 报告治标 calculate circuit health 断路器健康检查计算 下面来逐一讲解每个步骤的原理 1. 构建一个 HystrixCommand 或者 HystrixObservableCommand 一个 HystrixCommand 或一个 HystrixObservableCommand 对象，代表了对某个依赖服务发起的一次请求或者调用, 构造的时候，可以在构造函数中传入任何需要的配置参数 HystrixCommand：主要用于仅仅会返回一个结果的调用 HystrixObservableCommand：主要用于可能会返回多条结果的调用 2. 调用 command 的执行方法 执行 Command 就可以发起一次对依赖服务的调用， 要执行 Command，需要在 4 个方法中选择其中的一个：execute()、queue()、observe()、toObservable() 其中 execute() 和 queue() 仅仅对 HystrixCommand 适用 execute() 调用后直接 block 住，属于同步调用，直到依赖服务返回单条结果，或者抛出异常 queue() 返回一个 Future，属于异步调用，后面可以通过 Future 获取单条结果 observe() 订阅一个 Observable 对象，Observable 代表的是依赖服务返回的结果，获取到一个那个代表结果的 Observable 对象的拷贝对象 toObservable() 返回一个 Observable 对象，如果我们订阅这个对象，就会执行 command 并且获取返回结果 返回值 command K value = command.execute(); Future fValue = command.queue(); Observable ohValue = command.observe(); Observable ocValue = command.toObservable(); 注意，上面 4 种结果都依赖 toObservable()；这句话怎么理解？ 拿 execute 来举例，可以看到源码中的确是使用了 toObservable() 来调用的结果 com.netflix.hystrix.HystrixCommand#execute public R execute() { try { return queue().get(); } catch (Exception e) { throw Exceptions.sneakyThrow(decomposeException(e)); } } com.netflix.hystrix.HystrixCommand#queue public Future queue() { /* * The Future returned by Observable.toBlocking().toFuture() does not implement the * interruption of the execution thread when the \"mayInterrupt\" flag of Future.cancel(boolean) is set to true; * thus, to comply with the contract of Future, we must wrap around it. */ final Future delegate = toObservable().toBlocking().toFuture(); } 3. 检查是否开启缓存 从这一步开始，进入我们的底层的运行原理啦，了解 hysrix 的一些更加高级的功能和特性 如果这个 command 开启了请求缓存（request cache），而且这个调用的结果在缓存中存在，那么直接从缓存中返回结果 4. 检查是否开启了短路器 检查这个 command 对应的依赖服务是否开启了短路器，如果断路器被打开了，那么 hystrix 就不会执行这个 command， 而是直接去执行 fallback 降级机制 5. 检查线程池/队列/ semaphore 是否已经满了 如果 command 对应的线程池/队列/ semaphore 已经满了，那么也不会执行 command，而是直接去调用 fallback 降级机制 6. 执行 command 调用 HystrixObservableCommand.construct() 或 HystrixCommand.run() 来实际执行这个 command HystrixCommand.run() 是返回一个单条结果，或者抛出一个异常 HystrixObservableCommand.construct() 是返回一个 Observable 对象，可以获取多条结果 如果执行超过了 timeout 时长的话，那么 command 所在的线程就会抛出一个 TimeoutException， 如果 timeout 了，也会去执行 fallback 降级机制，而且就不会管 run() 或 construct() 返回的值了 这里要注意的一点是，我们是不可能终止掉一个调用严重延迟的依赖服务的线程的，只能说给你抛出来一个 TimeoutException， 但是还是可能会因为严重延迟的调用线程占满整个线程池的 对于上面一段话，本人知识储备不能很好的理解这一段话， hystrix 抛出了一个超时异常，但是对应的线程可能被卡住回不来? 这里的细节有点懵逼 如果没有 timeout 的话，那么就会拿到一些调用依赖服务获取到的结果，然后 hystrix 会做一些 logging 记录和 metric 统计 7. 短路健康检查 Hystrix 会将每一个依赖服务的调用成功、失败、拒绝、超时、等事件，都会发送给 circuit breaker 断路器， 短路器就会对调用成功/失败/拒绝/超时等事件的次数进行统计 短路器会根据这些统计次数来决定是否要进行短路，如果打开了短路器，那么在一段时间内就会直接短路， 然后如果在之后第一次检查发现调用成功了，就关闭断路器 8. 调用 fallback 降级机制 在以下几种情况中，hystrix 会调用 fallback 降级机制： run() 或 construct() 抛出一个异常 短路器打开 线程池/队列/ semaphore 满了 command 执行超时了 即使在降级中，一定要进行网络调用，也应该将那个调用放在一个 HystrixCommand 中，进行隔离 在 HystrixCommand 中，实现 getFallback() 方法，可以提供降级机制 在 HystirxObservableCommand 中，实现一个 resumeWithFallback() 方法，返回一个 Observable 对象，可以提供降级结果 如果 fallback 返回了结果，那么 hystrix 就会返回这个结果 对于 HystrixCommand，会返回一个 Observable 对象，其中会发返回对应的结果 对于 HystrixObservableCommand，会返回一个原始的 Observable 对象 如果没有实现 fallback，或者是 fallback 抛出了异常，Hystrix 会返回一个 Observable，但是不会返回任何数据 不同的 command 执行方式，其 fallback 为空或者异常时的返回结果不同 对于execute()：直接抛出异常 对于queue()：返回一个 Future，调用 get() 时抛出异常 对于observe()：返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError 方法 对于toObservable()：返回一个 Observable 对象，但是调用 subscribe() 方法订阅它时，立即抛出调用者的 onError 方法 不同的执行方式 execute()：获取一个 Future.get()，然后拿到单个结果 queue()：返回一个 Future observer()：立即订阅 Observable，然后启动 8 大执行步骤，返回一个拷贝的 Observable，订阅时立即回调给你结果 toObservable()：返回一个原始的 Observable，必须手动订阅才会去执行 8 大步骤 ::: tip 官网教程机翻之后，最后发现本笔记和官网教程的相似程度居然在 80% 以上 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/094.html":{"url":"hystrix/094.html","title":"094. 基于 request cache 请求缓存技术优化批量商品数据查询接口","keywords":"","body":" 094. 基于 request cache 请求缓存技术优化批量商品数据查询接口 官方教学 在业务背景下使用 hystrix 的 RequestCache 094. 基于 request cache 请求缓存技术优化批量商品数据查询接口 我们上一讲讲解的那个图片，顺着那个图片的流程，来一个一个的讲解 hystrix 的核心技术 创建 command，2 种 command 类型 执行 command，4 种执行方式 查找是否开启了 request cache，是否有请求缓存，如果有缓存，直接取用缓存，返回结果 官方教学 官网文档 Request Cache 您可以通过在 HystrixCommand 或 HystrixObservableCommand 对象上实现 getCacheKey() 方法来启用请求缓存，如下所示： import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; /** * * @author : zhuqiang * @date : 2019/6/3 21:51 */ public class CommandUsingRequestCache extends HystrixCommand { private final int value; protected CommandUsingRequestCache(int value) { super(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")); this.value = value; } @Override protected Boolean run() { // 当值为 0 或者是 2 的整倍数的时候，返回 true System.out.println(\"run 方法被执行\"); return value == 0 || value % 2 == 0; } @Override protected String getCacheKey() { return String.valueOf(value); } } 由于这取决于 请求上下文，我们必须初始化 HystrixRequestContext。在简单的单元测试中，您可以按如下方式执行此操作： @Test public void testWithoutCacheHits() { HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { assertTrue(new CommandUsingRequestCache(2).execute()); assertTrue(new CommandUsingRequestCache(2).execute()); assertFalse(new CommandUsingRequestCache(1).execute()); assertTrue(new CommandUsingRequestCache(2).execute()); assertTrue(new CommandUsingRequestCache(0).execute()); assertTrue(new CommandUsingRequestCache(58672).execute()); } finally { context.shutdown(); } } 测试结果只有 4 条 run 方法被执行 被打印，因为其中有 4 条是不相同的数字； 通常，此上下文将通过包装用户请求或其他生命周期挂钩的 ServletFilter 进行初始化和关闭。 以下示例显示 command 如何在请求上下文中从缓存中检索其值（以及如何查询对象以了解其值是否来自缓存）： @Test public void testWithCacheHits() { HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { CommandUsingRequestCache command2a = new CommandUsingRequestCache(2); CommandUsingRequestCache command2b = new CommandUsingRequestCache(2); assertTrue(command2a.execute()); // 第一次执行结果，所以不应该来自缓存 assertFalse(command2a.isResponseFromCache()); assertTrue(command2b.execute()); // 这是第二次执行结果，应该来自缓存 assertTrue(command2b.isResponseFromCache()); } finally { // 关闭上下文 context.shutdown(); } // 开始一个新的请求上下文 context = HystrixRequestContext.initializeContext(); try { CommandUsingRequestCache command3b = new CommandUsingRequestCache(2); assertTrue(command3b.execute()); // 当前的 command 是一个新的请求上下文 // 所以也不应该来自缓存 assertFalse(command3b.isResponseFromCache()); } finally { context.shutdown(); } } 日志也只会有两条日志，两个上下文中各一条 关于缓存的清除 CommandUsingRequestCache command2a = new CommandUsingRequestCache(2); assertTrue(command2a.execute()); // 第一次执行结果，所以不应该来自缓存 assertFalse(command2a.isResponseFromCache()); // commandKey 在声明 command 的时候我们可以自定义，所以很容易做成静态方法构建这个 key HystrixRequestCache.getInstance(command2a.getCommandKey(), HystrixConcurrencyStrategyDefault.getInstance()) .clear(command2a.getCacheKey()); 看代码相对来说比较麻烦，但是理解了一次请求上下文后，就应该明白，清除场景是很少见的； 看上下文和代码之间没有显式的进行关联，但是通过这句代码 HystrixRequestContext contextForCurrentThread = HystrixRequestContext.getContextForCurrentThread(); 能联想到是使用了 ThreadLocal 来保存上下文数据的。 这样一来在 ServletFilter 中初始化 HystrixRequestContext，就会让缓存生效，就清楚原理了 在业务背景下使用 hystrix 的 RequestCache 我们在批量获取商品接口中，使用单个获取的商品信息的 command 来测试缓存是否生效 public class GetProductCommand extends HystrixCommand { private Long productId; // 重写 getCacheKey 方法 @Override protected String getCacheKey() { return String.valueOf(productId); } // 其他代码不就粘贴了 } 在 filter 中初始化上下文 // 把 filter 按照 spring mvc 的方式注册 @Bean public FilterRegistrationBean filterRegistrationBean() { FilterRegistrationBean bean = new FilterRegistrationBean<>(); // 在 jdk8 中 Filter 接口 除了 javax.servlet.Filter.doFilter 方法外，其他两个方法都是默认方法了 // 所以这里使用了拉姆达表达式 bean.setFilter((request, response, chain) -> { HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { chain.doFilter(request, response); } finally { context.shutdown(); } }); bean.addUrlPatterns(\"/*\"); return bean; } 调用处修改下，以便测试缓存是否生效 /** * @param productIds 英文逗号分隔 */ @RequestMapping(\"/getProducts\") public void getProduct(String productIds) { List pids = Arrays.stream(productIds.split(\",\")).map(Long::parseLong).collect(Collectors.toList()); for (Long pid : pids) { GetProductCommand getProductCommand = new GetProductCommand(pid); getProductCommand.execute(); System.out.println(\"pid \" + pid + \"；是否来自缓存：\" + getProductCommand.isResponseFromCache()); } } 测试，访问 http://localhost:7001/getProducts?productIds=1,2,1,3 输出日志 pid 1；是否来自缓存：false pid 2；是否来自缓存：false pid 1；是否来自缓存：true pid 3；是否来自缓存：false 对于课程中的背景举例，个人感觉不太妥： 在一次请求上下文中，我们会去执行 N 多代码，调用 N 多依赖服务，有的依赖服务可能还会调用好几次，这个时候就可以使用请求缓存来提高性能； 但是目前个人实际开发中，对于在一个请求中药调用多次的，也会手动缓存起来，应该很少用到这种请求缓存把？ 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/095.html":{"url":"hystrix/095.html","title":"095. 开发品牌名称获取接口的基于本地缓存的 fallback 降级机制","keywords":"","body":" 095. 开发品牌名称获取接口的基于本地缓存的 fallback 降级机制 Hystrix fallback 的写法 fallback 使用场景 095. 开发品牌名称获取接口的基于本地缓存的 fallback 降级机制 创建 command 执行 command request cache 短路器如果打开了，执行 fallback 降级机制 按照流程来说，应该讲解断路器，但是为了讲解方便，把 fallback 提前了 因为只要是异常，都会调用 fallback ，如： 访问外部接口服务报错，访问 mysql、redis 等外部依赖报错 资源隔离被拒绝（reject） 访问外部依赖超时（timeout） 断路器被打开 断路器被打开不属于异常了，应该属于高可用的一个机制 如果短路器发现异常事件的占比达到了一定的比例，直接开启短路（circuit breaker） 那么 hystrix 中怎么开启降级机制的呢？ Hystrix fallback 的写法 public class CommandThatFailsFast extends HystrixCommand { private final boolean throwException; public CommandThatFailsFast(boolean throwException) { super(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")); this.throwException = throwException; } @Override protected String run() { if (throwException) { throw new RuntimeException(\"failure from CommandThatFailsFast\"); } else { return \"success\"; } } // 重写 getFallback 方法 @Override protected String getFallback() { return \"降级机制\"; } } 如果是 HystrixObservableCommand 则 @Override protected Observable resumeWithFallback() { if (throwException) { return Observable.error(new Throwable(\"failure from CommandThatFailsFast\")); } else { return Observable.just(\"success\"); } } 测试，如下代码，如果没有降级机制的话，这个调用不可能走到 System.out.println(execute) 这行代码 @Test public void testFailure() { String execute = new CommandThatFailsFast(true).execute(); System.out.println(execute); } fallback 使用场景 对于 fallback 的使用场景视频中也没有什么干货讲解，最后讲解了一个获取商品品牌信息的场景，如下 调用商品信息后，再去调用服务获取商品品牌信息 当品牌信息调用失败后，在 fallback 中获取一份在本地缓存保存的品牌数据 该品牌数据名称可能是落后的，也就是实际品牌名称为 ip7，本地缓存的名称为 ip fallback 最经典的两种降级机制：纯内存数据、默认值； 所以对于 hystrix 这种框架的时候，我觉得重点应该是使用场景，而不是官网教程中都已经很详细的教程 对于使用场景我唯一能想到的可能是游戏中的个人头像，个人喜欢玩游戏，但是有这么一个有趣的现象： 有时候个人信息的头像图片是不准确的，也就是不是自己设置的； 对于这个又去的现象，我唯一能想到的原因可能是： 获取头像信息失败，降级返回的一个本地内存中的一张图片或者返回的一个默认图片 由于是图片信息，占用网络带宽巨大，在繁忙的时候，对于这个服务进行降级，随机获取本地已有的图片返会；这里说的繁忙是指可能大量超时，导致获取失败，那么这样一来就省略了去服务中获取一次的网络开销了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/096.html":{"url":"hystrix/096.html","title":"096. 深入理解 hystrix 的短路器执行原理以及模拟接口异常时的短路实验","keywords":"","body":" 096. 深入理解 hystrix 的短路器执行原理以及模拟接口异常时的短路实验 断路器开关的条件与工作原理 测试断路器效果 断路器时间滑动窗口理解 断路器相关配置 096. 深入理解 hystrix 的短路器执行原理以及模拟接口异常时的短路实验 断路器开关的条件与工作原理 断路器的打开是多方度量的结果，受以下几方面影响 断路器上的流量达到某个阀值 HystrixCommandProperties.circuitBreakerRequestVolumeThreshold() 所有的调用都会经过断路器，它才能统计经过的流量 统计到异常占比达到某个阀值 HystrixCommandProperties.circuitBreakerErrorThresholdPercentage() 断路器从关闭（closed）状态到打开（open）状态 经过一段时间 HystrixCommandProperties.circuitBreakerSleepWindowInMilliseconds() 后 下一个请求如果通过（这个时候是半开状态（half-open）），断路器则关闭； 如果下一个请求失败，那么断路器将变成 open 状态，继续等待该配置时间后，再次尝试半开状态； 它的流程图大体是这样，10 秒是一个时间窗口 比如：10 秒内请求流量需要达到 10（默认值是 20 ） 个，并且异常占比 50%，也就是有 5 个请求 异常了，那么断路器就会开启。当开启 3 秒后，会允许一个请求通过，如果成功，则关闭断路器 测试断路器效果 把上面的比如描述，变为代码如下 public class CommandCircuit extends HystrixCommand { private final boolean throwException; public CommandCircuit(boolean throwException) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"test\")) .andCommandPropertiesDefaults( HystrixCommandProperties.Setter() // 10 秒时间窗口流量达到 10 个；默认是 20 .withCircuitBreakerRequestVolumeThreshold(10) // 当异常占比超过 50% ；默认值是 50 .withCircuitBreakerErrorThresholdPercentage(50) // 断路器打开之后，后续请求都会被拒绝并走降级机制，打开 3 秒后，变成半开状态 .withCircuitBreakerSleepWindowInMilliseconds(3000) ) ); this.throwException = throwException; } @Override protected String run() { if (throwException) { throw new RuntimeException(\"failure from CommandThatFailsFast\"); } else { return \"success\"; } } @Override protected String getFallback() { return \"降级机制\"; } } 测试 @Test public void test() throws InterruptedException { for (int i = 0; i 输出日志 22:57:29.155 [hystrix-test-3] 0 - 降级机制 1 - success 22:57:29.461 [hystrix-test-10] 318 - 降级机制 319 - success ... 320 - 降级机制 321 - 降级机制 999 - 降级机制 3 秒之后，断路器变成半开状态，一个请求通过 success 断路器关闭，尝试访问 success success success 对于测试代码来说，不太好控制，可能是没有理解那个 10 秒桶滑动时间窗口， 只能这种在短时间内产生大量请求，然后异常。 从日志中分析，在 22:57:29 秒钟产生了大量的请求，异常占比肯定达到了阀值，在当前的时间 10 秒 滑动时间窗口中满足了条件，断路器被打开。在休眠 3 秒后，由于这 3 秒没有产生任何请求，半开状态也通过，断路器就关闭了 断路器时间滑动窗口理解 上面没能理解时间滑动窗口，没能精准的测试出降级的效果，后来看懂了，再次尝试测试代码 @Test public void test() throws InterruptedException { for (int i = 0; i 输出日志为 23:15:43.292 [hystrix-test-1] 0 - 降级机制 1 - success 23:15:43.338 [hystrix-test-9] 8 - 降级机制 9 - success 流量 10 个，异常 50 % 达标：Tue Jun 04 23:15:43 CST 2019 尝试请求：Tue Jun 04 23:15:46 CST 2019 降级机制 降级机制 降级机制 3 秒之后，断路器变成半开状态，一个请求通过 success 断路器关闭，尝试访问 success success success 这次实验成功了，原因是什么呢？ 查看下图，就能大概明白这个 10 秒的滑动时间窗口是怎么回事了 断路器相关配置 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"test\")) .andCommandPropertiesDefaults( HystrixCommandProperties.Setter() // 手动控制断路器是否启用 .withCircuitBreakerEnabled(true) // 10 秒时间窗口流量达到 10 个；默认是 20 .withCircuitBreakerRequestVolumeThreshold(10) // 当异常占比超过 50% ；默认值是 50 .withCircuitBreakerErrorThresholdPercentage(20) // 断路器打开之后，后续请求都会被拒绝并走降级机制，打开 3 秒后，变成半开状态 .withCircuitBreakerSleepWindowInMilliseconds(3000) // 手动强制控制断路器是否打开 .withCircuitBreakerForceClosed(true) .withCircuitBreakerForceOpen(true) ) ); 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/097.html":{"url":"hystrix/097.html","title":"097. 深入理解线程池隔离技术的设计原则以及动手实战接口限流实验","keywords":"","body":" 097. 深入理解线程池隔离技术的设计原则以及动手实战接口限流实验 设计原则 线程池的好处 线程池的缺点 信号量 接口限流实验 097. 深入理解线程池隔离技术的设计原则以及动手实战接口限流实验 command 的创建和执行：资源隔离 request cache：请求缓存 fallback：优雅降级 circuit breaker：短路器，快速熔断（一旦后端服务故障，立刻熔断，阻止对其的访问） 把一个分布式系统中的某一个服务，打造成一个高可用的服务 资源隔离，优雅降级，熔断 判断，线程池或者信号量的容量是否已满，reject，有限流做用 限流，限制对后端的服务的访问量，比如说你对 mysql，redis，zookeeper，各种后端的中间件的资源，访问，其实为了避免过大的流浪打死后端的服务，线程池，信号量，限流 限制服务对后端的资源的访问 设计原则 官网教程 Hystrix 采取了 bulkhead 舱壁隔离技术，来将外部依赖进行资源隔离，进而避免任何外部依赖的故障导致本服务崩溃 线程池隔离，学术名称：bulkhead 舱壁隔离 外部依赖的调用在单独的线程中执行，这样就能跟调用线程隔离开来，避免外部依赖调用 timeout 耗时过长，导致调用线程被卡死 Hystrix 对每个外部依赖用一个单独的线程池，这样的话，如果对那个外部依赖调用延迟很严重， 最多就是耗尽那个依赖自己的线程池而已，不会影响其他的依赖调用 当然可以不使用线程池，但这需要客户端被信任非常快速地失败（网络连接/读取超时和重试配置）并始终表现良好。 Netflix 在其 Hystrix 设计中选择使用线程和线程池来实现隔离，原因有很多，其中包括： 每个服务可能都会调用数十个依赖服务，然而那些依赖服务通常是由很多不同的团队开发的 每个后端服务都提供自己的 client 库 比如说用 thrift 的话，就会提供对应的 thrift 依赖 client 调用库随时会变更 client 调用库随时可能会增加新的网络请求的逻辑 client 调用库可能会包含诸如自动重试，数据解析，内存中缓存等逻辑 client 调用库一般都对调用者来说是个黑盒，包括实现细节，网络访问，默认配置，等等 在真实的生产环境中经常会出现调用者突然间惊讶的发现 client 调用库发生了某些变化 即使 client 调用库没有改变，依赖服务本身可能有会发生逻辑上的变化 有些依赖的 client 调用库可能还会拉取其他的依赖库，而且可能那些依赖库配置的不正确 大多数网络请求都是同步调用的 调用失败和延迟，也有可能会发生在 client 调用库本身的代码中，不一定就是发生在网络请求中 简单来说，就是你必须默认 client 调用库就很不靠谱，而且随时可能各种变化，所以就要用强制隔离的方式来确保任何服务的故障不能影响当前服务； 我不知道在学习这个课程的学员里，有多少人真正参与过一些复杂的分布式系统的开发，在一些大公司里，做一些复杂的项目的话，如广告计费系统特别复杂， 可能涉及多个团队，总共三四十个人，五六十个人，一起去开发一个系统，每个团队负责一块儿； 每个团队里的每个人，负责一个服务，或者几个服务，比较常见的大公司的复杂分布式系统项目的分工合作的一个流程 线程池的好处 任何一个依赖服务都可以被隔离在自己的线程池内，即使自己的线程池资源填满了，也不会影响任何其他的服务调用 服务可以随时引入一个新的依赖服务，因为即使这个新的依赖服务有问题，也不会影响其他任何服务的调用 当一个故障的依赖服务重新变好的时候，可以通过清理掉线程池，瞬间恢复该服务的调用，而如果是 tomcat 线程池被占满，再恢复就很麻烦 如果一个 client 调用库配置有问题，线程池的健康状况随时会报告，比如成功/失败/拒绝/超时的次数统计，然后可以近实时热修改依赖服务的调用配置，而不用停机 如果一个服务本身发生了修改，需要重新调整配置，此时线程池的健康状况也可以随时发现，比如成功/失败/拒绝/超时的次数统计，然后可以近实时热修改依赖服务的调用配置，而不用停机 基于线程池的异步本质，可以在同步的调用之上，构建一层异步调用层 简单来说，最大的好处就是资源隔离，确保说，任何一个依赖服务故障，不会拖垮当前的这个服务 ::: tip 注意 尽管隔离是一个单独的线程提供的，但您的底层客户端代码也应该有超时和/或响应线程中断， 这样它就无法无限期地阻塞并使 Hystrix 线程池饱和。 ::: 怎么理解注意事项？我工作中就遇到过，调用网站的服务，调用的那个线程永远阻塞住， 我猜想是他内部出错了，一直阻塞没有返回，而我自己服务中的那个线程就被占用， 所以就算是使用 hystrix，你也应该在调用第三方服务的时候进行超时配置。 多线程这一块不是很熟悉，所以也是懵懵懂懂的 线程池的缺点 线程池的主要缺点是它们增加了计算开销。每个命令执行都涉及在单独的线程上运行命令所涉及的排队，调度和上下文切换。 Hystrix 官方自己做了一个多线程异步带来的额外开销，通过对比多线程异步调用+同步调用得出， Netflix API 每天通过 hystrix 执行 10亿 次调用，每个服务实例有 40 个以上的线程池， 每个线程池有 10 个左右的线程；最后发现说，用 hystrix 的额外开销，就是给请求带来了 3ms 左右的延时， 最多延时在 10ms 以内，相比于可用性和稳定性的提升，这是可以接受的 信号量 我们可以用 hystrix semaphore 技术来实现对某个依赖服务的并发访问量的限制，而不是通过线程池/队列的大小来限制流量 sempahore 技术可以用来限流和削峰，但是不能用来对调研延迟的服务进行 timeout 和隔离 execution.isolation.strategy，设置为 SEMAPHORE，那么 hystrix 就会用 semaphore 机制来替代线程池机制，来对依赖服务的访问进行限流 如果通过 semaphore 调用的时候，底层的网络调用延迟很严重，那么是无法 timeout 的，只能一直 block 住 一旦请求数量超过了 semephore 限定的数量之后，就会立即开启限流 接口限流实验 其实在前面我自己记录笔记的时候就尝试过这个 实验 的 package cn.mrcode.cachepdp.eshop.cache.ha; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixCommandProperties; import com.netflix.hystrix.HystrixThreadPoolProperties; import org.junit.Test; import java.util.Date; import java.util.concurrent.CountDownLatch; import java.util.concurrent.TimeUnit; /** * ${todo} * * @author : zhuqiang * @date : 2019/6/5 22:15 */ public class CommandLimit extends HystrixCommand { public CommandLimit() { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"test-group\")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 配置线程池大小，同时并发能力个数 .withCoreSize(2) // 配置等待线程个数；如果不配置该项，则没有等待，超过则拒绝 .withMaxQueueSize(5) // 由于 maxQueueSize 是初始化固定的，该配置项是动态调整最大等待数量的 // 可以热更新；规则：只能比 MaxQueueSize 小， .withQueueSizeRejectionThreshold(2) ) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionTimeoutInMilliseconds(2000)) // 修改为 2 秒超时 ); } @Override protected String run() throws Exception { TimeUnit.MILLISECONDS.sleep(800); return \"success\"; } @Override protected String getFallback() { return \"降级\"; } @Test public void test() throws InterruptedException { int count = 13; CountDownLatch downLatch = new CountDownLatch(count); for (int i = 0; i { CommandLimit commandLimit = new CommandLimit(); String execute = commandLimit.execute(); System.out.println(Thread.currentThread().getName() + \" \" + finalI + \" : \" + execute + \" : \" + new Date()); downLatch.countDown(); }).start(); } downLatch.await(); } } 输出日志为 Thread-0 0 : 降级 : Wed Jun 05 23:07:16 CST 2019 Thread-11 11 : 降级 : Wed Jun 05 23:07:16 CST 2019 Thread-1 1 : 降级 : Wed Jun 05 23:07:16 CST 2019 Thread-6 6 : 降级 : Wed Jun 05 23:07:16 CST 2019 Thread-12 12 : 降级 : Wed Jun 05 23:07:16 CST 2019 Thread-10 10 : 降级 : Wed Jun 05 23:07:16 CST 2019 Thread-3 3 : success : Wed Jun 05 23:07:17 CST 2019 Thread-5 5 : success : Wed Jun 05 23:07:17 CST 2019 Thread-4 4 : success : Wed Jun 05 23:07:18 CST 2019 Thread-7 7 : success : Wed Jun 05 23:07:18 CST 2019 Thread-2 2 : 降级 : Wed Jun 05 23:07:18 CST 2019 Thread-8 8 : 降级 : Wed Jun 05 23:07:18 CST 2019 Thread-9 9 : 降级 : Wed Jun 05 23:07:18 CST 2019 看到只有 4 个被执行成功了； 特别注意：withQueueSizeRejectionThreshold 是热更新 withMaxQueueSize 配置的； 在该测试中，休眠和超时很重要，因为： 休眠少了，那么执行速度过快，输出日志可能大于 withCoreSize + withQueueSizeRejectionThreshold 数量； 休眠多了，那么排队中被释放出来的时候发现已经超时就走降级机制了，而不是还去请求； 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/098.html":{"url":"hystrix/098.html","title":"098. 基于 timeout 机制来为商品服务接口的调用超时提供安全保护","keywords":"","body":"098. 基于 timeout 机制来为商品服务接口的调用超时提供安全保护 一般来说在调用依赖服务的接口的时候，比较常见的一个问题就是 超时 调用各种依赖服务，特别是在大公司，你所调用的服务你可能都不知道是谁写的，不知道这个人的技术水平， 特别是分布式系统中，多个团队大型协作，服务是谁的，你不了解，很有可能是一个实习生写的。 如果你不对各种依赖服务调用做超时控制，那么很有可能你的服务就被各种垃圾依赖服务的性能给拖死了。 笔者在之前也说过一个血的教训：一个定时任务的系统中，用 quartz，大概有 200 多个定时任务， 里面会调用第三方的依赖服务，运行一段时间之后，所有任务都不正常了，感觉假死一般。 最后找到问题就是默认的 quartz 线程数量是 25 个，这 25 个线程在运行一个月左右就都被卡死了， 通过 jconsole 找到有调用我们公司其他部门的服务，有第三方邮件发送接口的， 由于之前对超时不重视，该系统经过了一年才找到这个问题根源（之前都不知道用 jconsole） super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"test-group\")) .andCommandPropertiesDefaults(HystrixCommandProperties.Setter() .withExecutionTimeoutInMilliseconds(2000) // 超时配置 .withExecutionTimeoutEnabled(true) // 是否开启超时 ) // 修改为 2 秒超时 ); 一般超时配置就这两个，但是有一个需要注意的点：当超时之后，如果你有降级机制，则会调用降级方法返回结果 对于超时的实验前面已经做过很多了，这里就不再写测试用例了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/099.html":{"url":"hystrix/099.html","title":"099. 基于 hystrix 的高可用分布式系统架构项目实战课程的总结","keywords":"","body":"099. 基于 hystrix 的高可用分布式系统架构项目实战课程的总结 前面 hystrix 已经学到的核心知识如下： hystrix 内部工作原理：8 大执行步骤和流程 资源隔离：你如果有很多个依赖服务，那么想要高可用性，就先做资源隔离，任何一个依赖服务的故障不会导致你的服务的资源耗尽，不会崩溃 请求缓存：对于一个 request context 内的多个相同 command，使用 request cache，提升性能 熔断：基于短路器，采集各种异常事件、报错、超时、reject、短路，熔断后一定时间范围内就不允许访问了，直接降级，并提供自动恢复的机制 降级：报错、超时、reject、熔断后，就降级，服务提供容错的机制 限流：在你的服务里面，通过线程池或者信号量，限制对某个后端的服务或资源的访问量，避免从你的服务这里过去太多的流量，打死某个资源 超时：避免某个依赖服务性能过差，导致大量的线程阻塞去调用那个服务，会导致你的服务本身性能也比较差 以上知识点已经可以快速利用 hystrix 给自己开发的服务增加各种高可用的保障措施了， 避免你的系统因为各种各样的异常情况导致崩溃，不可用 后续要讲解的 hystrix 的高阶知识 request collapser，请求合并技术 fail-fast 和 fail-slient，高阶容错模式 static fallback 和 stubbed fallback，高阶降级模式 嵌套 command 实现的发送网络请求的降级模式 基于 facade command 的多级降级模式 request cache 的手动清理 生产环境中的线程池大小以及 timeout 配置优化经验 线程池的自动化动态扩容与缩容技术 hystrix 的 metric 高阶配置 基于 hystrix dashboard 的可视化分布式系统监控 生产环境中的 hystrix 工程运维经验 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/100.html":{"url":"hystrix/100.html","title":"100. 基于 request collapser 请求合并技术进一步优化批量查询","keywords":"","body":" 100. 基于 request collapser 请求合并技术进一步优化批量查询 请求合并原理 - 官网 为什么要使用 request collapser？ 请求合并的多种级别 请求合并的代价是多少？ 请求合并流程 Request Collapsing 请求合并例子 - 官网 在业务背景中实现请求合并效果 请求合并配置 小结 100. 基于 request collapser 请求合并技术进一步优化批量查询 hystrix 高级的技术：request collapser（请求合并技术，直译为 collapser 折叠） 请求合并原理 - 官网 官网 request collapser 直达 使用 HystrixCollapser 类型的 HystrixCommand 来实现请求合并， 可以实现将多个请求合并为对后端依赖服务的单个调用 怎么理解上面的话？先看下面的图，在线程数量和网络连接数量上的比较 为什么要使用 request collapser？ 上面的图片是否是看得不太懂？ 来简述下上图的含义： 当使用普通单个请求 command 时：1个 请求 = 1个 线程 = 1个 网络连接 比如：在 10 毫秒内，有 20 个并发请求，那么此时如果没有设置等待队列的话，应该只会有 10 个请求执行成功，剩下 10 个被拒绝了 当使用请求合并 command 时：在一个请求窗口内 = 1个 线程 = 1个 网络连接 比如：在 10 毫秒内，有 20 个并发请求，那么此时如果没有设置等待队列的话，那么可能只会占用 2 个请求线程 上面的解释之后是否比较有一点感觉了？至于你现在更多的问题，在后面示例代码之后，就会明白这个功能实现的原理了 使用请求合并来减少并发执行 HystrixCommand 执行所需的线程数和网络连接数。而且请求合并是自动的，不会强制你手动协调合并哪些请求。 请求合并的多种级别 global context tomcat 所有调用线程，对一个依赖服务的任何一个 command 调用都可以被合并在一起，hystrix 就传递一个 HystrixRequestContext 对于这一级别，没有更多的解释，官方推荐使用下面的那一个 user request context tomcat 内某一个调用线程，将某一个 tomcat 线程对某个依赖服务的多个 command 调用合并在一起 此种方式在前面 request cache 中已经演示过怎么配置请求上下文了 此种级别是 hystrix 默认级别 object modeling 基于对象的请求合并 如果有几百个对象，遍历后依次调用每个对象的某个方法，可能导致发起几百次网络请求，基于 hystrix 可以自动将对多个对象模型的调用合并到一起 此方式也不推荐使用，没有更多的描述 上面的一个能实现的重要点是：使用请求合并 command 的时候，是需要单独提供一个批量获取数据的接口。 hystrix 负责把某一个时间窗口内的 command 合并成一个 collapser ，然后由你去调用这个批量获取数据接口 请求合并的代价是多少？ 使用请求合并技术最大的代价就是：导致延迟大幅度增强，因为需要将一定时间内的多个请求合并 比如：发送 10 个请求，每个请求大概是 5 毫秒可以返回，要把 10 个请求合并在一个 command 内，统一执行，那么就有一个等待时间，假设是 10 毫秒，执行时间就可能变成 15 毫秒，延迟了 10 毫秒；通常情况下请求不会恰好在等待时间到达刚好 10 毫秒的时候才发出去请求，所以延迟还可以降一半，本例子为 5ms。 这个延迟开销是否是有意义的取决于正在执行的 command，并且与并发数量也有关系，因为有两个策略会触发合并请求发出去，一个是请求达到阀值，一个是等待时间达到阀值。请求合并并不适合在单线程中等待执行，这样会加大延迟。 比如 10 个请求单个数据总耗时将花费 50 毫秒，平均 5 毫秒；当并发 10 线程请求的时候，那么只需要 5 毫秒即可拿到数据； 同样，为了尽快达到合并发出请求阀值，在并发下会更快的达到阀值，批量请求数据可能需要耗时 10 毫秒。 总起看起来，不合并至需要 5 毫秒，合并之后可能需要 15 毫秒（10 毫秒请求接口，因为是批量数据的接口，假设会比单个耗时一点 + 5 毫秒的等待时间） 那么好处是什么呢？单个接口获取数据与批量接口获取数据去掉获取数据的时间因素（往往是业务聚合等因素），那么就是网络开销了； 如果特定命令同时大量使用并且可以将数十个甚至数百个调用一起批量处理，那么由于 Hystrix 减少了所需的线程数量和网络连接数量，因此实现的吞吐量通常远远超过了这个代价 所以总结：请求合并适合在高延迟 + 大量并发的情况下使用 每个请求就 2ms，batch 需要 8~10ms，延迟增加了 4~5 倍 每个请求本来就 30ms~50ms，batch 需要 35ms~55ms，延迟增加不太明显 请求合并流程 hystrix 会在 10ms（默认）内等待一批请求 到达 10ms 时会将此次时间内收集到的单个请求合并调用一个批处理接口 批处理响应返回后，需要为每个请求设置响应映射，不然怎么分得清楚哪个请求了什么？ 原理和流程，优缺点了解了，但是感觉还是有点懵逼，没有关系，看看例子 Request Collapsing 请求合并例子 - 官网 官网直达 package cn.mrcode.cachepdp.eshop.cache.ha; import com.netflix.hystrix.HystrixCollapser; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixCommandKey; import java.util.ArrayList; import java.util.Collection; import java.util.List; /** * @param 一个合并请求 HystrixCommand 执行返回的结果 * @param 单个请求返回的结果 * @param 单个 HystrixCommand 请求参数 * @author : zhuqiang * @date : 2019/6/11 21:04 */ public class CommandCollapserGetValueForKey extends HystrixCollapser, String, Integer> { private final Integer key; public CommandCollapserGetValueForKey(Integer key) { this.key = key; } /** * 单个 command 请求参数 */ @Override public Integer getRequestArgument() { return key; } /** * 聚合多个命令由框架完成，这里只需要创建我们的 batchCommand 即可 * * @param collapsedRequests 这个是多个请求的参数列表 */ @Override protected HystrixCommand> createCommand(Collection> collapsedRequests) { return new BatchCommand(collapsedRequests); } /** * 将返回的数据对请求进行映射，外部的单个请求才能获取到对应的结果 */ @Override protected void mapResponseToRequests(List batchResponse, Collection> collapsedRequests) { int count = 0; for (CollapsedRequest collapsedRequest : collapsedRequests) { // 把请求回来的结果再分发到对应的请求中去 collapsedRequest.setResponse(batchResponse.get(count++)); } } /** * 发起批量请求的 command */ private static final class BatchCommand extends HystrixCommand> { private final Collection> requests; public BatchCommand(Collection> requests) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"GetValueForKy\")) ); this.requests = requests; } @Override protected List run() throws Exception { List response = new ArrayList<>(); // 这里模拟拿到这一组的请求参数去请求接口，然后返回数据 for (CollapsedRequest request : requests) { response.add(\"ValueForKey：\" + request.getArgument()); } System.out.println(\"请求合并-BatchCommand 执行\"); return response; } } } 以上代码定义了三个泛型参数，现在一般还在懵逼状态中，下面来看看调用处是怎么使用的 @Test public void testCollapser() throws Exception { // 创建一个上下文 HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { // 这里不能使用多线程来并发请求，因为请求合并技术依赖于上下文 // 而这里初始化的是一个 HystrixRequestContext 上下文，也就是线程级别的 // 如果使用多线程，那么必然会报错 // 而且请求合并默认合并范围也是单个线程范围 // 这里在一个线程中请求多次 Future f1 = new CommandCollapserGetValueForKey(1).queue(); Future f2 = new CommandCollapserGetValueForKey(2).queue(); Future f3 = new CommandCollapserGetValueForKey(3).queue(); Future f4 = new CommandCollapserGetValueForKey(4).queue(); System.out.println(f1.get()); System.out.println(f2.get()); System.out.println(f3.get()); System.out.println(f4.get()); // 后续新增代码处 } finally { context.shutdown(); } } 在一个线程中请求多次，查看打印的日志 请求合并-BatchCommand 执行 ValueForKey：1 ValueForKey：2 ValueForKey：3 ValueForKey：4 可以看到虽然在外部调用了多次，但是多个请求被合并了，只发生了一次调用 还可以通过 log 工具获取到当前执行的请求 command ，在上面的测试代码「后续新增代码处」新增如下代码 HystrixRequestLog currentRequest = HystrixRequestLog.getCurrentRequest(); Collection> allExecutedCommands = currentRequest.getAllExecutedCommands(); System.out.println(\"当前线程中请求实际发起了几次：\" + allExecutedCommands.size()); HystrixCommand[] hystrixCommands = allExecutedCommands.toArray(new HystrixCommand[allExecutedCommands.size()]); HystrixCommand hystrixCommand = hystrixCommands[0]; System.out.println(\"其中第一个 command 的名称：\" + hystrixCommand.getCommandKey()); System.out.println(\"command 执行事件\" + hystrixCommand.getExecutionEvents()); 打印日志如下：可以看到当次执行实际被合并成了 2 个请求，其中一个执行事件也是 COLLAPSED 和执行成功 请求合并-BatchCommand 执行 ValueForKey：1 请求合并-BatchCommand 执行 ValueForKey：2 ValueForKey：3 ValueForKey：4 当前线程中请求实际发起了几次：2 其中第一个 command 的名称：GetValueForKy command 执行事件[SUCCESS, COLLAPSED] 走到这里，相信你已经明白了请求合并是怎么写的，大体上原理是什么了 下面来在我们的业务背景中实现请求合并 在业务背景中实现请求合并效果 在 eshop-product-ha 项目中先增加一个提供批量获取商品详情的接口 cn.mrcode.cachepdp.eshop.product.ha.controller.ProductController /** * 批量返回商品信息的接口 * * @param productIdsStr 商品 id 用英文逗号分隔 */ @RequestMapping(\"/getProducts\") public List getProduct(String productIdsStr) { String[] productIds = productIdsStr.split(\",\"); return Arrays.stream(productIds) .map(productId -> { String productInfoJSON = \"{\\\"id\\\": \" + productId + \", \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; return productInfoJSON; }) .collect(Collectors.toList()); } 在 eshop-cache-ha 项目中编写请求合并 command 相关代码 package cn.mrcode.cachepdp.eshop.cache.ha.hystrix.command; import com.alibaba.fastjson.JSON; import com.netflix.hystrix.HystrixCollapser; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixCommandKey; import java.util.Collection; import java.util.List; import java.util.stream.Collectors; import cn.mrcode.cachepdp.eshop.cache.ha.http.HttpClientUtils; import cn.mrcode.cachepdp.eshop.cache.ha.model.ProductInfo; /** * 请求合并 command * @author : zhuqiang * @date : 2019/6/11 21:55 */ public class CollapserGetProductCommand extends HystrixCollapser, ProductInfo, Long> { private final Long productId; public CollapserGetProductCommand(Long productId) { this.productId = productId; } @Override public Long getRequestArgument() { return productId; } @Override protected HystrixCommand> createCommand(Collection> collapsedRequests) { // 实现注意:要快。（ batchResponse, Collection> collapsedRequests) { int count = 0; for (CollapsedRequest collapsedRequest : collapsedRequests) { collapsedRequest.setResponse(batchResponse.get(count++)); } System.out.println(\"映射数量：\" + collapsedRequests.size()); } public static class BatchCommand extends HystrixCommand> { private final Collection> requests; public BatchCommand(Collection> requests) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"GetValueForKy\")) ); this.requests = requests; System.out.println(\"此次请求大小：\" + requests.size()); } @Override protected List run() throws Exception { // 从当前合并的多个请求中，按顺序拼接请求的 pid String productIdsStr = requests.stream() .map(item -> item.getArgument()) .map(item -> String.valueOf(item)) .collect(Collectors.joining(\",\")); System.out.println(\"执行批量接口请求:\" + productIdsStr); String url = \"http://localhost:7000/getProducts?productIdsStr=\" + productIdsStr; String response = HttpClientUtils.sendGetRequest(url); return JSON.parseArray(response, String.class) .stream() .map(item -> JSON.parseObject(item, ProductInfo.class)) .collect(Collectors.toList()); } } } 在 cn.mrcode.cachepdp.eshop.cache.ha.controller.CacheController 中批量触发获取商品接口中调用请求合并的 command /** * @param productIds 英文逗号分隔 */ @RequestMapping(\"/getProducts\") public void getProduct(String productIds) throws ExecutionException, InterruptedException { List pids = Arrays.stream(productIds.split(\",\")).map(Long::parseLong).collect(Collectors.toList()); // 在批量获取商品接口中来使用请求合并 // for (Long pid : pids) { // CollapserGetProductCommand getProductCommand = new CollapserGetProductCommand(pid); // Future queue = getProductCommand.queue(); // System.out.println(\"请求结果：\" + queue.get() + \": 是否只请求合并：\" + queue.isCancelled()); // } // 不要使用上面的调用方式，因为这样做就相当于是同步调用了，一个请求回来之后才能继续下一个 List> results = pids.stream() .map(pid -> { CollapserGetProductCommand getProductCommand = new CollapserGetProductCommand(pid); return getProductCommand.queue(); }) .collect(Collectors.toList()); for (Future result : results) { System.out.println(\"请求结果：\" + result.get()); } } 测试，访问地址：http://localhost:7001/getProducts?productIds=1,2,1,3 日志输出 此次请求大小：3 执行批量接口请求:1,2,3 请求结果：ProductInfo{id=1, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} 映射数量：3 请求结果：ProductInfo{id=2, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} 请求结果：ProductInfo{id=1, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} 请求结果：ProductInfo{id=3, name='iphone7手机', price=5599.0, pictureList='a.jpg,b.jpg', specification='iphone7的规格', service='iphone7的售后服务', color='红色,白色,黑色', size='5.5', shopId=1, modifyTime=Mon May 13 22:00:00 CST 2019} 注意看上面的日志信息：在真正执行请求的时候本来传递了 4 个商品 id，其中 id=1 的有两个， 但是合并之后只有 3 个请求了， 但是神奇的是，在调用处打印数据的时候，却是有 4 个结果； 也就是说：请求合并自带类似请求缓存功能，保证在一次上下文中相同参数的 command 只会被请求一次 ::: tip 注意 在测试的时候，第一次访问很有可能报错，报超时异常，这个很正常，由于第一次某些类需要初始化， 耗时较长，就会触发请求超时，这个之前讲过的，可以设置长一点的超时时间 ::: 请求合并配置 public CollapserGetProductCommand(Long productId) { super(Setter.withCollapserKey(HystrixCollapserKey.Factory.asKey(\"CollapserGetProductCommand\")) .andCollapserPropertiesDefaults(HystrixCollapserProperties.Setter() // 在 TimerDelayInMilliseconds 内最多允许多少个 request 被合并 // 默认是无限大，该参数一般不使用，而是使用时间来触发合并请求提交 .withMaxRequestsInBatch(10) // 时间窗口：合并请求需要等待多久 // 默认是 10ms ， .withTimerDelayInMilliseconds(20) ) ); this.productId = productId; } 小结 request collapser 能做到在一个上下文中使用异步方式大量请求相同接口时进行请求合并； 它的本质是：提供了一个范围内的拦截器和调度器，等待请求到达，然后把请求参数给我们自己的业务逻辑去进行批量请求 有一个疑问，还是使用场景相关的： 在上面的演示中，一个 tomcat 请求（也就是一个线程），在什么情况下才会使用这种大量异步单个商品请求的情况？暂时是在想象不到；很简单的一个就是全局上下文中，这种情况下多个用户并发请求过来，很容易就被合并了，目前能想到的是这个，但是官方不推荐此方式； 上下文除了全局的，都是基于当前线程，所以在主线程中初始化了上下文，也不能分多线程去执行 还是这个使用场景问题 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"hystrix/101.html":{"url":"hystrix/101.html","title":"101. hystirx 的 fail-fast 与 fail-silient 两种最基础的容错模式","keywords":"","body":" 101. hystirx 的 fail-fast 与 fail-silient 两种最基础的容错模式 fail-fast fail-silent 101. hystirx 的 fail-fast 与 fail-silient 两种最基础的容错模式 下面几节是 HystrixCommand 和 HystrixObservableCommand 的常见用法和使用模式。 fail-fast 没有 fallback 降级逻辑，报错的话，异常可以被捕获到 public class CommandThatFailsFast extends HystrixCommand { private final boolean throwException; public CommandThatFailsFast(boolean throwException) { super(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")); this.throwException = throwException; } @Override protected String run() { if (throwException) { throw new RuntimeException(\"failure from CommandThatFailsFast\"); } else { return \"success\"; } } } @Test public void testFailure2() { try { new CommandThatFailsFast(true).execute(); } catch (HystrixRuntimeException e) { // 异常可以被捕获到 System.out.println(\"xxx\"); assertEquals(\"failure from CommandThatFailsFast\", e.getCause().getMessage()); e.printStackTrace(); // 如果提供了降级机制，那么这里就不会被捕获到异常 } } fail-silent 有 fallback 降级逻辑；如果执行报错了，会走 fallback 降级，返回 fallback 的值给你 HystrixCommand @Override protected String getFallback() { return \"降级机制\"; } HystrixObservableCommand @Override protected Observable resumeWithFallback() { return Observable.empty(); } 很少会用 fail-fast 模式，比较常用的可能还是 fail-silent，特别常用，既然都到了 fallback 里面，肯定要做点降级的事情 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/102.html":{"url":"hystrix/102.html","title":"102. 为商品服务接口调用增加 stubbed fallback 降级机制","keywords":"","body":" 102. 为商品服务接口调用增加 stubbed fallback 降级机制 返回单个值的 HystrixCommand stubbed 使用方式 批量获取 HystrixObservableCommand 的 stubbed 使用方式 简述视频示例 102. 为商品服务接口调用增加 stubbed fallback 降级机制 ::: tip 注意 在这章节中，可能已经注意到了，和视频中内容可能有不一致的地方， 是因为我发现视频中其实也是跟着官网教程走的，这个还不是最重要的。 最重要的问题是：视频中根本就没有讲解在商品缓存这个背景下的一个比较合理的场景来演示， 只是简单的把官网的例子用商品信息来表述了； 使用框架最重要的是场景，可惜在 hystrix 这章节中失望了，场景太少太少了 所以在 hystrix 中，本人觉得没有什么价值的演示不会做，跟着翻译官网的教程视乎更有意义， 当然还是会与视频内容主题对应 ::: 官网直达 stubbed：译为残缺的 什么意思？首先要明白，stubbed fallback 不是一种新的功能，只是降级机制的一种使用/应用方式， 如 java 中的注解功能，spring 就用来做各种功能。这只是一种使用方式 当你返回包含多个字段的复合对象时，通常使用 stubbed fallback，其中一些字段可以从其他请求状态确定， 而其他字段则设置为默认值。这些值可以再以下地方获取到: cookies 请求参数和请求头 在当前服务请求失败之前，来自以前服务请求的响应；也就是本地的一些缓存 比如下面这个降级机制： 返回单个值的 HystrixCommand stubbed 使用方式 public class CommandWithStubbedFallback extends HystrixCommand { private final int customerId; private final String countryCodeFromGeoLookup; /** * @param customerId * The customerID to retrieve UserAccount for * @param countryCodeFromGeoLookup * The default country code from the HTTP request geo code lookup used for fallback. */ protected CommandWithStubbedFallback(int customerId, String countryCodeFromGeoLookup) { super(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")); this.customerId = customerId; this.countryCodeFromGeoLookup = countryCodeFromGeoLookup; } @Override protected UserAccount run() { // fetch UserAccount from remote service // return UserAccountClient.getAccount(customerId); throw new RuntimeException(\"forcing failure for example\"); } @Override protected UserAccount getFallback() { // 返回一些带过来的请求参数 // 和一些默认值 // 注意：不要在这里去远程请求，否则有可能出现这里请求又失败的问题 return new UserAccount(customerId, \"Unknown Name\", countryCodeFromGeoLookup, true, true, false); } public static class UserAccount { private final int customerId; private final String name; private final String countryCode; private final boolean isFeatureXPermitted; private final boolean isFeatureYPermitted; private final boolean isFeatureZPermitted; UserAccount(int customerId, String name, String countryCode, boolean isFeatureXPermitted, boolean isFeatureYPermitted, boolean isFeatureZPermitted) { this.customerId = customerId; this.name = name; this.countryCode = countryCode; this.isFeatureXPermitted = isFeatureXPermitted; this.isFeatureYPermitted = isFeatureYPermitted; this.isFeatureZPermitted = isFeatureZPermitted; } } } HystrixObservableCommand 则可以使用下面的方式 @Override protected Observable resumeWithFallback() { return Observable.just( new UserAccount(customerId, \"Unknown Name\", countryCodeFromGeoLookup, true, true, false) ); } 批量获取 HystrixObservableCommand 的 stubbed 使用方式 如果在批量获取中，你可能想知道在失败之前获取到了哪条数据？并在失败降级处理时接着这条数据处理 // 请忽略类名，为了快速随意复制的一个类修改的 public class CommandThatFailsFast2 extends HystrixObservableCommand { private int lastSeen = 0; public CommandThatFailsFast2() { super(HystrixCommandGroupKey.Factory.asKey(\"ExampleGroup\")); } @Override protected Observable construct() { // 这句代码的意思是：产生了 1,2,3 个数值（模拟请求），在第 4 个请求时抛出了一个异常 return Observable.just(1, 2, 3) .concatWith(Observable.error(new RuntimeException(\"forced error\"))) .doOnNext(t1 -> lastSeen = t1) .subscribeOn(Schedulers.computation()); } @Override protected Observable resumeWithFallback() { // 走到降级机制里面，会判定这个进度，接着进度返回 if (lastSeen 测试代码 @Test public void fun4() { CommandThatFailsFast2 commandThatFailsFast2 = new CommandThatFailsFast2(); Iterator iterator = commandThatFailsFast2.observe().toBlocking().getIterator(); while (iterator.hasNext()) { System.out.println(\"响应结果：\" + iterator.next()); } } 输出日志如下，可以看出来 HystrixObservableCommand 是一条一条消费的，已经消费过的不影响，没有消费过的可以通过降级机制进行续上 响应结果：1 响应结果：2 响应结果：3 22:42:30.964 [RxComputationScheduler-2] DEBUG com.netflix.hystrix.AbstractCommand - Error executing HystrixCommand.run(). Proceeding to fallback logic ... java.lang.RuntimeException: forced error at cn.mrcode.cachepdp.eshop.cache.ha.CommandThatFailsFast2.construct(CommandThatFailsFast2.java:25) ... 响应结果：4 简述视频示例 视频中修改了之前获取单个商品详情信息的 GetProductCommand， 在降级机制中使用了传递进来的商品 id 参数，从本地缓存获取了其他的商品信息，返回的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/103.html":{"url":"hystrix/103.html","title":"103. 基于双层嵌套 command 开发商品服务接口的多级降级机制","keywords":"","body":" 103. 基于双层嵌套 command 开发商品服务接口的多级降级机制 多级降级机制 Fallback: Cache via Network 商品服务接口，使用多级降级策略 103. 基于双层嵌套 command 开发商品服务接口的多级降级机制 多级降级机制 顾名思义就是有多种降级方案呢，这里使用 2 级； 就是 一个正常 command 中 fallback 调用另外 command； 常见的多级降级做法，如： 访问 mysql 数据库，但是访问报错 降级去 redis 中获取数据，由于是通过网络，有可能也报错 再次降级从本地 ehcache 中获取数据 通过下面官网小例子代码，非常方便的能看出来这个多级降级是怎么回事 Fallback: Cache via Network 官网直达 有时，如果后端服务失败，可以从 memcached 等缓存服务检索过时的数据版本。 由于 failback 将通过网络传递，这是另一个可能的故障点，因此还需要使用 HystrixCommand或 HystrixObservableCommand 对其进行包装。 public class CommandWithFallbackViaNetwork extends HystrixCommand { private final int id; protected CommandWithFallbackViaNetwork(int id) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"RemoteServiceX\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"GetValueCommand\"))); this.id = id; } @Override protected String run() { // RemoteServiceXClient.getValue(id); throw new RuntimeException(\"force failure for example\"); } @Override protected String getFallback() { // 降级机制执行了另外一个 command return new FallbackViaNetwork(id).execute(); } private static class FallbackViaNetwork extends HystrixCommand { private final int id; public FallbackViaNetwork(int id) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"RemoteServiceX\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"GetValueFallbackCommand\")) // 注意这里：需要使用和正常 command 不一样的线程池 // 因为正常 comman 执行降级的话有可能是因为线程池满了导致的 .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"RemoteServiceXFallback\"))); this.id = id; } @Override protected String run() { // 第一级降级策略：通过网络获取数据 MemCacheClient.getValue(id); } @Override protected String getFallback() { // 第二级降级策略：可以使用 stubbed fallback 方案返回残缺的数据 // 也可以返回一个 null return null; } } } 把这个降级机制运用到我们的业务中 商品服务接口，使用多级降级策略 商品接口拉取 command，主流程是从主机房去访问商品服务，如果主机房的服务出现了故障（如机房断电，机房的网络负载过高，机器硬件出了故障） 第一级降级策略：去访问备用机房的服务 第二级降级策略：用 stubbed fallback 降级策略，比较常用的，返回一些残缺的数据回去 package cn.mrcode.cachepdp.eshop.cache.ha.hystrix.command; import com.alibaba.fastjson.JSON; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixThreadPoolKey; import cn.mrcode.cachepdp.eshop.cache.ha.http.HttpClientUtils; import cn.mrcode.cachepdp.eshop.cache.ha.model.ProductInfo; /** * ${todo} * * @author : zhuqiang * @date : 2019/6/1 23:45 */ public class GetProductCommand2 extends HystrixCommand { private final Long productId; public GetProductCommand2(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) // 不同的线程池 .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"GetProductCommand2Pool\")) ); this.productId = productId; } @Override protected String getCacheKey() { return String.valueOf(productId); } @Override protected ProductInfo run() throws Exception { System.out.println(\"正常流程获取\"); if (productId == 2) { throw new RuntimeException(\"模拟正常流程获取失败\"); } String url = \"http://localhost:7000/getProduct?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); return JSON.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { System.out.println(\"正常流程降级策略\"); return new CommandWithFallbackViaNetwork(productId).execute(); } public class CommandWithFallbackViaNetwork extends HystrixCommand { private Long productId; protected CommandWithFallbackViaNetwork(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"CommandWithFallbackViaNetworkGroup\")) // 不同的线程池 .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"CommandWithFallbackViaNetworkPool\")) ); } @Override protected ProductInfo run() throws Exception { System.out.println(\"第一级降级\"); if (productId == 2) { throw new RuntimeException(\"模拟一级策略获取失败\"); } // 第一级降级策略：本来是该调用另外一个机房的服务 // 我们这里没有另外的机房，还是调用原来的服务 String url = \"http://localhost:7000/getProduct?productId=\" + productId; String response = HttpClientUtils.sendGetRequest(url); return JSON.parseObject(response, ProductInfo.class); } @Override protected ProductInfo getFallback() { System.out.println(\"第二级降级\"); // 第二级降级策略：使用残缺模式返回数据 ProductInfo productInfo = new ProductInfo(); productInfo.setId(productId); // 下面的数据可以从本地 ehcache 中获取数据填充后返回 productInfo.setName(\"二级降级：残缺数据\"); return productInfo; } } } 测试地址：http://localhost:7001/getProduct?productId=2 (在对应 Controller 中调用就不贴了) 输出日志如下 正常流程获取 正常流程降级策略 第一级降级 第二级降级 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"hystrix/104.html":{"url":"hystrix/104.html","title":"104. 基于 facade command 开发商品服务接口的手动降级机制","keywords":"","body":" 104. 基于 facade command 开发商品服务接口的手动降级机制 Primary + Secondary with Fallback 视频中用例 104. 基于 facade command 开发商品服务接口的手动降级机制 Primary + Secondary with Fallback 官网直达 有些系统具有双重模式行为——主模式和辅助模式，或者主模式和故障转移模式。 有时，次要或故障转移被认为是一种故障状态，它仅用于 fallback；在这些场景中，前面章节的 Fallback: Cache via Network 符合这种需求 而本章的准备模式降级策略适合你已经知道该使用主还是备用 command；比如测试一段新的代码， 通过一个状态在新老代码之前手动进行切换； 实现本模式的核心思路是： 有 3 个 command 一个主 command ：比如调用新代码的逻辑 一个备用/次要 command：比如调用旧代码的逻辑 一个门面 command：它只用来决策到底是调用 主？还是备？ 三个 command 都有降级策略，如果主备都不能使用，且降级策略也不能使用，那么将会走门面 command 的降级策略 这里解释下为什么门面 command 需要使用信号量：因为它的工作只是通过状态判定是否走主备， 而主备则会通过线程池的方式去真正执行业业务逻辑。所以这里是一个优化性能的地方 package cn.mrcode.cachepdp.eshop.cache.ha; import com.netflix.config.ConfigurationManager; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixCommandKey; import com.netflix.hystrix.HystrixCommandProperties; import com.netflix.hystrix.HystrixThreadPoolKey; import com.netflix.hystrix.strategy.concurrency.HystrixRequestContext; import org.apache.commons.configuration.AbstractConfiguration; import org.junit.Test; import static org.junit.Assert.assertEquals; public class CommandFacadeWithPrimarySecondary extends HystrixCommand { private final boolean usePromary; private final int id; public CommandFacadeWithPrimarySecondary(int id) { super(Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"SystemX\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"PrimarySecondaryCommand\")) .andCommandPropertiesDefaults( // 这里使用信号量，因为至少包装其他两个 command， // 其他两个 command 会使用线程池 HystrixCommandProperties.Setter() .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE))); this.id = id; AbstractConfiguration configInstance = ConfigurationManager.getConfigInstance(); System.out.println(configInstance); usePromary = configInstance.getBoolean(\"primarySecondary.usePrimary\"); } @Override protected String run() { System.out.println(\"============================= usePromary：\" + usePromary); if (usePromary) { return new PrimaryCommand(id).execute(); } else { return new SecondaryCommand(id).execute(); } } @Override protected String getFallback() { return \"static-fallback-\" + id; } // 在同一个上下文中执行 相同 id的话，打开了缓存就会走缓存的 // @Override // protected String getCacheKey() { // return String.valueOf(id); // } private static class PrimaryCommand extends HystrixCommand { private final int id; private PrimaryCommand(int id) { super(Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"SystemX\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"PrimaryCommand\")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"PrimaryCommand\")) .andCommandPropertiesDefaults( // 设置为超时未 600 毫秒 HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(600))); this.id = id; } @Override protected String run() { // 执行主服务调用 System.out.println(\"--------------- \" + \"responseFromPrimary-\" + id); return \"responseFromPrimary-\" + id; } } private static class SecondaryCommand extends HystrixCommand { private final int id; private SecondaryCommand(int id) { super(Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"SystemX\")) .andCommandKey(HystrixCommandKey.Factory.asKey(\"SecondaryCommand\")) .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"SecondaryCommand\")) .andCommandPropertiesDefaults( // 设置超时为 100 毫秒 HystrixCommandProperties.Setter().withExecutionTimeoutInMilliseconds(100))); this.id = id; } @Override protected String run() { // 由于超时的设置，意味着备用服务将会更快的响应数据 // 主备设置不同的超时时间，表达的意思是，他们调用响应数据的时间一个慢，一个快 System.out.println(\"--------------- \" + \"responseFromSecondary-\" + id); return \"responseFromSecondary-\" + id; } } public static class UnitTest { @Test public void testPrimary() { HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { ConfigurationManager.getConfigInstance().setProperty(\"primarySecondary.usePrimary\", true); assertEquals(\"responseFromPrimary-20\", new CommandFacadeWithPrimarySecondary(20).execute()); // 切换为使用备用 ConfigurationManager.getConfigInstance().setProperty(\"primarySecondary.usePrimary\", false); assertEquals(\"responseFromSecondary-20\", new CommandFacadeWithPrimarySecondary(20).execute()); } finally { context.shutdown(); ConfigurationManager.getConfigInstance().clear(); } } @Test public void testSecondary() { HystrixRequestContext context = HystrixRequestContext.initializeContext(); try { ConfigurationManager.getConfigInstance().setProperty(\"primarySecondary.usePrimary\", false); assertEquals(\"responseFromSecondary-20\", new CommandFacadeWithPrimarySecondary(20).execute()); } finally { context.shutdown(); ConfigurationManager.getConfigInstance().clear(); } } } } 输出日志 com.netflix.config.ConcurrentCompositeConfiguration@794cb805 ============================= usePromary：true --------------- responseFromPrimary-20 com.netflix.config.ConcurrentCompositeConfiguration@794cb805 ============================= usePromary：false --------------- responseFromSecondary-20 视频中用例 和上面例子一样， 3 个 command 主：使用正常的接口请求数据 备：使用残缺的拼接返回数据 门面：通过自定义的一个配置类中的一个属性，进行判定走哪一个 command 最后在 controller 中提供了一个接口，在程序运行过程中，我们可以访问这个接口， 手动的去更改这个配置的属性，进行手动降级 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"hystrix/105.html":{"url":"hystrix/105.html","title":"105. 生产环境中的线程池大小以及 timeout 超时时长优化经验总结","keywords":"","body":" 105. 生产环境中的线程池大小以及 timeout 超时时长优化经验总结 优化配置流程 优化经验 105. 生产环境中的线程池大小以及 timeout 超时时长优化经验总结 在生产环境里面，一个线程池的大小和 timeout 时长怎么设置？如果设置不合理问题还是很大的 优化配置流程 有两种方案： 一上来就设置大一点，然后再逐步优化降低 一上来就默认配置，然后再逐步优化 这里讲解使用默认值方式，再逐步优化的流程 一开始先不要设置 tomeout 超时时长，默认就是 1000ms（1s） 一开始先不要设置线程池大小，默认就是 10 直接部署到生产环境，如果运行良好，那么就不用管了 依赖标准的监控和报警机制来捕获到系统的异常运行情况（观察 24 小时） 根据调用延迟的占比、流量来计算出让断路器生效的最小配置数值 可以通过 hystrix 配置进行热修改，然后继续在 hystrix dashboard 上监控 监控在后面章节会讲解，但是这个热修改不知道后面有没有， 前面的章节热修改只提到过等待队列的热修改，但是还有限制。 通过以上步骤循环观察，知道调优到最佳 优化经验 假设对一个依赖服务的高峰调用 QPS 是每秒 30 次 默认线程池大小为 10 每秒高峰访问次数 99% 的访问延时 + 队列 buffer = 30 0.2 + 4 = 10 线程 也就是说，99 % 的访问都在 200 毫秒（0.2 秒）内响应；每个线程 1 秒钟处理 3 个线程； 3 * 200 = 600 毫秒；但是别忘记了，还有队列中等待的时间，一个线程先执行，最后一个线程最长会等待 400 毫秒时间 超时时间：按照 99.5% 的访问延时时间进行设置 一次访问 200 毫秒内响应（TP99 如果是 200 ms），再加一次重试时间，为 300 毫秒 疑问：那么理想的超时时间是 400 毫秒（上面已经说过，加上等待时间），但是这里感觉又不对， 假如是 400 毫秒，都超时那么低三个线程将会超过 1 秒钟，导致 1 秒钟内处理不了 3 个线程。 下一秒就会有另外的 30 个线程进来，就会导致线程池已满，部分线程就直接被 reject 了 所以这第一步计算线程公司是不是就有问题？ 总之，这里的意思就是要让 1 秒内所有的线程都能执行完，根据并发计算到的数量； 那么线程大小和超时时间能导致什么情况出现呢？如下图 左侧为 tomcat 容器线程，右侧为 hystrix 线程池，如果设置的不合理， 将导致大量线程被阻塞在 hystrix 上，也就间接导致 tomcat 的线程被阻塞住， 这样就会导致 tomcat 没有更多的资源去处理其他的服务调用； 如果设置得合理，就算这个服务调用比较耗时，那么也只是这个服务被降级的多，很快就返回了， 并不会影响其他的服务资源调用 大家可能会想，每秒的高峰访问次数是 30 次，如果是 300 次，甚至是 3000 次、30000次呢？ 30000 * 0.2 = 6000 + buffer = 6100，一个服务器内一个线程池给 6000 个线程， 这很恐怖的，如果你一个依赖服务占据的线程数量太多的话，会导致其他的依赖服务对应的线程池里没有资源可以用了， 因为一个 tomcat 总的并发数量是有限的。 那么在这种情况下，其实就需要更多的机器来支撑了。 你要真的说是，你的公司服务的用户量，或者数据量，或者请求量，真要是到了每秒几万的QPS， 30000QPS：60 * 3 = 1分钟 180万 访问量，一小时就按 1 亿计算，这个就很大了的量了； 用几十台服务器去支撑，我觉得很正常，一般 QPS 每秒在几千都算多的了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/106.html":{"url":"hystrix/106.html","title":"106. 生产环境中的线程池自动扩容与缩容的动态资源分配经验","keywords":"","body":" 106. 生产环境中的线程池自动扩容与缩容的动态资源分配经验 配置 withMaximumSize 无效解决 106. 生产环境中的线程池自动扩容与缩容的动态资源分配经验 可能会出现一种情况，比如说我们的某个依赖在高峰期，需要耗费 100 个线程，但是在那个时间段，刚好其他的依赖的线程池其实就维持一两个就可以了 但是，如果我们都是设置死的，每个服务就给 10个 线程，那就很坑，可能就导致有的服务在高峰期需要更多的资源，但是没资源了，导致很多的 reject 但是其他的服务，每秒钟就易一两个请求，结果也占用了 10个 线程，占着茅坑不拉屎 可以利用 hystrix 的配置成弹性的线程资源调度的模式 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"test-group\")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 配置线程池大小，同时并发能力个数 .withCoreSize(10) // 设置线程池的最大大小，只有在设置 allowMaximumSizeToDivergeFromCoreSize 的时候才能生效 .withMaximumSize(100) // 设置之后，其实 coreSize 就失效了 .withAllowMaximumSizeToDivergeFromCoreSize(true) // 设置保持存活的时间，单位是分钟，默认是 1 // 当线程池中线程空闲超过该时间之后，就会被销毁 .withKeepAliveTimeMinutes(1) // 配置等待线程个数；如果不配置该项，则没有等待，超过则拒绝 .withMaxQueueSize(5) // 由于 maxQueueSize 是初始化固定的，该配置项是动态调整最大等待数量的 // 可以热更新；规则：只能比 MaxQueueSize 小， .withQueueSizeRejectionThreshold(2) ) ); 通过以下测试方法 @Test public void test2() throws InterruptedException { int count = 13; CountDownLatch downLatch = new CountDownLatch(count); for (int i = 0; i { CommandLimit commandLimit = new CommandLimit(); String execute = commandLimit.execute(); System.out.println(Thread.currentThread() + \" \" + finalI + \" : \" + execute + \" : \" + new Date()); downLatch.countDown(); }).start(); } downLatch.await(); test3(); // 休眠一分钟后，再次访问，查看线程池中线程 TimeUnit.MINUTES.sleep(1); test3(); } @Test public void test3() throws InterruptedException { int count = 13; CountDownLatch downLatch = new CountDownLatch(count); for (int i = 0; i { CommandLimit commandLimit = new CommandLimit(); String execute = commandLimit.execute(); System.out.println(Thread.currentThread() + \" \" + finalI + \" : \" + execute + \" : \" + new Date()); downLatch.countDown(); }).start(); } downLatch.await(); } 输出日志 Thread[Thread-5,5,main] 5 : 降级 : Sun Jun 16 16:19:07 CST 2019 Thread[Thread-11,5,main] 11 : 降级 : Sun Jun 16 16:19:07 CST 2019 Thread[Thread-6,5,main] 6 : 降级 : Sun Jun 16 16:19:07 CST 2019 Thread[Thread-1,5,main] 1 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-2,5,main] 2 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-8,5,main] 8 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-0,5,main] 0 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-3,5,main] 3 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-7,5,main] 7 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-4,5,main] 4 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-12,5,main] 12 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-10,5,main] 10 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-9,5,main] 9 : success : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-25,5,main] 12 : 降级 : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-17,5,main] 4 : 降级 : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-24,5,main] 11 : 降级 : Sun Jun 16 16:19:08 CST 2019 Thread[Thread-14,5,main] 1 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-13,5,main] 0 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-16,5,main] 3 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-15,5,main] 2 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-18,5,main] 5 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-19,5,main] 6 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-20,5,main] 7 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-21,5,main] 8 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-23,5,main] 10 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-22,5,main] 9 : success : Sun Jun 16 16:19:09 CST 2019 Thread[Thread-38,5,main] 12 : 降级 : Sun Jun 16 16:20:09 CST 2019 Thread[Thread-37,5,main] 11 : 降级 : Sun Jun 16 16:20:09 CST 2019 Thread[Thread-36,5,main] 10 : 降级 : Sun Jun 16 16:20:09 CST 2019 Thread[Thread-27,5,main] 1 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-28,5,main] 2 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-31,5,main] 5 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-26,5,main] 0 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-30,5,main] 4 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-35,5,main] 9 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-33,5,main] 7 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-29,5,main] 3 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-32,5,main] 6 : success : Sun Jun 16 16:20:10 CST 2019 Thread[Thread-34,5,main] 8 : success : Sun Jun 16 16:20:10 CST 2019 三个被降级，因为一次只能最大 10 个线程，会发现所有的线程名称都是新的， 所以这里每一个 command 都是一个新的线程，但是在官网文档中又看到说，在空闲之后， 会把线程销毁。这个就看不太明白是怎么一回事情了 配置 withMaximumSize 无效解决 项目中实战 hystrix 的时候发现这里的配置老是不太对, 如下面这个配置，我给了最大线程 100，但是给 50 个并发线程就会大量的降级 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"test-group\")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 配置线程池大小，同时并发能力个数 .withCoreSize(10) // 设置线程池的最大大小，只有在设置 allowMaximumSizeToDivergeFromCoreSize 的时候才能生效 .withMaximumSize(100) // 设置之后，其实 coreSize 就失效了 .withAllowMaximumSizeToDivergeFromCoreSize(true) // 设置保持存活的时间，单位是分钟，默认是 1 // 当线程池中线程空闲超过该时间之后，就会被销毁 .withKeepAliveTimeMinutes(1) // 配置等待线程个数；如果不配置该项，则没有等待，超过则拒绝 .withMaxQueueSize(5) // 由于 maxQueueSize 是初始化固定的，该配置项是动态调整最大等待数量的 // 可以热更新；规则：只能比 MaxQueueSize 小， .withQueueSizeRejectionThreshold(2) ) ); 这个问题困扰了我很长时间，后来跟踪源码，当 command 被拒绝的时候，会报错 Caused by: java.util.concurrent.RejectedExecutionException: Rejected command because thread-pool queueSize is at rejection threshold. at com.netflix.hystrix.strategy.concurrency.HystrixContextScheduler$HystrixContextSchedulerWorker.schedule(HystrixContextScheduler.java:103) 进入源码后会发现，这里会先判定线程池队列大小是否已经超过了 queueSizeRejectionThreshold 大小； 在前面已经学到过该大小是可以动态调整的。含义是调整队列大小； com.netflix.hystrix.strategy.concurrency.HystrixContextScheduler.HystrixContextSchedulerWorker#schedule(rx.functions.Action0) @Override public Subscription schedule(Action0 action) { if (threadPool != null) { if (!threadPool.isQueueSpaceAvailable()) { throw new RejectedExecutionException(\"Rejected command because thread-pool queueSize is at rejection threshold.\"); } } return worker.schedule(new HystrixContexSchedulerAction(concurrencyStrategy, action)); } com.netflix.hystrix.HystrixThreadPool.HystrixThreadPoolDefault#isQueueSpaceAvailable @Override public boolean isQueueSpaceAvailable() { if (queueSize 那么这里有一个疑问了，为什么会先判断是否已经超了？ 这句代码 return worker.schedule(new HystrixContexSchedulerAction(concurrencyStrategy, action)); 继续深入 schedule 方法源码 ThreadPoolExecutor executor = (ThreadPoolExecutor) threadPool.getExecutor(); FutureTask f = (FutureTask) executor.submit(sa); 会发现使用了 ThreadPoolExecutor.submit 方法；我在 debug 的时候看过了 ThreadPoolExecutor 的参数，的确是我们设置的； 之前不知道 ThreadPoolExecutor 的用法，所以根本想不到为什么。 百度之后，ThreadPoolExecutor 会检查队列，但是在这之前 hystrix 会先检查一次，所以就导致了还没有进入线程池就报错了； 那么问题是为什么队列里面有这么的线程，而且在日志中看不出来到底有几个线程在执行呢？最简单的回答就是 hystrix 包装了线程池； 最后找到一个根源问题：百度之后，创建 ThreadPoolExecutor 的时候，会传入一个 BlockingQueue，如果使用无限容量的阻塞队列(如 LinkedBlockingQueue)时， 不会创建临时线程(因为队列不会满)，所以线程数保持 corePoolSize。 而刚好在 debug 时看到的队列就是 LinkedBlockingQueue； 这个时候就真相大白了；原因就是设置了队列大小！下面使用一个 ThreadPoolExecutor 的测试用例来浮现这个问题 @Test public void test4() throws InterruptedException, ExecutionException { BlockingQueue queue = new SynchronousQueue(); // 1 queue = new LinkedBlockingDeque<>(20); // 2 ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor( 10, 50, 60, TimeUnit.SECONDS, queue); CountDownLatch c = new CountDownLatch(40); IntStream.range(0, 40) .parallel() .mapToObj(item -> (Runnable) () -> { System.out.println(Thread.currentThread().getName()); try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } c.countDown(); }) .forEach(item -> threadPoolExecutor.submit(item)); c.await(); } 当使用 LinkedBlockingDeque 时，运行后观察控制台，只会发现最多只有 20 个线程被复用；这里的队列大小是 20； 当使用 SynchronousQueue 时，就会瞬间出现 40 个线程 回到最初的问题：配置 withMaximumSize 偶尔有效偶尔无效 解决方案：想使用 withMaximumSize 动态调整线程数量的时候，就不要设置等待队列； 如下 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) // 不同的线程池 .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"GetProductCommand2Pool\")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 配置线程池大小，同时并发能力个数 .withCoreSize(10) // 设置线程池的最大大小，只有在设置 allowMaximumSizeToDivergeFromCoreSize 的时候才能生效 .withMaximumSize(50) // 设置之后，其实 coreSize 就失效了 .withAllowMaximumSizeToDivergeFromCoreSize(true) // 设置保持存活的时间，单位是分钟，默认是 1 // 当线程池中线程空闲超过该时间之后，就会被销毁 .withKeepAliveTimeMinutes(1) // 配置等待线程个数；如果不配置该项，则没有等待，超过则拒绝 // .withMaxQueueSize(20) // .withQueueSizeRejectionThreshold(20) // 由于 maxQueueSize 是初始化固定的，该配置项是动态调整最大等待数量的 // 可以热更新；规则：只能比 MaxQueueSize 小， // .withQueueSizeRejectionThreshold(2) ) ); ::: tip 前面和本章讲解的配置相关实践的内容，这些在 官网文档 中都有写到，老样子就是全部是英文文档 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/107.html":{"url":"hystrix/107.html","title":"107. hystrix 的 metric 统计相关的各种高阶配置讲解","keywords":"","body":" 107. hystrix 的 metric 统计相关的各种高阶配置讲解 为什么需要监控与报警？ hystrix 的事件类型 execute event type fallback event type 其他的 event type thread pool event type collapser event type metric storage metric 统计相关的配置 metrics.rollingStats.timeInMilliseconds metrics.rollingStats.numBuckets metrics.rollingPercentile.enabled metrics.rollingPercentile.timeInMilliseconds metrics.rollingPercentile.numBuckets metrics.rollingPercentile.bucketSize metrics.healthSnapshot.intervalInMilliseconds 107. hystrix 的 metric 统计相关的各种高阶配置讲解 metric：度量标准 为什么需要监控与报警？ HystrixCommand 执行的时候，会生成一些执行耗时等方面的统计信息。 这些信息对于系统的运维来说，是很有帮助的，因为我们通过这些统计信息可以看到整个系统是怎么运行的。 hystrix 对每个 command key 都会提供一份 metric，而且是秒级统计粒度的。 这些统计信息，无论是单独看，还是聚合起来看，都是很有用的。 如果将一个请求中的多个 command 的统计信息拿出来单独查看，包括耗时的统计，对 debug 系统是很有帮助的。 聚合起来的 metric 对于系统层面的行为来说，是很有帮助的，很适合做报警或者报表。 hystrix dashboard 就很适合。 hystrix 的事件类型 官网直达 很详细的 metric 讲解，还有他的流使用示例 对于 hystrix command 来说只会返回一个值，execute 只有一个 event type，fallback 也只有一个event type，那么返回一个 SUCCESS 就代表着命令执行的结束 对于 hystrix observable command 来说，多个值可能被返回，所以： emit event 代表一个 value 被返回 success 代表成功 failure 代表异常 execute event type EMIT observable command 返回一个 value SUCCESS 完成执行，并且没有报错 FAILURE 执行时抛出了一个异常，会触发 fallback TIMEOUT 开始执行了，但是在指定时间内没有完成执行，会触发 fallback BAD_REQUEST 执行的时候抛出了一个 HystrixBadRequestException SHORT_CIRCUITED 短路器打开了，触发 fallback THREAD_POOL_REJECTED 线程池的容量满了，被 reject，触发 fallback SEMAPHORE_REJECTED 信号量的容量满了，被 reject，触发 fallback fallback event type FALLBACK_EMIT observable command，fallback value 被返回了 FALLBACK_SUCCESS fallback 逻辑执行没有报错 FALLBACK_FAILURE fallback 逻辑抛出了异常，会报错 FALLBACK_REJECTION fallback 的信号量容量满了，fallback 不执行，报错 FALLBACK_MISSING fallback 没有实现，会报错 其他的 event type EXCEPTION_THROWN command 生命自周期是否抛出了异常 RESPONSE_FROM_CACHE command 是否在 cache 中查找到了结果 COLLAPSED command 是否是一个合并 batch 中的一个 thread pool event type EXECUTED 线程池有空间，允许 command 去执行了 REJECTED 线程池没有空间，不允许 command 执行，reject 掉了 collapser event type BATCH_EXECUTED collapser 合并了一个 batch，并且执行了其中的 command ADDED_TO_BATCH command 加入了一个 collapser batch RESPONSE_FROM_CACHE 没有加入 batch，而是直接取了 request cache 中的数据 metric storage metric 被生成之后，就会按照一段时间来存储，存储了一段时间的数据才会推送到其他系统中， 比如 hystrix dashboard；另外一种方式，就是每次生成 metric 就实时推送 metric 流到其他地方， 但是这样的话，会给系统带来很大的压力 hystrix 的方式是将 metric 写入一个内存中的数据结构中，在一段时间之后就可以查询到； hystrix 1.5x 之后，采取的是为每个 command key 都生成一个 start event 和 completion event 流， 而且可以订阅这个流。每个 thread pool key 也是一样的，包括每个 collapser key 也是一样的。 每个 command 的 event 是发送给一个线程安全的 RxJava 中的 rx.Subject，因为是线程安全的，所以不需要进行线程同步 因此每个 command 级别的，threadpool 级别的，每个 collapser 级别的，event 都会发送到对应的 RxJava的 rx.Subject 对象中。这些 rx.Subject 对象接着就会被暴露出 Observable 接口，可以被订阅。 metric 统计相关的配置 官网直达 metrics.rollingStats.timeInMilliseconds 设置统计的 rolling window，单位是毫秒，hystrix 只会维持这段时间内的 metric 供短路器统计使用 这个属性是不允许热修改的，默认值是 10000，就是 10 秒钟 HystrixCommandProperties.Setter() .withMetricsRollingStatisticalWindowInMilliseconds(int value) metrics.rollingStats.numBuckets 该属性设置每个滑动窗口被拆分成多少个 bucket，而且滑动窗口对这个参数必须可以整除，同样不允许热修改 默认值是 10，也就是说，每秒钟是一个 bucket 随着时间的滚动，比如又过了一秒钟，那么最久的一秒钟的 bucket 就会被丢弃，然后新的一秒的 bucket 会被创建 HystrixCommandProperties.Setter() .withMetricsRollingStatisticalWindowBuckets(int value) metrics.rollingPercentile.enabled 控制是否追踪请求耗时，以及通过百分比方式来统计，默认是 true HystrixCommandProperties.Setter() .withMetricsRollingPercentileEnabled(boolean value) metrics.rollingPercentile.timeInMilliseconds 设置 rolling window 被持久化保存的时间，这样才能计算一些请求耗时的百分比，默认是 60000 = 60s，不允许热修改 相当于是一个大的 rolling window，专门用于计算请求执行耗时的百分比 HystrixCommandProperties.Setter() .withMetricsRollingPercentileWindowInMilliseconds(int value) metrics.rollingPercentile.numBuckets 设置 rolling percentile window 被拆分成的 bucket 数量，上面那个参数除以这个参数必须能够整除，不允许热修改 默认值是 6，也就是每 10s 被拆分成一个 bucket HystrixCommandProperties.Setter() .withMetricsRollingPercentileWindowBuckets(int value) metrics.rollingPercentile.bucketSize 设置每个 bucket 的请求执行次数被保存的最大数量，如果在一个 bucket 内，执行次数超过了这个值，那么就会重新覆盖从 bucket 的开始再写 举例来说，如果 bucket size 设置为 100，而且每个 bucket 代表一个 10 秒钟的窗口， 但是在这个 bucket 内发生了 500 次请求执行，那么这个 bucket 内仅仅会保留 100 次执行 如果调大这个参数，就会提升需要耗费的内存，来存储相关的统计值，不允许热修改 默认值是 100 HystrixCommandProperties.Setter() .withMetricsRollingPercentileBucketSize(int value) metrics.healthSnapshot.intervalInMilliseconds 控制成功和失败的百分比计算，与影响短路器之间的等待时间，默认值是 500 毫秒 HystrixCommandProperties.Setter() .withMetricsHealthSnapshotIntervalInMilliseconds(int value) ::: tip 一般来说这些配置使用默认即可 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/108.html":{"url":"hystrix/108.html","title":"108. hystrix dashboard 可视化分布式系统监控环境部署","keywords":"","body":" 108. hystrix dashboard 可视化分布式系统监控环境部署 turbine-web 安装 hystrix-metrics-event-stream turbine-web 启动后效果查看 安装 hystrix-dashboard 仪表盘注意事项 hystrix-dashboard 含义 线程池 108. hystrix dashboard 可视化分布式系统监控环境部署 官网直达 里面讲解了怎么安装 dashboard 和监控中的一些数值表示什么意思 turbine-web 安装 turbine-web 是监控集群的，可以聚合集群中的所有机器相同的度量标准到一个报表中 由于在官网中，只找到了打包好的 turbine-web ，就使用它来启动 下载：https://github.com/downloads/Netflix/Turbine/turbine-web-1.0.0.war 要使用集群功能，需要先配置有哪些机器 在 /WEB-INF/classes/config.properties 文件中增加以下配置 turbine.ConfigPropertyBasedDiscovery.default.instances=localhost turbine.instanceUrlSuffix=:7001/hystrix.stream 以上配置： instances ： 实例 ip instanceUrlSuffix：实例前缀 instanceUrlSuffix 是什么意思呢？在前面已经讲解到，单个项目需要暴露出订阅流信息， 供其他程序进行处理展示，所以需要先配置暴露流的功能 hystrix-metrics-event-stream 官网直达 增加依赖 compile 'com.netflix.hystrix:hystrix-metrics-event-stream:1.4.10' 将提供的 HystrixMetricsStreamServlet 注册到系统中 @Bean public ServletRegistrationBean indexServletRegistration() { ServletRegistrationBean registration = new ServletRegistrationBean(new HystrixMetricsStreamServlet()); registration.addUrlMappings(\"/hystrix.stream\"); return registration; } 测试地址：http://localhost:7001/hystrix.stream ping: // 当刚启动系统时，没有任何度量数据会一直刷屏 ping // 然后访问一下触发几个 command 执行之后，就会有相关的统计信息输出了 data: {\"type\":\"HystrixCommand\",\"name\":\"GetValueForKy\",\" turbine-web 启动后效果查看 由于下载的是 war 包，修改配置之后，放在一个 tomcat7+ 版本中，启动 tomcat war 包地址： D:\\Program Files\\apache-tomcat-7.0.42\\webapps\\turbine.war (原始 war 包名称修改过) 运行 bat 启动 tomcat D:\\Program Files\\apache-tomcat-7.0.42\\bin\\startup.bat 访问地址：http://localhost:8080/turbine/turbine.stream ，可以看到和访问单个项目是一样的效果 安装 hystrix-dashboard 下面的 war 包是被发布在 maven 仓库中的 war 包下载地址 下载后将 hystrix-dashboard-1.5.18.war 改名为 hystrix-dashboard.war，然后放到 tomcat D:\\Program Files\\apache-tomcat-7.0.42\\webapps\\ 下。重启 tomcat 访问地址：http://localhost:8080/hystrix-dashboard/ 本机缓存服务实例：http://localhost:7001/hystrix.stream 本机 turbine 集群：http://localhost:8080/turbine/turbine.stream 单个实例截图 集群实例截图 dashboard 上的指标都是什么？下面有说明一些简要的 圆圈的颜色和大小代表了健康状况以及流量，折线代表了最近 2 分钟的请求流量 监控图中用圆点来表示服务的健康状态，健康度从 100%-0% 分别会用绿色、黄色、橙色、红色来表示。 另外，这个圆点也会随着流量的增多而变大。 监控图中会用曲线（圆点旁边）来表示服务的流量情况，通过这个曲线可以观察单个接口的流量变化/趋势 Hosts：集群中的机器数量 Host、Cluster：集群中 QPS 平均值 上面有好几个彩色的数据： 右侧有颜色说明，10 秒内请求数量统计 那个灰色的百分比则是最近 10 秒内的异常请求比例 circuit：断路器的状态 最近一分钟的请求延时百分比，TP90，TP99，TP99.5 比如第一个 GetValueForKy：百分之 90 的耗时在 20 毫秒 本文中部分图来自 贲_WM 仪表盘注意事项 比如以下配置，使用动态大小这种 super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductCommandGroup\")) // 不同的线程池 .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"GetProductCommand2Pool\")) .andThreadPoolPropertiesDefaults(HystrixThreadPoolProperties.Setter() // 配置线程池大小，同时并发能力个数 .withCoreSize(5) // 设置线程池的最大大小，只有在设置 allowMaximumSizeToDivergeFromCoreSize 的时候才能生效 .withMaximumSize(100) // 设置之后，其实 coreSize 就失效了 .withAllowMaximumSizeToDivergeFromCoreSize(true) // 设置保持存活的时间，单位是分钟，默认是 1 // 当线程池中线程空闲超过该时间之后，就会被销毁 .withKeepAliveTimeMinutes(1) // 配置等待线程个数；如果不配置该项，则没有等待，超过则拒绝 // .withMaxQueueSize(5) // 由于 maxQueueSize 是初始化固定的，该配置项是动态调整最大等待数量的 // 可以热更新；规则：只能比 MaxQueueSize 小， // .withQueueSizeRejectionThreshold(2) ) ); 在仪表盘中是如下图显示，也就是说 ui 上显示的 poolSize 貌似就是这里 coreSize 的大小； 所以对于 ui 上每项含义还需要进一步弄清楚是什么意思 hystrix-dashboard 含义 线程池 Cluster：待证实 Host：待证实 Active：待证实 Queued：待证实 Pool Size ：目前线程池中存活的线程 注意：在 ui 上变大能看到变化，数值变小貌似是个 bug，不会实时的显示， 需要自己手动刷新页面才能看到变化 Max Active：待证实 Executions：目前正在执行的线程 Queue Size：等待队列大小 如果该项不为 0 那么一定是哪里设置了 maxQueueSize 或者 QueueSizeRejectionThreshold 的值 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"hystrix/109.html":{"url":"hystrix/109.html","title":"109. 生产环境中的hystrix分布式系统的工程运维经验总结","keywords":"","body":" 109. 生产环境中的hystrix分布式系统的工程运维经验总结 经验 笔者疑问 109. 生产环境中的hystrix分布式系统的工程运维经验总结 经验 如果发现了严重的依赖调用延时，先不用急着去修改配置，如果一个 command 被限流了，可能本来就应该限流 在 netflix 早期的时候，经常会有人在发现短路器因为访问延时发生的时候，去热修改一些配置遏制，比如线程池大小、队列大小、超时时长等等，给更多的资源，但是这其实是不对的 如果我们之前对系统进行了良好的配置，然后现在在高峰期，系统在进行线程池 reject、超时、短路、那么此时我们应该集中精力去看底层根本的原因，而不是调整配置 为什么在高峰期，一个 10 个线程的线程池，搞不定这些流量呢？代码写的太烂了？ 千万不要急于给你的依赖调用过多的资源，比如线程池大小、队列大小、超时时长、信号量容量等等，因为这可能导致我们自己对自己的系统进行 DDOS 攻击（疯狂的大量的访问你的机器，最后给打垮） 举例来说，想象一下，我们现在有 100 台服务器组成的集群，每台机器有 10个 线程大小的线程池去访问一个服务，那么我们对那个服务就有 1000个 线程资源去访问了 在正常情况下，可能只会用到其中 200~300个 线程去访问那个后端服务，但是如果在高峰期出现了访问延时，可能导致 1000 个线程全部被调用去访问那个后端服务，如果我们调整到每台服务器 20 个线程呢？ 如果因为你的代码等问题导致访问延时，即使有 20个 线程可能还是会导致线程池资源被占满，此时就有 2000个 线程去访问后端服务，可能对后端服务就是一场灾难 这就是断路器的作用了，如果我们把后端服务打死了，或者产生了大量的压力，有大量的 timeout 和 reject，那么就自动短路，一段时间后，等流量洪峰过去了，再重启访问 简单来说，让系统自己去限流、短路、超时、以及 reject，直到系统重新变得正常了 最后总结：就是不要随便乱改资源配置，不要随便乱增加线程池大小，等待队列大小，异常情况是正常的 笔者疑问 今天打算在 spring cloud 子系统中加上 hystrix-dashboard；通过 yml 自动配置最少参数后，能看见 ui 图了， 但是我发现它默认是「一个服务名一个线程池」，意思是所有的接口访问都是一个线程池管理的， 然而不加大线程没法让访问不报错。 项目是一个后台定时任务调度项目，调用接口频率高的时候，每秒平均也就 10 来个请求； 所以还是不太明白在实际开发中到底要怎么来分这个线程池？ 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"110.html":{"url":"110.html","title":"110. 高并发场景下恐怖的缓存雪崩现象以及导致系统全盘崩溃的后果","keywords":"","body":" 110. 高并发场景下恐怖的缓存雪崩现象以及导致系统全盘崩溃的后果 什么是缓存雪崩？ 行业真实的缓存雪崩经验和教训 110. 高并发场景下恐怖的缓存雪崩现象以及导致系统全盘崩溃的后果 什么是缓存雪崩？ 简单说：由于缓存不可用，导致大量请求访问后端服务，可能 mysql 扛不住高并发而打死， 像滚雪球一样，影响越来越大，最后导致整个网站崩溃不可用 至于为什么会像滚雪球一样？整个与整个系统的架构有关； 比如在这之前讲解的知识点（本教程 001~060 章内容），根据这个背景来讲解下雪崩过程 redis 集群彻底崩溃：不可用 缓存服务在请求 redis 时，会有大量的线程阻塞，占用资源 超时请求失败之后，会去 mysql 查询原始数据，mysql 抗不住，被打死 源头服务由于 mysql 被打死，对源服务的请求也被阻塞，占用资源 缓存服务大量的资源全部耗费在访问 redis 和 源服务上；最后自己被拖死，无法提供服务 nginx 无法访问缓存服务，只能基于本地缓存提供服务，当缓存过期后，就耗费在访问缓存服务上 最后整个网站崩溃，页面加载不出来任何数据 以下是这个流程的一个图解 行业真实的缓存雪崩经验和教训 某电商，之前就是出现过，整个缓存的集群彻底崩溃了，因为主要是集群本身的 bug，导致自己把自己给弄死了，虽然当时也是部署了双机房的，但是还是死了 该电商几乎所有的应用都是基于那个缓存集群去开发的，导致各种服务的线程资源全部被耗尽，然后用在了访问那个缓存集群时的等待、超时和报错上了；然后导致各种服务就没有资源对外提供服务 再加上各种降级措施也没做好，直接就是整体系统的全盘崩溃；导致网站就没法对外出售商品，导致了很大数额的经济的损失 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"111.html":{"url":"111.html","title":"111. 缓存雪崩的基于事前+事中+事后三个层次的完美解决方案","keywords":"","body":" 111. 缓存雪崩的基于事前+事中+事后三个层次的完美解决方案 事前解决方案 事中解决方案 ehcache 本地缓存 对 redis 访问的资源隔离 对源服务访问的限流以及资源隔离 事后解决方案 小结 111. 缓存雪崩的基于事前+事中+事后三个层次的完美解决方案 相对来说，考虑的比较完善的一套方案，分为事前、事中、事后三个层次去思考再怎么来应对缓存雪崩的场景 对于解决方案，再次强调一下，这个需要有上下文的，本课程的方案基本上就是基于本课程的缓存架构方案来讲解的 通过下面的架构图，来分析具体的方案内容 事前解决方案 发生缓存雪崩之前，事情之前，怎么去避免 redis 彻底挂掉 redis本身的高可用性、复制、主从架构，操作主节点，读写，数据同步到从节点，一旦主节点挂掉，从节点跟上 双机房部署，一套 redis cluster，部分机器在一个机房，另一部分机器在另外一个机房 还有一种部署方式，两套 redis cluster，两套 redis cluster 之间做一个数据的同步，redis 集群是可以搭建成树状的结构的 对于这种方式，没有明白怎么做数据同步？ 一旦说单个机房出了故障，至少说另外一个机房还能有些 redis 实例提供服务 事中解决方案 redis cluster 已经彻底崩溃了，已经开始大量的访问无法访问到 redis 了 ehcache 本地缓存 所做的多级缓存架构的作用上了 ，ehcache 的缓存应对零散的 redis 中数据被清除掉的现象，另外一个主要是预防 redis 彻底崩溃 多台机器上部署的缓存服务实例的内存中，还有一套 ehcache 的缓存，还能支撑一阵 对 redis 访问的资源隔离 对 redis 访问使用 hystrix 进行隔离，防止自己资源大量阻塞在访问 redis 上 对源服务访问的限流以及资源隔离 同上，防止自己资源大量阻塞在访问源服务上，同时 hystrix 在资源隔离时也做到了限流 事后解决方案 redis 数据可以恢复，之前讲解过各种备份机制，redis 数据备份和恢复，redis 重新启动起来 redis 数据彻底丢失了或者数据过旧，快速缓存预热，redis 重新启动起来 由于事中做了限流与隔离，缓存服务不会被打死，通过熔断策略 和 half-open 策略， 可以自动可以恢复对 redis 的访问，发现 redis 可以访问了，就自动恢复了 小结 基于 hystrix 的高可用服务这块技术之后，先讲解缓存服务如何设计成高可用的架构 缓存架构应对高并发下的缓存雪崩的解决方案，基于 hystrix 去做缓存服务的保护 要带着大家去实现的有什么东西？事前和事后不用了吧，事中 ehcache 本身也做好了 基于 hystrix 对 redis 的访问进行保护，对源服务的访问进行保护，讲解 hystrix 的时候， 也说过对源服务的访问怎么怎么进行这种高可用的保护 但是站的角度不同，源服务如果自己本身不知道什么原因出了故障，我们怎么去保护，调用商品服务的接口大量的报错、超时 总的来说就是：限流、资源隔离、降级 保证缓存服务不能死掉，同时快速恢复 redis cluster 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"112.html":{"url":"112.html","title":"112. 基于 hystrix 完成对 redis 访问的资源隔离以避免缓存服务被拖垮","keywords":"","body":"112. 基于 hystrix 完成对 redis 访问的资源隔离以避免缓存服务被拖垮 从本章开始，用几讲的时间，给咱们的 redis 访问这一块，加上保护措施，给商品服务的访问加上限流的保护措施（这里其实已经重复了，但是角度不一样，也就是场景不一样） 这里会使用之前的项目，就是有 storm 缓存预热的项目，那一套里面完成了之前的课程的知识点 redis 这一块，全都用 hystrix 的 command 进行封装，做资源隔离，确保 redis 的访问只能在固定的线程池内的资源来进行访问，哪怕是 redis 访问的很慢，有等待和超时也不要紧，只有少量额线程资源用来访问，缓存服务不会被拖垮 eshop-cache 项目中，对 redis 的操作有以下几个方法 // cn.mrcode.cachepdp.eshop.cache.service.impl.CacheServiceImpl# saveProductInfo2RedisCache getProductInfoOfRedisCache saveShopInfo2RedisCache getShopInfoOfRedisCache 把这几个方法修改成 hystrix 调用方式 添加依赖：compile 'com.netflix.hystrix:hystrix-core: 1.5.12' 上面四个方法，这里只贴出来其中一对，因为逻辑真的非常简单。 SaveProductInfo2RedisCommand.java package cn.mrcode.cachepdp.eshop.cache.command; import com.alibaba.fastjson.JSONObject; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import cn.mrcode.cachepdp.eshop.cache.model.ProductInfo; import redis.clients.jedis.JedisCluster; /** * @author : zhuqiang * @date : 2019/6/23 15:07 */ public class SaveProductInfo2RedisCommand extends HystrixCommand { private JedisCluster jedisCluster; private final ProductInfo productInfo; public SaveProductInfo2RedisCommand(ProductInfo productInfo) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"SaveProductInfo2RedisCommand\"))); this.productInfo = productInfo; } @Override protected Boolean run() throws Exception { String key = \"product_info_\" + productInfo.getId(); jedisCluster.set(key, JSONObject.toJSONString(productInfo)); return true; } public JedisCluster getJedisCluster() { return jedisCluster; } public void setJedisCluster(JedisCluster jedisCluster) { this.jedisCluster = jedisCluster; } } 调用处 /** * 将商品信息保存到redis中 */ public void saveProductInfo2RedisCache(ProductInfo productInfo) { // String key = \"product_info_\" + productInfo.getId(); // jedisCluster.set(key, JSONObject.toJSONString(productInfo)); SaveProductInfo2RedisCommand command = new SaveProductInfo2RedisCommand(productInfo); command.setJedisCluster(jedisCluster); command.execute(); } GetProductInfoOfRedisCommand.java package cn.mrcode.cachepdp.eshop.cache.command; import com.alibaba.fastjson.JSON; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import cn.mrcode.cachepdp.eshop.cache.model.ProductInfo; import redis.clients.jedis.JedisCluster; /** * @author : zhuqiang * @date : 2019/6/23 15:17 */ public class GetProductInfoOfRedisCommand extends HystrixCommand { private JedisCluster jedisCluster; private final Long productId; public GetProductInfoOfRedisCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductInfoOfRedisCommand\"))); this.productId = productId; } @Override protected ProductInfo run() throws Exception { String key = \"product_info_\" + productId; String json = jedisCluster.get(key); return JSON.parseObject(json, ProductInfo.class); } public JedisCluster getJedisCluster() { return jedisCluster; } public void setJedisCluster(JedisCluster jedisCluster) { this.jedisCluster = jedisCluster; } } ` 调用处 @Override public ProductInfo getProductInfoOfRedisCache(Long productId) { // String key = \"product_info_\" + productId; // String json = jedisCluster.get(key); // return JSON.parseObject(json, ProductInfo.class); GetProductInfoOfRedisCommand command = new GetProductInfoOfRedisCommand(productId); command.setJedisCluster(jedisCluster); return command.execute(); } 启动项目后访问：http://localhost:6002/getProductInfo?productId=1 测试该方法的调用成功 可以看到只是把原有的逻辑放到 command 中去了，这里可以尝试使用 command 的注解，就不用这么麻烦的抽到类中了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"113.html":{"url":"113.html","title":"113. 为 redis 集群崩溃时的访问失败增加 fail silent 容错机制","keywords":"","body":"113. 为 redis 集群崩溃时的访问失败增加 fail silent 容错机制 上一节课，我们已经通过 hystrix command 对 redis的访问进行了资源隔离； 避免 redis 访问频繁失败或者频繁超时的时候，耗尽大量的 tomcat 容器的资源阻塞在 redis 的访问上，限定只有一部分线程资源可以用来访问 redis 如果 redis 集群彻底崩溃了，这个时候，可能 command 对 redis 的访问大量的报错和 timeout 超时， 就会触发熔断（短路）机制 那么熔断机制触发后，就会调用降级 fallback，这里使用 fail silent 策略，直接返回 null 由于比较简单，只贴出其中一个 command 的降级 @Override protected ProductInfo getFallback() { return null; } 这里为什么会选择返回 null？其实与这块架构代码实现有关。 @RequestMapping(\"/getProductInfo\") @ResponseBody public ProductInfo getProductInfo(Long productId) { ProductInfo productInfo = cacheService.getProductInfoOfRedisCache(productId); log.info(\"从 redis 中获取商品信息\"); if (productInfo == null) { productInfo = cacheService.getProductInfoFromLocalCache(productId); log.info(\"从 ehcache 中获取商品信息\"); } if (productInfo == null) { // 两级缓存中都获取不到数据，那么就需要从数据源重新拉取数据，重建缓存 // 假设这里从数据库中获取的数据 String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); rebuildCache.put(productInfo); } return productInfo; } 从代码中可以看到： 先从 redis 中获取商品信息 如果没有获取到，则从 ehcache 中获取 如果 ehcache 中没有获取到，则从源服务获取 这里的多级缓存架构导致使用 fail silent 返回 null 非常合适， 当 redis 崩溃之后，触发了降级策略，在调用处（也就是上面代码中）是感知不到的， 只会说发现缓存命中率直线下降，因为全部返回了 null 当后续 redis 恢复后，短路器被关闭，又可以正常访问 redis 了，全自动 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"114.html":{"url":"114.html","title":"114. 为 redis 集群崩溃时的场景部署定制化的熔断策略","keywords":"","body":" 114. 为 redis 集群崩溃时的场景部署定制化的熔断策略 timeout 超时 熔断策略 有关配置代码 hystrix 配置默认值 114. 为 redis 集群崩溃时的场景部署定制化的熔断策略 缓存雪崩的解决方案的事中解决方案：发生缓存雪崩的时候 当 redis 集群崩溃的时候，会怎样? 首先大量的超时、等待、报错 如果是短时间内报错，会直接走 fallback 降级，直接返回 null 如果是大面积超时报错，就会开启短路器 这里就涉及到这几个参数的调整，不推荐使用默认值，需要根据业务场景和一定实际测试得出来的 timeout 超时 HystrixCommandProperties.Setter() .withExecutionTimeoutInMilliseconds(int value) 实际超时参数推荐先使用 hystrix dashboard 统计下 Tp99/95/90 ， 假设 TP99 在 100ms，设置为 100ms + 10ms 就可以了。 （在讲 hystrix 的时候，对于这个超时的计算感觉有点复杂，没有太搞明白） 配置超时的意义? 一旦 redis 出现了大面积故障，此时肯定访问耗时超过 100ms 以上，大量等待和超时， 此时超时的意义就有了，保证了不会让大量请求阻塞过长时间。在 hystrix 中超时就走 fallback 降级了 熔断策略 HystrixCommandProperties.Setter() .withCircuitBreakerRequestVolumeThreshold() 在滑动窗口中（rolling window），最少有多少个请求（默认为 20），才出发短路 如果设置为 20，那么在一个 10秒 的滑动窗口内，如果只有 19 个请求，就算这 19个 请求都是异常的， 也不会出发短路器 这个滑动请求数量，也应该根据业务访问量来设置； 比如平时一般的时候，浏览也可以再每秒 QPS 100，10秒 的滑动窗口也就是 1000， 一般来说，就可以设置为 1000 左右，该值过大或过小都不太合适 举个例子：假如设置为 20 ，那么在晚上最低峰的时候，刚好是 30 ，而且刚好超时多了一些， 结果就直接给短路了 HystrixCommandProperties.Setter() .withCircuitBreakerErrorThresholdPercentage(50) 这是异常请求量的百分比，当异常请求达到这个百分比时，就打开短路器，默认为 50 （也就是 50%） 同样，该值也需要根据业务场景来配置，比如 redis 真正崩溃了，那么肯定是百分百都超时异常了， 但是还有一种情况，可能是网络抖动导致短时间内超时，比如 10秒，1分钟 后就正常了 还有一种业务情况，金融类支付接口：这类接口可能就会设置得很低，因为对异常必须敏感， 可能就 10% 异常，就开启短路了，因为要求改接口必须稳定，不能容忍任何的延迟或是报错， 一旦有 10% 异常的话，基本上就可以认为这个接口已经出问题了，再继续提供服务的话， 有可能造成资金的错乱等问题，造成实际损失； 针对我们的缓存来说，对异常不是特别的敏感，这里设置为 80% ::: tip RequestVolumeThreshold 与 ErrorThresholdPercentage 是一个组合参数啊， 需要满足这两个条件才会触发，需要一起考虑 ::: HystrixCommandProperties.Setter() .withCircuitBreakerSleepWindowInMilliseconds() 在短路之后，需要在多长实际内直接 reject 请求？在这段时间之后，再重新 holf-open 状态， 尝试允许请求通过以及自动回复。默认值是 5000ms 同样，看场景。比如 redis 崩溃了，能在 5秒 内恢复吗？ 当然短路器开启与多方维度有关系，该值可以短一点，而且还有一个 holf-open 流程状态， 也不能太短，太短的话，也是没有任何意义的。 本列设置为 1 分钟 有关配置代码 .andCommandPropertiesDefaults( HystrixCommandProperties.Setter() .withExecutionTimeoutInMilliseconds(100) .withCircuitBreakerRequestVolumeThreshold(20) .withCircuitBreakerErrorThresholdPercentage(80) .withCircuitBreakerSleepWindowInMilliseconds(1000 * 60) ) hystrix 配置默认值 在以下类中定义的 com.netflix.hystrix.HystrixCommandProperties 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 14:59:34 "},"115.html":{"url":"115.html","title":"115. 基于 hystrix 限流完成源服务的过载保护以避免流量洪峰打死 MySQL","keywords":"","body":"115. 基于 hystrix 限流完成源服务的过载保护以避免流量洪峰打死 MySQL 前面讲解了 redis 集群彻底崩溃的时候，对 redis本身做资源隔离、超时控制、熔断策略； 那么此时的流量肯定都会转义到源服务去（提供商品数据的商品服务），源服务中又直接查询 mysql， 如果此时有 QPS 10000 转移到 mysql 上？那么 mysql 可能就凶多吉少了。 本章就是要解决对商品服务这种源服务的访问和增加限流措施 hystr 的限流机制这里就不再复述了，难的还是在业务场景上， 那么这里线程池相关参数设置多大合适？ 这个可以通过简单的计算一下：TP99 的商品访问能在 200ms 内响应，1个 线程每秒可以响应 5次， 假设我们一个缓存实例对这个商品服务的访问在每秒 150次。那么需要 30个线程，就可以满足需求； 另外一个，可以考虑在非正常情况下，比如网络短时间抖动，这时候 30 个线程 1 秒内肯定就处理不过来了， 这个时候就可以是适当的设置下等待队列的值。 ::: tip 重要 关于等待队列的值本人到现在都没有搞明白，需要怎么计算才算合适， 通过测试发现：假设 2秒 可以执行 1个 请求，我们设置超时时间为 3 秒，等待队列为 1个， 那么在 2 秒后，等待队列中的会释放出来执行，但是由于已经等待了 2 秒，他自身需要 2 秒才能响应， 这个时候其实第二个请求就会失败；如果是这样的话，那么我是不是就可以认为只要有等待队列，超时时间是否应该设置为双倍呢？ ::: .andThreadPoolPropertiesDefaults( HystrixThreadPoolProperties.Setter() .withCoreSize(30) .withMaxQueueSize(5) ) 业务代码如下，同样这里也只展示商品获取服务 package cn.mrcode.cachepdp.eshop.cache.command; import com.alibaba.fastjson.JSONObject; import com.netflix.hystrix.HystrixCommand; import com.netflix.hystrix.HystrixCommandGroupKey; import com.netflix.hystrix.HystrixThreadPoolProperties; import cn.mrcode.cachepdp.eshop.cache.model.ProductInfo; /** * @author : zhuqiang * @date : 2019/6/23 15:17 */ public class GetProductInfoOfMysqlCommand extends HystrixCommand { private final Long productId; public GetProductInfoOfMysqlCommand(Long productId) { super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(\"GetProductInfoOfMysqlCommand\")) .andThreadPoolPropertiesDefaults( HystrixThreadPoolProperties.Setter() .withCoreSize(30) .withMaxQueueSize(5) ) ); this.productId = productId; } @Override protected ProductInfo run() throws Exception { String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; ProductInfo productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); return productInfo; } @Override protected ProductInfo getFallback() { // 至于降级怎么做，下一章节会讲解 // 本人也希望能在下一章能讲解到稍微真实的一点场景处理 // 业务在本缓存架构代码中，假定是能百分比获取到商品信息的，如果被 reject 了，那么该怎么办？ return null; } } 调用处 @RequestMapping(\"/getProductInfo\") @ResponseBody public ProductInfo getProductInfo(Long productId) { ProductInfo productInfo = cacheService.getProductInfoOfRedisCache(productId); log.info(\"从 redis 中获取商品信息\"); if (productInfo == null) { productInfo = cacheService.getProductInfoFromLocalCache(productId); log.info(\"从 ehcache 中获取商品信息\"); } if (productInfo == null) { // 两级缓存中都获取不到数据，那么就需要从数据源重新拉取数据，重建缓存 // 假设这里从数据库中获取的数据 // String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + // \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; // productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); GetProductInfoOfMysqlCommand command = new GetProductInfoOfMysqlCommand(productId); productInfo = command.execute(); rebuildCache.put(productInfo); } return productInfo; } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"116.html":{"url":"116.html","title":"116. 为源头服务的限流场景增加 stubbed fallback 降级机制","keywords":"","body":" 116. 为源头服务的限流场景增加 stubbed fallback 降级机制 hbase 冷数据备份降级策略方案 缓存雪崩回顾 116. 为源头服务的限流场景增加 stubbed fallback 降级机制 我们上一讲讲到限流，计算了一下线程池的最大的大小，和这个等待队列，去限制了每秒钟最多能发送多少次请求到商品服务；避免大量的请求都发送到商品服务商去 限流过后，就会导致什么呢，比如 redis 集群崩溃了，雪崩，大量的请求涌入到商品服务调用的 command 中， 这个时候线程池肯定不够处理比预估并发量大的请求的，那么超出了处理能力的请求，就会被 reject 掉， 从而走 fallback 降级机制 这里再来理清楚一些前提： 首先请求能到这里来，那么 nginx 本地缓存肯定失效了、redis 已经崩溃了、ehcache 中没有这条数据的缓存。 只能从源头的商品服务里面去查询，但是被限流走了降级机制 所以本章重点就是降级机制怎么实现的问题。 这里根据自身的业务需求和场景来定，本章给出一个方案：多级降级策略 第一级：走 hbase 查询 第二级：走 stubbed 策略，也就是残缺策略 疑问就是本人没有做过电商相关系统，在本课程背景中的代码中，只能拿到一个商品 id， 而且本地缓存又没有商品数据了，怎么做残缺策略呢？ 由于之前讲过多级降级策略，这里也不示范了。而且还有 hbase，也不可能花很多时间来讲解。 这里介绍下 hbase 这种方案思路 hbase 冷数据备份降级策略方案 hbase： 基于 hdfs 分布式存储基础之上，封装了一个系统，叫做 hbase，分布式在线存储，分布式 NoSQL数据库，里面可以放大量的冷数据。 由于是分布式 NoSQL数据库，只要运维还过得去，比 mysql 并发强太多。 什么叫冷数据备份？可以是把一天前、一周前的商品数据版本做一个快照存放在 Hbase 中 缓存雪崩回顾 到这里基本上缓存雪崩已经讲解完了 事前 redis 高可用性，redis cluster、sentinal、复制、主从，从->主，双机房部署 事中 ehcache 可以抗一抗，redis 挂掉之后的资源隔离、超时控制、熔断，商品服务的访问限流、多级降级，缓存服务在雪崩场景下存活下来，基于 ehcache 和存活的商品服务提供数据 事后 快速恢复 Redis，备份+恢复，快速的缓存预热的方案 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"117.html":{"url":"117.html","title":"117. 高并发场景下的缓存穿透导致 MySQL 压力倍增问题以及其解决方案","keywords":"","body":"117. 高并发场景下的缓存穿透导致 MySQL 压力倍增问题以及其解决方案 什么是缓存穿透？如上图所表述， 简单来说：一个 key 在所有缓存中都不存在，并且在 mysql 中也不存在，叫做缓存穿透 根源就在于不存在的 key，考虑一个场景，如果大量不存在的 key 穿透到 mysql，恐怖的事情就来了，很有可能 mysql 被打死 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"118.html":{"url":"118.html","title":"118. 在缓存服务中开发缓存穿透的保护性机制","keywords":"","body":" 118. 在缓存服务中开发缓存穿透的保护性机制 缓存穿透解决方案 代码中解决 118. 在缓存服务中开发缓存穿透的保护性机制 缓存穿透解决方案 我们的缓存穿透解决方案，其实非常简单：每次从源服务（商品服务）查询到的数据为空，就说明这个数据根本就不存在，需要往 redis 和 ehcache 等缓存中写入一条空数据。 另外再配合缓存变更监听推送事件，能让缓存中的空商品信息及时的被变更 代码中解决 GetProductInfoOfMysqlCommand @Override protected ProductInfo run() throws Exception { // 假设 100 的 id 是数据库中不存在的 // 这里返回一个空的 // 这里只是模拟从 mysql 查询 if (productId == 100) { ProductInfo productInfo = new ProductInfo(); productInfo.setId(productId); return productInfo; } String productInfoJSON = \"{\\\"id\\\": 1, \\\"name\\\": \\\"iphone7手机\\\", \\\"price\\\": 5599, \\\"pictureList\\\":\\\"a.jpg,b.jpg\\\", \\\"specification\\\": \\\"iphone7的规格\\\", \\\"service\\\": \\\"iphone7的售后服务\\\", \\\"color\\\": \\\"红色,白色,黑色\\\", \\\"size\\\": \\\"5.5\\\", \\\"shopId\\\": 1,\" + \"\\\"modifyTime\\\":\\\"2019-05-13 22:00:00\\\"}\"; ProductInfo productInfo = JSONObject.parseObject(productInfoJSON, ProductInfo.class); return productInfo; } 经过尝试，访问正常，无任何报错；在本用例中其他的地方没有依赖获取到的商品进行计算什么的，所以这种缓存穿透基本上外面雾感知，只是在页面上展示时全是 null 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"119.html":{"url":"119.html","title":"119. 高并发场景下的 nginx 缓存失效导致 redis 压力倍增问题以及解决方案","keywords":"","body":" 119. 高并发场景下的 nginx 缓存失效导致 redis 压力倍增问题以及解决方案 什么是缓存失效? 缓存失效的问题 缓存失效解决方案 119. 高并发场景下的 nginx 缓存失效导致 redis 压力倍增问题以及解决方案 什么是缓存失效? 我们在 nginx 中设置本地缓存时，给了一个过期时间，比如是 10 分钟， 10 分钟后会自动过期，这个就叫做缓存失效 缓存失效的问题 比如同时来了 1000 个请求，10 分钟后会失效（同时来，也是同时失效）， 这就会导致大量的请求高并发到 redis 上去了，同时网络负载也会加重 缓存失效解决方案 解决的核心思路就是：让所有缓存的过期时间尽量保证不在同一时间失效，可以使用一个过期区间， 在这个区间内随机过期时间 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"120.html":{"url":"120.html","title":"120. 在 nginx lua 脚本中开发缓存失效的保护性机制","keywords":"","body":"120. 在 nginx lua 脚本中开发缓存失效的保护性机制 -- 获取到之后，再设置到缓存中 math.randomseed(tostring(os.time()):reverse():sub(1, 7)) local expireTime = math.random(600, 1200) cache_ngx:set(productCacheKey, productCache, expireTime) 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"121.html":{"url":"121.html","title":"121. 支撑高并发与高可用的大型电商详情页系统的缓存架构课程总结","keywords":"","body":"121. 支撑高并发与高可用的大型电商详情页系统的缓存架构课程总结 亿级流量电商网站的商品详情页系统架构 面临难题：对于每天上亿流量，拥有上亿页面的大型电商网站来说，能够支撑高并发访问， 同时能够秒级让最新模板生效的商品详情页系统的架构是如何设计的？ 解决方案：异步多级缓存架构 + nginx 本地化缓存 + 动态模板渲染的架构 redis 企业级集群架构 面临难题：如何让 redis 集群支撑几十万 QPS 高并发 + 99.99% 高可用 + TB 级海量数据 + 企业级数据备份与恢复？ 解决方案：redis 的企业级备份恢复方案 + 复制架构 + 读写分离 + 哨兵架构 + redis cluster 集群部署 多级缓存架构设计 面临难题：如何将缓存架构设计的能够支撑高性能以及高并发到极致？同时还要给缓存架构最后的一个安全保护层？ 解决方案：nginx 抗热点数据 + redis 抗大规模离散请求 + ehcache 抗 redis 崩溃的三级缓存架构 数据库 + 缓存双写一致性解决方案 面临难题：高并发场景下，如何解决数据库与缓存双写的时候数据不一致的情况？ 解决方案：异步队列串行化的数据库 + 缓存双写一致性解决方案 缓存维度化拆分解决方案 面临难题：如何解决大 value 缓存的全量更新效率低下问题？ 解决方案：商品缓存数据的维度化拆分解决方案 缓存命中率提升解决方案 面临难题：如何将缓存命中率提升到极致？ 解决方案：双层 nginx 部署架构 + lua 脚本实现一致性 hash 流量分发策略 缓存并发重建冲突解决方案 面临难题：如何解决高并发场景下，缓存重建时的分布式并发重建的冲突问题？ 解决方案：基于 zookeeper 分布式锁的缓存并发重建冲突解决方案 缓存预热解决方案 面临难题：如何解决高并发场景下，缓存冷启动导致 MySQL 负载过高，甚至瞬间被打死的问题？ 解决方案：基于 storm 实时统计热数据的分布式快速缓存预热解决方案 热点缓存自动降级方案 面临难题：如何解决热点缓存导致单机器负载瞬间超高？ 解决方案：基于 storm 的实时热点发现+毫秒级的实时热点缓存负载均衡降级 高可用分布式系统架构设计 面临难题：如何解决分布式系统中的服务高可用问题？避免多层服务依赖因为少量故障导致系统崩溃？ 解决方案：基于 hystrix 的高可用缓存服务，资源隔离 + 限流 + 降级 + 熔断 + 超时控制 复杂的高可用分布式系统架构设计 面临难题：如何针对复杂的分布式系统将其中的服务设计为高可用架构？ 解决方案：基于 hystrix 的容错 + 多级降级 + 手动降级 + 生产环境参数优化经验 + 可视化运维与监控 缓存雪崩解决方案 面临难题：如何解决恐怖的缓存雪崩问题？避免给公司带来巨大的经济损失？ 解决方案：全网独家的事前 + 事中 + 事后三层次完美缓存雪崩解决方案 缓存穿透解决方案 面临难题：如何解决高并发场景下的缓存穿透问题？避免给 MySQL 带来过大的压力？ 解决方案：缓存穿透解决方案 缓存失效解决方案 面临难题：如何解决高并发场景下的缓存失效问题？避免给 redis 集群带来过大的压力？ 解决方案：基于随机过期时间的缓存失效解决方案 硬件规划：这里以每日上亿流量，高峰 QPS 过 1万 来大概估计下 nginx 部署，因为抗了大量（百分之八九十）请求，所以负载很重，使用 16 核 32G，建议给 3~5 台以上就非常充裕了，每台抗个几千 QPS 缓存服务部署，4 核 8G，按照每台 QPS 支撑 500，部署个 10~20 台 redis 部署，每台给 8 核 16G（原因是 redis 快照机制不能给太多内存），根据数据量以及并发读写能力来看，部署 5~10 个 master，每个 master 挂一个 slave，主要是为了支撑更多数据量，1万 并发读写肯定没问题了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"122.html":{"url":"122.html","title":"122. 如何将课程中的东西学以致用在自己目前的项目中去应用？","keywords":"","body":"122. 如何将课程中的东西学以致用在自己目前的项目中去应用？ 到目前为止大家肯定学到了很多东西、技术、解决方案、架构， 那么肯定会问：我怎么用？ 这里提供一个思路：首先必须得对课程里的东西彻底吸收了，然后再看看自己的项目， 有哪些是可以用到其中一些技术的。 比如本人：由于是后台营销管理系统，而且本人只负责其中的后台管理系统， 所以场景上有限，目前用了 ehcache 在一些对数据实时性不敏感的调用上进行缓存， 用了 hystrix 对整个服务调用， spring boot 默认配置的 hystrix 是以服务名为一个线程池，各个调用为一个 command， 我用 hystrix 在原有超时基础上进行包装一层，能看到各个 command 的调用情况， 但是默认的仪表盘由于只是展示最近 10 秒的情况，没有历史曲线等功能， 这个时候针对我这里的需求，其实就可以自己接收 hystrix-stream 进行汇聚数据， 进行历史曲线的展示（虽然目前自己还做不到，因为涉及到大量的数据，不知道怎么计算和存储） 通过以上本人对目前自己项目的思考，你会发现，在学习后你虽然没有一个非常靠近的环境， 来使用课程中所有的东西，但是能用到其中的一部分，拿来改造你的项目 这个也是最重要的：这套课程学完以后，你对自己的系统多一些思考，多一些架构上的思考和设计 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"123.html":{"url":"123.html","title":"123. 如何带着课程中讲解的东西化为自己的技术并找一份更好的工作？","keywords":"","body":"123. 如何带着课程中讲解的东西化为自己的技术并找一份更好的工作？ ::: tip 以下内容，完全 coyp 课程原笔记； 简单总结一下内容：找工作不要拿你没有做过的项目去，要套在自己目前的业务系统上去； 只有这样才不会被戳穿； 因为任何课程都不可能用几十个小时讲解一个真实的项目实现；里面细节性的东西很容易被问懵逼； ::: 我做线上培训课程很长时间了，做java架构的课，没有出过 主要是出一些大数据spark的课程，其中有一个课程是一个纯企业级的大型的spark的大数据项目的课程 很多人学习了以后，有个问题 直接原封不动，直接就拿着课程里学到的东西，出去找工作，坑爹，很多人的简历，写自己做了个什么大数据的项目，几乎是一模一样 必死 你没做过电商的项目，结果你拿着这个电商项目出去，你就完蛋了 基本上，在行业里，一些项目实战类的课程，100%，都是用简化后的业务，来讲解各种技术和架构，方案 绝对不可能说一个几十个小时的课程，可以给你讲完一个大型项目所有的业务，带你做出来一个一模一样的项目 大项目，10+，做至少半年，5 8 = 40 4 = 160 ， 1000个小时，1w个小时 几十个小时，能给你讲清楚技术、架构和方案是足够的，但是也仅此而已了 任何项目实战类的课程，项目，简化业务，拿业务背景出来串起来所有的技术和知识，架构，让你在具体的业务背景中去学习 而不是简简单单的干学技术，写了一堆demo代码，没有这个技术在项目中如何运用的sense 你拿着一个项目实战的课程，原封不动出去找工作，你肯定完蛋了 你根本不了解这里面的业务，比如这个电商，你了解这个电商的业务吗？市面上任何一个课程讲解了吗？超级简化和简单的一个小电商？订单管理+购物车+首页 技巧 你肯定是要将项目课程中的技术嚼烂了，吞下去，变成自己的东西，然后呢？？？ 跟自己的项目结合起来 项目实战课程，spark电商用户行为分析，很多人没有做过电商业务，银行信用卡系统，电信计费系统，网络运营系统，游戏运营系统 将一个课程中的技术全部提取出来，融合进你的项目的业务背景中 银行信用卡用户行为分析系统 电信用户短信通话行为分析系统 网络流量分析系统 游戏用户行为分析系统 技术提取出来，融入你自己的业务中去，业务你很清楚，门儿清，技术你也学到了 缓存架构 电商的缓存架构，商品详情页 银行信用卡系统的缓存架构，多级缓存，各种方案 电信计费系统的缓存架构 网络运营系统的缓存架构 游戏运营系统的缓存架构 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"124.html":{"url":"124.html","title":"124. 大型电商网站的商品详情页的深入分析","keywords":"","body":" 124. 大型电商网站的商品详情页的深入分析（第二版开始） 说明 商品详情页介绍 商品详情页的多模板化 商品详情页结构 时效性比较低的数据 时效性比较高的数据 亿级流量电商网站的商品详情页访问情况和特点 124. 大型电商网站的商品详情页的深入分析（第二版开始） 说明 之前，在讲解这个商品详情页系统的架构，着重点是：缓存架构、高可用服务 相关思路与技术深入讲解 商品详情页系统，我们只是抽取了其中一部分来讲解，而且还做了很大程度的简化， 主要是为了用一个较为拟真的这么一个业务场景，重点是要讲解：缓存架构、高可用服务（hystrix） 在讲解完了之前的内容之后，相信大家也都掌握了一定的基础了，然后接下来我们就要去动手纯实战， 去开发出来一个较为完整的亿级流量大型电商网站的商品详情页系统 升级内容（也就是第二版）： 纯实战：我们不会过多的讲解一些技术（redis、缓存架构、hystrix 高可用服务），在第一章中对 redis 和 hystrix 都是对技术的深入讲解 技术讲解过多了，对于我们这样的一套单品课程来说，自然在第一版的时候，业务和完整架构自然讲解就少了 这次升级，策略跟第一版不同，重点就是完整架构的项目实战，亿级流量电商详情页系统架构的完整架构的项目实战 纯实战，做东西，不会深入讲解任何技术 对于中间件依赖部署，简单的讲解，开始做 商品详情页介绍 商品详情页的多模板化 多套模板：聚划算、天猫超时、淘抢购、电器城 不同模板的元数据大部分是一样的，只是展示方式不一样， 不同的业务，商品学详情页的个性化需求很多，数据来源也很多 商品详情页结构 拿一个淘宝详情页来举例 时效性比较低的数据 一个详情页包含了不同的维度（也就是各种各样的数据） 商品维度：标题、图片、属性 等等 主商品维度：商品介绍、规格参数 等等 对于没有做过电商的我来说，还是不理解，后来工作中一个业务需求才让我明白：颜色与尺码的笛卡尔积组成了不同的 sku， 一般来看商品介绍、规则参数等都是公用的，而颜色与尺码又组成了一个 sku 概念，但是他们没有独立的商品介绍 分类维度：分类列表 商家维度 针对这些数据，采用的方案（后面会详细讲解）是当一个商品详情页被访问到的时候， 将数据动态渲染/填充到一个 html 模板中去，因为在浏览器展现的时候，数据写死在 html 中的， 直接就显示出来了。 比如一个商品的数据变更了，可能是异步的去更新数据的，可能需要 5 分钟，或者 10 分钟，才能将变更的数据反映的商品详情页中去； 这种形式在前端中来说就是 SSR（服务端渲染） 时效性比较高的数据 实时价格、实时促销、广告词、配送至 xxx、预售、库存 等数据，变更频繁，实时要求较高 针对于这类数据通过 ajax 异步加载，在访问商品详情页的时候， 价格、库存、促销活动、广告词，都没有直接写死到 html 中，直接是在 html 里放了一个 js 脚本， html 在浏览器显示出来的时候，js 脚本运行发送 ajax 请求到后端，后端接口直接查询相关数据返回。 只要变更了数据，那么在下一次商品详情页展示的时候，一定可以将最新的数据展示出来 在淘宝网上展示一个通用商品模板，商品详情页结构拆解说明，分析一个商品详情页的多维度构成 亿级流量电商网站的商品详情页访问情况和特点 访问量：比如双 11 活动，商品详情页的 pv 至少达到几亿次，但是经过良好设计的详情页系统，响应时间小于几十毫秒 访问特点：离散访问，特点数据少 一般来说访问比较均匀，很少说集中式访问某个商品详情页，除非是那种秒杀活动， 那是集中式访问某个商品详情页 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"125.html":{"url":"125.html","title":"125. 大型电商网站的商品详情页系统架构是如何一步一步演进的","keywords":"","body":" 125. 大型电商网站的商品详情页系统架构是如何一步一步演进的 第一个版本 架构设计 架构缺陷 第二个版本 架构设计 架构缺陷 架构优化 架构优化后的缺陷 第三个版本 需要支持的需求 系统架构设计 如何解决的问题 小结 125. 大型电商网站的商品详情页系统架构是如何一步一步演进的 商品详情页系统架构演进历程，这里分三个版本来讲解 为什么要演进？这个只有经历过才会深刻明白，只要想到成本和时间就明白了， 在一定的时间和成本下，一般都是从第一个版本开始的，后续根据业务发展， 用户越来越多，业务越来越复杂，支撑不了的时候，就需要开始改版了 第一个版本 架构设计 J2EE + Tomcat + MySQL 动态页面，每次请求都要调用多个依赖服务的接口，从数据库里查询数据， 然后通过类似 JSP 的技术渲染到 HTML 模板中，返回最终 HTML 页面 架构缺陷 每次请求都是要访问数据库的，性能肯定很差 每次请求都要调用大量的依赖服务，依赖服务不稳定导致商品详情页展示的性能经常抖动 第二个版本 架构设计 页面静态化技术 通过 MQ 得到商品详情页涉及到的数据的变更消息 通过 Java Worker 服务全量调用所有的依赖服务的接口 查询数据库，获取到构成一个商品详情页的完整数据，并通过 velocity 等模板技术生成静态 HTML 将静态 HTML 页面通过 rsync 工具直接推送到多台 nginx 服务器上， 每台 nginx 服务器上都有全量的HTML静态页面，nginx 对商品详情页的访问请求直接返回本地的静态 HTML 页面 架构缺陷 全量更新问题 如果某一个商品分类、商家等信息变更了 那么那个分类、店铺、商家下面所有的商品详情页都需要重新生成静态 HTML 页面 更新速度过慢问题 分类、店铺、商家、商品越来越多，重新生成 HTML 的负载越来越高，rsync 全量同步所有 nginx 的负载也越来越高 从数据变更到生成静态 HTML，再到全量同步到所有 nginx，时间越来越慢 扩容问题 因为每个商品详情页都要全量同步到所有的 nginx 上，导致系统无法扩容，无法增加系统容量 架构优化 解决全量更新问题（维度化） 每次 Java Worker 收到某个维度的变更消息，不是拉去全量维度并生成完整 HTML，而是按照维度拆分， 生成一个变化维度的 HTML 片段 nginx 对多个 HTML 片段通过 SSI 合并 html 片段然后输出一个完整的 html 解决扩容问题（集群化，每台机器负责其中一部分） 每个商品详情页不是全量同步到所有的 nginx 而是根据商品 id 路由到某一台 nginx 上，同时接入层 nginx 按照相同的逻辑路由请求 解决更新速度过慢问题（增加更多机器资源） 多机房部署，每个机房部署一套 Java Worker + 应用 Nginx， 所有机房用一套负载均衡设备，在每个机房内部完成全流程，不跨机房 架构优化后的缺陷 更新速度还是不够快的问题 商品的每个维度都有一个 HTML 片段，rsync 推送大量的 HTML 片段，负载太高，性能较差 Nginx 基于机械硬盘进行 SSI 合并，性能太差 还是存在全量更新的问题 虽然解决了分类、商家、店铺维度的变更，只要增量重新生产较小的 HTML 片段即可， 不用全量重新生成关联的所有商品详情页的 HTML 但是如果某个页面模板变更，或者新加入一个页面模板，还是会导致几亿个商品的 HTML 片段都要重新生成和 rsync， 要几天时间才能完成，无法响应需求 还是存在容量问题 nginx 存储有限，不能无限存储几亿，以及增长的商品详情页的 HTML 文件 如果 nginx 存储达到极限，需要删除部分商品详情页的 HTML 文件，改成 nginx 找不到 HTML， 则调用后端接口，回到动态页面的架构 动态页面架构在高并发访问的情况下，会对依赖系统造成过大的压力，几乎扛不住 第三个版本 需要支持的需求 迅速响应各种页面模板的改版和个性化需求的新模板的加入 页面模块化，页面中的某个区域变化，只要更新这个区域中的数据即可 支持高性能访问 支持水平扩容的伸缩性架构 系统架构设计 依赖服务有数据变更发送消息到 MQ 监听数据变更事件，写入缓存 数据异构 Worker 服务监听 MQ 中的变更消息，调用依赖服务的接口，仅仅拉取有变更的数据即可，然后将数据存储到 redis 中 数据异构 Worker 存储到 redis 中都是原子未加工数据，包括商品基本信息、商品扩展属性、 商品其他信息、商品规格参数、商品分类、商家信息 数据异构 Worker 发送消息到 MQ，数据聚合 Worker 监听到 MQ 消息 数据聚合 Worker 将原子数据从 redis 中取出，按照维度聚合后存储到 redis 中，包括三个维度 基本信息维度：基本信息、扩展属性 - 商品介绍：PC 版、移动版 - 其他信息：商品分类、商家信息 nginx+lua：lua 从 redis 读取商品各个维度的数据，通过 nginx 动态渲染到 html 模板中，然后输出最终的 html 简单说最大的变化就是：使用上了缓存、html 是通过模板动态渲染的 如何解决的问题 更新问题：不再是生成和推送 html 片段了，不再需要合成 html，直接数据更新到 redis，然后走动态渲染，性能大大提升 全量更新问题：数据和模板分离，数据更新呢就更新数据，模板更新直接推送模板到 nginx，不需要重新生成所有 html，直接走动态渲染 容量问题：不需要依赖 nginx 所在机器的磁盘空间存储大量的 html，将数据放 redis，html 就存放模板，大大减少空间占用，而且 redis 集群可扩容 小结 从三个版本架构的演进来看，其实就是由一个从简到繁，不停的打补丁，最后打补丁都无法解决了， 才明白问题是出在根源上：没有使用缓存！ 当第三个版本的时候，使用上了缓存，整个架构从根上就开始变化，后续的手段都是基于这个变化而变化的。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"126.html":{"url":"126.html","title":"126. 亿级流量大型电商网站的商品详情页系统架构的整体设计","keywords":"","body":"126. 亿级流量大型电商网站的商品详情页系统架构的整体设计 商品详情页整体架构组成（也就是上一章节的第三版） 动态渲染系统 将页面中静的数据，直接在变更的时候推送到缓存，然后每次请求页面动态渲染新数据 商品详情页系统（负责静的部分）：被动接收数据，存储 redis，nginx+lua 动态渲染 商品详情页动态服务系统（对外提供数据接口） 提供各种数据接口 动态调用依赖服务的接口，产生数据并且返回响应 从商品详情页系统处理出来的 redis 中，获取数据，并返回响应 OneService 系统 动的部分，都是走 ajax 异步请求的，不是走动态渲染的， 商品详情页统一服务系统（负责动的部分） 前端页面 静的部分，直接被动态渲染系统渲染进去了 动的部分，html 一到浏览器，直接走 js 脚本，ajax 异步加载 商品详情页，分段存储，ajax 异步分屏加载 工程运维：限流、压测、灰度发布 从上图和描述来看，暂时笔者还不知道为什么有动态渲染系统 和 OneService 系统， 他们在这里表现出来只是少了一个 mq，但是在高峰期要么失败，要么降级， 与 mq 排队更新，貌似是差不多的 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/127.html":{"url":"dr/127.html","title":"127. 商品详情页动态渲染系统：架构整体设计","keywords":"","body":" 127. 商品详情页动态渲染系统：架构整体设计 动态渲染系统架构设计 动态渲染系统思想 数据闭环 数据维度化 系统拆分 异步化 动态化 多机房多活 小结 127. 商品详情页动态渲染系统：架构整体设计 动态渲染系统架构设计 依赖服务（源） -> MQ -> 动态渲染服务 -> 多级缓存 负载均衡 -> 分发层nginx -> 应用层nginx -> 多级缓存 多级缓存 -> 数据直连服务 如上图：分为了三块： 负载均衡 + 多机房模板渲染 + 1、2 级缓存 当自己缓存失效时，请求数据直连服务 数据直连服务 + 三级缓存 找自己本地缓存，不存在时从主集群获取信息， 如果主集群也没有，则直接从源服务获取信息，并重建主集群缓存， 返回应用层 nginx 动态渲染服务 + 主集群 通过 mq 接收源服务的各种事件，把更改变更到主集群中去， 而主集群与从集群通过什么手段进行同步过去。 动态渲染系统思想 数据闭环 数据闭环架构 依赖服务：商品基本信息，规格参数，商家/店铺，热力图，商品介绍，商品维度，品牌，分类，其他 依赖服务发送数据变更消息到 MQ 数据异构 Worker 集群，监听 MQ，将原子数据存储到 redis，发送消息到 MQ 数据聚合 Worker 集群，监听 MQ，将原子数据按维度聚合后存储到 redis，三个维度（商品基本信息、商品介绍、其他信息） 数据闭环，就是数据的自我管理，所有数据原样同步后，根据自己的逻辑进行后续的数据加工，走系统流程，以及展示 数据形成闭环之后，依赖服务的抖动或者维护，不会影响到整个商品详情页系统的运行 数据闭环的流程： 数据异构（多种异构数据源拉取） 数据原子化 数据聚合（按照维度将原子数据进行聚合） 数据存储（Redis） 数据维度化 数据维度分为以下几大类： 商品基本信息：标题、扩展属性、特殊属性、图片、颜色尺码、规格参数 商品介绍 非商品维度其他信息：分类、商家、店铺、品牌 商品维度其他信息：采用 ajax 异步加载，如：价格、促销、配送至、广告、推荐、最佳组合，等等 存储策略： ssdb: 这种基于磁盘的大容量/高性能的 kv 存储，保存商品维度、主商品维度、商品维度其他信息，数据量大，不能光靠内存去支撑 redis：纯内存的 kv 存储，保存少量的数据，比如非商品维度的其他数据：商家数据、分类数据、品牌数据 一个完整的数据，拆分成多个维度，每个维度独立存储，就避免了一个维度的数据变更就要全量更新所有数据的问题 不同维度的数据，因为数据量的不一样，可以采取不同的存储策略 系统拆分 系统拆分更加细：依赖服务、MQ、数据异构 Worker、数据同步 Worker、Redis、Nginx+Lua， 每个部分的工作专注，影响少，适合团队多人协作 异构 Worker 的原子数据，基于原子数据提供的服务更加灵活 聚合 Worker 将数据聚合后，减少 redis 读取次数，提升性能 前端展示分离为商品详情页前端展示系统和商品介绍前端展示系统，不同特点，分离部署，不同逻辑，互相不影响 异步化 异步化，提升并发能力，流量削峰 消息异步化，让各个系统解耦合 如果使用依赖服务调用商品详情页系统接口同步推送，那么就是耦合的 缓存数据更新异步化 数据异构 Worker 同步调用依赖服务接口，但是异步更新 redis 动态化 数据获取动态化 nginx+lua 获取商品详情页数据的时候，按照维度获取，比如商品基本数据、其他数据（分类、商家） 模板渲染实时化 支持模板页面随时变化，因为采用的是每次从 nginx+redis+ehcache 缓存获取数据，渲染到模板的方式， 因此模板变更不用重新静态化 HTML 重启应用秒级化：nginx+lua 架构，重启在秒级 需求上线快速化：使用 nginx+lua 架构开发商品详情页的业务逻辑，非常快速 多机房多活 Worker 无状态 同时部署在各自的机房时采取不同机房的配置，来读取各自机房内部部署的数据集群（redis、mysql等） 将数据异构 Worker 和数据聚合Worker设计为无状态化，可以任意水平扩展， Worker 无状态化，但是配置文件有状态，不同的机房有一套自己的配置文件，只读取自己机房的 redis、ssdb、mysql 等数据 每个机房配置全链路接入 nginx 商品详情页 nginx + 商品基本信息 redis 集群 + 其他信息 redis 集群、商品介绍 nginx + 商品介绍 redis 集群 部署统一的 CDN 以及 LVS+KeepAlived 负载均衡设备 小结 本章还是很吃力的，光看上层架构，是看懂了，但是一想具体的业务场景，就懵逼了。期待后续的课程 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/128.html":{"url":"dr/128.html","title":"128. 商品详情页动态渲染系统：大型网站的多机房 4级 缓存架构设计","keywords":"","body":" 128. 商品详情页动态渲染系统：大型网站的多机房 4级 缓存架构设计 本地缓存 4 级缓存的作用是什么？ 128. 商品详情页动态渲染系统：大型网站的多机房 4级 缓存架构设计 多机房缓存，主从缓存集群通过同步，各个机房读取自己机房的从集群，防止跨集群访问集群 本地缓存 使用 nginx shared dict 作为 local cache，http-lua-module 的 shared dict 可以作为缓存， 而且 reload nginx 不会丢失 也可以使用 nginx proxy cache 做 local cache 双层 nginx 部署，一层接入，一层应用，接入层用 hash 路由策略提升缓存命中率，比如 库存缓存数据的 TP99 为 5s，本地缓存命中率 25%，redis 命中率 28%，回源命中率 47% （回源就是数据直连服务 + 主集群） 一次普通秒杀活动的命中率，本地缓存 55%，分布式 redis 命中率 15%，回源命中率 27% 使用 hash 路由策略，最高可以提升命中率达到 10%；（分发层把同一个商品 ID 分发到同一个应用层 nginx 上） 全缓存链路维度化存储，如果有 3个 维度的数据，只有其中 1个 过期了，那么只要获取那 1个 过期的数据即可， nginx local cache 的过期时间一般设置为 30min，到后端的流量会减少至少 3 倍 4 级缓存的作用是什么？ nginx 本地缓存：抗热点数据，小内存缓存访问最频繁的数据 各个机房的 redis 集群： 抗大量离散数据，采用一致性 hash 策略构建分布式 redis 缓存集群 tomcat 中的动态服务本地 jvm 堆缓存 支持在一个请求中多次读取一个数据，或者与该数据相关的数据 作为 redis 崩溃的备用防线 固定缓存一些访问频繁更改较少的数据，比如分类、品牌等数据 对缓存过期时间为 redis 过期时间的一半 主 redis 集群 命中率非常低，小于 5%； （为什么?应该是跟过期时间有关系把？前 2级 缓存都没有到这里一般也不会有？） 防止主从同步延迟导致的数据读取 miss；从集群还未同步到，会通过数据直连服务请求主集群 防止各个机房的从 redis 集群崩溃之后，全量走依赖服务会导致雪崩，主 redis 集群是后背防线 主 redis 集群：采取多机房一主三从的高可用部署架构 如上图：redis 集群部署采取双机房一主三活的架构： 机房 A ：部署主集群 + 一个从集群， 机房 B ：部署一个从集群（从机房 A 主集群）+ 一个从集群（从机房 B 从集群） 双机房一主三活的架构，保证了机房 A 彻底故障的时候，机房 B 还有一套备用的集群，可以升级为一主一从 如果采取机房 A 部署一主一从，机房 B 一从，那么机房 A 故障时，机房 B 的一从承载所有读写压力，压力过大，很难承受 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/129.html":{"url":"dr/129.html","title":"129. 商品详情页动态渲染系统：复杂的消息队列架构设计","keywords":"","body":" 129. 商品详情页动态渲染系统：复杂的消息队列架构设计 几种队列解释 多队列架构总结 129. 商品详情页动态渲染系统：复杂的消息队列架构设计 消息队列很多时候并不是简单的使用一个 MQ 就可以的，为了应对复杂的场景，需要设计一套复杂的队列 几种队列解释 本列有以下几种队列： 任务等待队列 任务排重队列（异构 Worker 对一个时间段内的变更消息做排重） 失败任务队列（失败重试机制） 优先级队列，刷数据队列（依赖服务洗数据）、高优先级队列（活动商品优先级高） 这些队列服务于某一种业务场景而存在的，如下图，已经很复杂了 依赖服务 数据变更信息投递到 消息等待队列 中 针对活动商品需要快速响应，投递到 高优先级队列 中 数据同步服务 消费 消息等待队列 中的事件，把原子数据写入到缓存集群中 消费 高优先级队列 同时可以对短时间内多次修改的商品去重，然后投递到 去重队列 这里猜想应该是设置一个阀值，比如 5 分钟往去重队列投递一次 需要写缓存，可能因为网络问题导致写入失败，投递到 数据同步服务异常消息队列 自己又要消费 数据同步服务异常消息队列 中的数据（相当于重试） 还要保证脏数据的处理，比如一个商品修改三次，中间那一次写失败， 那么在处理中间这次重试的时候，还要保证这条脏数据不被写入缓存中 数据聚合服务 消费 去重队列 进行数据的聚合，然后写入 缓存集群 中 消费 自己的高优先级队列 消费 数据聚合服务异常消息队列 刷数据队列 用来解决全量数据更新，这个队列只在晚上 12 点之后开始消费。 多队列架构总结 看图很复杂，简单一点： 数据同步服务 通过 消息等待队列 与源服务交互 数据聚合服务 通过 去重队列 与数据同步服务进行交互 三个服务之间的交互，每个服务之间的交互都有以下队列： 正常交互队列 重试队列 高优先级队列 另外的队列就是全量刷数据。 总结结束；大概意思本人明白了，其中一些细节点还不知道，期待后续 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/130.html":{"url":"dr/130.html","title":"130. 商品详情页动态渲染系统：使用多线程并发提升系统吞吐量的设计","keywords":"","body":"130. 商品详情页动态渲染系统：使用多线程并发提升系统吞吐量的设计 数据同步服务做并发化+合并 将多个变更消息合并在一起，调用依赖服务一次接口获取多个数据，采用多线程并发调用 这里不明白的是，收到变更事件之后，需要去拉取数据？而不是把变更的数据随着事件传递吗？ 数据聚合服务做并发化 每次重新聚合数据的时候，对多个原子数据用多线程并发从 redis 查询 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/131.html":{"url":"dr/131.html","title":"131. 商品详情页动态渲染系统：redis 批量查询性能优化设计","keywords":"","body":"131. 商品详情页动态渲染系统：redis 批量查询性能优化设计 由于是 redis 集群，所以在存储维度数据的时候（商品价格、描述等维度）， 可能会被自动路由到多个 redis 节点上，可以使用 hash tag 功能强制路由到一个节点上， 再配合 mget 语法批量获取到 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/132.html":{"url":"dr/132.html","title":"132. 商品详情页动态渲染系统：全链路高可用架构设计","keywords":"","body":"132. 商品详情页动态渲染系统：全链路高可用架构设计 动态渲染系统在 读链路 高可用方面怎么考虑， 由于采用了多级缓存，每级缓存都会通过网络访问，就有可能导致读异常（也有可能服务挂掉） 如上图： nginx local cache -> 缓存从集群 -> 数据直连服务：缓存主集群 -> 数据直连服务: 依赖服务 读链路多级降级：本机房从集群 -> 主集群 -> 直连 缓存从集群故障 简单断路功能，降级访问数据直连服务 数据直连服务故障 断路功能，直接访问主集群 主集群故障 断路功能，直接调用依赖服务 貌似会觉得这个就是多次缓存找不到的一层一层去重建的过程，实际上还是有区别的， 这三个故障的请求控制都在 nginx 上，而多级缓存失效重建的流程却是分散在各个节点上的 全链路隔离 基于 hystrix 的依赖调用隔离、限流、熔断、降级 普通服务的多机房容灾冗余部署以及隔离 限流等功能能保证每个节点服务不会被直接打死，虽然限流，至少系统还能处理部分请求， 不至于全盘崩溃来的好。 通过这一章的分析，在降级方面的应用，多了一点理解 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/133.html":{"url":"dr/133.html","title":"133. 商品详情页动态渲染系统：微服务架构设计","keywords":"","body":"133. 商品详情页动态渲染系统：微服务架构设计 该课程分为两大块： 真实的完整的亿级流量高并发高可用的电商详情页系统的架构实战 其中的各种服务使用微服务架构来做，相当于是真实业务场景下的微服务项目实战 微服务架构内容如下： 领域驱动设计 我们需要对这个系统设计到的领域模型进行分析，然后进行领域建模，最后设计出我们对应的微服务模型 spring cloud 微服务的基础架构使用 spring cloud 来做 持续交付流水线 jekins + git + 自动化持续集成 + 自动化测试 + 自动化部署 doker 大量微服务的部署与管理 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/134.html":{"url":"dr/134.html","title":"134. 商品详情页动态渲染系统：机房与机器的规划","keywords":"","body":"134. 商品详情页动态渲染系统：机房与机器的规划 规划看之前的架构图，这里只是一部分，不包含微服务的，微服务的在后续部署时再规划，里面包含了 spring cloud 的一套组件，要结合那个去规划 ::: tip 注意 以下机器规划非实际生产，由于本地电脑内存有限，尽量模拟真实的， 会把一些能放一起的合并起来。 但是并不表示在生产环境中也是这样规划，肯定要更多的机器 ::: 负载均衡：2 台机器 lvs + keepalived 双机搞可用 两个机房，一机房一台：共 2 台 分发层 nginx + 应用层 nginx + 缓存从集群 缓存主集群：放在上面两个机房上 在实际生成环境中，的确可能会在相同机房，但是肯定是多机器 缓存集群分片中间件：与缓存集群部署在一起 rabbitmq 和 mysql：1 台机器 总共需要 5 台虚拟机 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/135.html":{"url":"dr/135.html","title":"135. 商品详情页动态渲染系统：部署 CentOS 虚拟机集群","keywords":"","body":" 135. 商品详情页动态渲染系统：部署 CentOS 虚拟机集群 在虚拟机中安装 CentOS 虚拟机信息： 按照 host only 方式配置之后无法访问外网 135. 商品详情页动态渲染系统：部署 CentOS 虚拟机集群 再次把基础设施都合并下，讲师电脑是 24 G 运行电脑，而我才 16 G; 目前架构中用到的有：基础设施、spring cloud、docker、jenkins， 我就按 8 G 内存分 4 台服务器，一台 2 G内存（讲师是 4 台 4 G 机器）。 在虚拟机中安装 CentOS ::: tip 之前章节的 4 台虚拟机可以删除，不会再使用了 ::: 这里安装 2 台 2 G 内存的虚拟机 基本安装与配置请参考 virtualbox 网络篇- Host Only 配置方式 在 virtualbox 中使用 host only 方式比较方便，换一个网络环境也不影响， 所以这里还是使用之前的虚拟网卡 ip 来为虚拟机分配： 网关 192.168.99.1 机器 ip 段从：192.168.99.11 开始 安装内容如下（这里的内容其实就是上面参考资料里面的教程笔记）： 2 台 2 G 内存虚拟机 在每个 CentOS 中都安装 Java 和 Perl 配置 ssh 免密通信 虚拟机信息： eshop-detail01：192.168.99.11 eshop-detail02：192.168.99.12 用户密码统一使用：hadoop 由于本人开发中使用 1.8，所以 JDK 版本使用 jdk-8u202-linux-i586.rpm ，此版本是最后一个免费版本，目前下载需要登录官网后才能下载 按照 host only 方式配置之后无法访问外网 之前的虚拟机可以正常上网，新安装的虚拟机可以与宿主机连接，但是 [root@eshop-detail01 ~]# ping www.baidu.com ping: unknown host www.baidu.com [root@eshop-detail01 ~]# ping 8.8.8.8 PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=50 time=39.3 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=50 time=39.6 ms 无法通过域名连通外网，解决方案：检查 /etc/resolv.conf 文件， 查看是否有配置 nameserver，如果没有请按如下配置 nameserver 223.5.5.5 nameserver 223.6.6.6 nameserver 114.114.114.114 nameserver 8.8.8.8 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/136.html":{"url":"dr/136.html","title":"136. 商品详情页动态渲染系统：双机房部署接入层与应用层 Nginx+Lua","keywords":"","body":"136. 商品详情页动态渲染系统：双机房部署接入层与应用层 Nginx+Lua 会分两个目录来安装 OpenResty: distribution_nginx：分发层，使用端口 80 app_nginx：应用层，使用端口 8000 基本目录：/usr/servers/ 安装 openResty + lua 请参考这里，按照之前的安装方式，安装在这里的指定目录下，部署内容不包括工程化 nginx+lua 项目结构 过程中需要注意的是路径，比如下面这个 --prefix 的路径一定要是对应的 nginx 路径，否则就装成之前的了 ./configure --prefix=/usr/servers/distribution_nginx --with-http_realip_module --with-pcre --with-luajit --add-module=./bundle/ngx_cache_purge-2.3/ --add-module=./bundle/nginx_upstream_check_module-0.3.0/ -j2 验证的时候使用下面的，如 distribution_nginx 目录下的 nginx，才表示没有安装错目录 [root@eshop-detail01 nginx]# /usr/servers/distribution_nginx/nginx/sbin/nginx -t nginx: the configuration file /usr/servers/distribution_nginx/nginx/conf/nginx.conf syntax is ok nginx: configuration file /usr/servers/distribution_nginx/nginx/conf/nginx.conf test is successful 同样启动也是启动对应目录下的 /usr/servers/distribution_nginx/nginx/sbin/nginx 启动后访问：http://192.168.99.11/lua 表示搭建第一个成功 只搭建了一个目前，后续几个照样搭建。 在搭建 app_nginx 的时候，需要把 nginx.conf 中的端口号修改为 8000，还有 lua.conf 中的也修改为 8000，才能正常启动 安装完毕后，访问一下地址是可以访问到的 http://192.168.99.11/lua http://192.168.99.11:8000/lua http://192.168.99.12/lua http://192.168.99.12:8000/lua 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/137.html":{"url":"dr/137.html","title":"137. 商品详情页动态渲染系统：为什么是 twemproxy+redis 而不是 redis cluster？","keywords":"","body":" 137. 商品详情页动态渲染系统：为什么是 twemproxy+redis 而不是 redis cluster？ LVS 那块不讲解 redis cluster 的问题 twemproxy+redis 如何选择？ 137. 商品详情页动态渲染系统：为什么是 twemproxy+redis 而不是 redis cluster？ LVS 那块不讲解 LVS+KeepAlived 负载均衡 、MySQL+Atlas 分库分表，不讲解了，很鸡肋 原因：单课，聚焦，围绕一个主题去讲解，太发散了以后，什么都讲，没有围绕主题去讲解，意义不是太大 本课主题：商品详情页系统，亿级流量大电商，核心的东西 随着课程不断讲解，可能会有 10% 的出入，砍掉或者调整一些细枝末节，大的思路是 ok 的，不会改变的 redis cluster 的问题 之前讲解过 redis cluster，那么他的优缺点如下 不好做读写分离 读写请求全部落到主实例上的，如果要扩展写 QPS，或者是扩展读 QPS，都是需要扩展主实例的数量，从实例就是一个用做热备 + 高可用 不好跟 nginx+lua 直接整合 lua->redis 的 client api，但是不太支持 redis cluster，中间就要走一个中转的 java 服务 不好做树状集群结构 比如 redis 主集群一主三从双机房架构，redis cluster 不太好做成那种树状结构 优点就是方便 相当于是上下线节点、集群扩容、运维工作、高可用自动切换，比较方便 twemproxy+redis twemproxy+redis 做集群，redis 部署多个主实例，每个主实例可以挂载一些 redis 从实例，如果将不同的数据分片写入不同的 redis 主实例中，twemproxy 这么一个缓存集群的中间件 上线下线节点，有一些手工维护集群的成本 支持 redis 集群 + 读写分离 就是最基本的多个 redis 主实例，数据落到哪一个实例上是 twemproxy 这个中间件来决定的，java/nginx+lua 客户端，是连接 twemproxy 中间件的。每个 redis 主实例就挂载了多个 redis 从实例，高可用 -> 可以使用哨兵 redis cluster 读写都要落到主实例的限制，你自己可以决定写主，读从，等等 支持 redis cli 协议，可以直接跟 nginx+lua 整合 可以搭建树状集群结构 如何选择？ 就是看你的架构中是否能容忍 redis cluster 的前 3 个问题 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/138.html":{"url":"dr/138.html","title":"138. 商品详情页动态渲染系统：redis 复习以及 twemproxy 基础知识讲解","keywords":"","body":" 138. 商品详情页动态渲染系统：redis 复习以及 twemproxy 基础知识讲解 部署 redis twemproxy 部署 redis 复习 redis 部分配置 redis 主从 twemproxy 讲解 138. 商品详情页动态渲染系统：redis 复习以及 twemproxy 基础知识讲解 部署 redis 本次使用 redis-2.8.19.tar.gz，关于该版本过旧问题：实际在做类似这种 nginx+lua 生产环境的部署的时候，不一定用最新的版本就是最好，老版本一般比较稳定，nginx+lua 整合用老点的版本，会比较保险一些 cd /usr/local/redis-test tar -zxvf redis-2.8.19.tar.gz cd redis-2.8.19 make nohup /usr/local/redis-test/redis-2.8.19/src/redis-server /usr/local/redis-test/redis-2.8.19/redis.conf & # 查看 redis 进程 ps -aux | grep redis # 连接该实例 /usr/local/redis-test/redis-2.8.19/src/redis-cli -p 6379 # 简单测试是否正常存取值 set k1 v1 get k1 twemproxy 部署 不要使用自动安装，因为对版本有要求，自动安装的版本过低 yum install -y autoconf automake libtool # 由于对版本有要求，所以这里还是需要自己手动安装 yum remove -y autoconf --直接将 autoconf 和 automake、libtool 都删除掉了 安装到 /usr/local/twemproxy-test 目录下 cd /usr/local/twemproxy-test # 依赖安装 wget ftp://ftp.gnu.org/gnu/autoconf/autoconf-2.69.tar.gz tar -zxvf autoconf-2.69.tar.gz cd autoconf-2.69 ./configure --prefix=/usr make && make install wget http://ftp.gnu.org/gnu/automake/automake-1.14.tar.gz tar -zxvf automake-1.14.tar.gz cd automake-1.14 ./bootstrap.sh ./configure --prefix=/usr make && make install wget http://ftpmirror.gnu.org/libtool/libtool-2.4.2.tar.gz tar -zxvf libtool-2.4.2.tar.gz cd libtool-2.4.2 ./configure --prefix=/usr make && make install # twemproxy 安装 tar -zxvf twemproxy-0.4.0.tar.gz cd twemproxy-0.4.0 autoreconf -fvi ./configure && make # 修改配置文件 cd /usr/local/twemproxy-test/twemproxy-0.4.0/conf/ # 先备份一下原始配置文件 mv nutcracker.yml nutcracker.back.yml vi nutcracker.yml # 将一下内容写入新建的 nutcracker.yml 文件中，配置什么意思先不要管，后续会讲解 server1: listen: 127.0.0.1:1111 hash: fnv1a_64 distribution: ketama redis: true servers: - 127.0.0.1:6379:1 # 启动：-d 后台启动 -c 指定配置文件 /usr/local/twemproxy-test/twemproxy-0.4.0/src/nutcracker -d -c /usr/local/twemproxy-test/twemproxy-0.4.0/conf/nutcracker.yml # 查看启动进程 ps -aux | grep nutcracker # 使用 redis-cli 连接到 tewmproxy 中间件 /usr/local/redis-test/redis-2.8.19/src/redis-cli -p 1111 get k1 set k1 v2 get k1 redis 复习 这里的复习，本人未练习，之前讲过的 redis 部分配置 redis.conf port 6379 logfile \"\" maxmemory 100mb maxmemory-policy volatile-lru maxmemory-samples 10 redis 主从 # 两份文件分别将端口设置为 6379 和 6380 cp redis.conf redis_6379.conf cp redis.conf redis_6380.conf # 分别启动两个 redis 实例 nohup /usr/local/redis-2.8.19/src/redis-server /usr/local/redis-2.8.19/redis_6379.conf & nohup /usr/local/redis-2.8.19/src/redis-server /usr/local/redis-2.8.19/redis_6380.conf & ps -aux | grep redis # 通过从实例客户端挂载主从 /usr/local/redis-2.8.19/src/redis-cli -p 6380 slaveof 127.0.0.1 6379 info replication # 测试注册是否正常 /usr/local/redis-2.8.19/src/redis-cli -p 6379 set k2 v2 /usr/local/redis-2.8.19/src/redis-cli -p 6380 get k2 twemproxy 讲解 eshop-detail-test: listen: 127.0.0.1:1111 hash: fnv1a_64 distribution: ketama timeout: 1000 redis: true servers: - 127.0.0.1:6379:1 test-redis-01 - 127.0.0.1:6380:1 test-redis-02 eshop-detail-test：redis 集群的逻辑名称 listen：twemproxy 监听的端口号 hash：hash 散列算法 distribution：分片算法：有一致性 hash、取模等 timeout：与 redis 连接的超时时长 redis：true 为 redis，否则是 memcached servers：redis 实例列表 一定要加别名，否则默认使用 ip:port:weight 来计算分片，如果宕机后更换机器，那么分片就不一样了，因此加了别名后，可以确保分片一定是准确的 你的客户端，java/nginx+lua，连接 twemproxy 写数据的时候，twemproxy 负责将数据分片，写入不同的 redis 实例 如果某个 redis 机器宕机，需要自动从一致性 hash 环上摘掉，等恢复后自动上线，可以使用以下配置 auto_eject_hosts: true server_retry_timeout: 30000 server_failure_limit: 2 auto_eject_hosts：自动摘除故障节点 server_retry_timeout：每隔 30 秒判断故障节点是否正常，如果正常则放回一致性 hash 环 server_failure_limit：多少次无响应，就从一致性 hash 环中摘除 关于一致性 hash 前面也讲解过了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/139.html":{"url":"dr/139.html","title":"139. 商品详情页动态渲染系统：部署双机房一主三从架构的 redis 主集群","keywords":"","body":" 139. 商品详情页动态渲染系统：部署双机房一主三从架构的 redis 主集群 部署架构 开始搭建 验证主从是否生效 139. 商品详情页动态渲染系统：部署双机房一主三从架构的 redis 主集群 部署架构 使用一个 redis 客户端，多份配置文件的方式启动 8 台 redis 实例 192.168.99.11 （机房1） 192.168.99.12 （机房2） master：6401 master：6402 slave: 6401 slave: 6402 slave: 6403 slave：6404 slave: 6403 slave：6404 主从示意如下，一个 master 有一个从，从下面又挂一个从，组成了一个树形结构 192.168.99.11 master：6401 192.168.99.11 slave: 6403 ↑ 192.168.99.12 slave: 6401 ↑ 192.168.99.12 slave: 6403 ↑ 192.168.99.11 master：6402 192.168.99.11 slave: 6404 ↑ 192.168.99.12 slave: 6402 ↑ 192.168.99.12 slave: 6404 ↑ 看最上面的示意图。当机房 1 挂掉之后，由于机房 2 的 6401 是机房 1 的 6403 的从， 所以机房 2 的 6401 会升级为 master，这样一来，在机房 2 上还是存在一主一从的高可用 redis 开始搭建 首先要把之前部署的 redis-test 给 kill 掉。在 /usr/local/redis 中安装一个 reds [root@eshop-detail01 local]# ps -ef | grep redis root 12126 21476 0 06:29 pts/4 00:00:00 grep redis root 31096 21476 0 02:43 pts/4 00:00:20 /usr/local/redis-test/redis-2.8.19/src/redis-server *:6379 [root@eshop-detail01 local]# kill -9 31096 cd /usr/local/redis tar -zxvf redis-2.8.19.tar.gz cd redis-2.8.19 make # 复制 4 份配置文件，并修改配置文件中的 prot 值为文件名对应的端口号 cd /usr/local/redis/redis-2.8.19 cp redis.conf redis-6401.conf cp redis.conf redis-6402.conf cp redis.conf redis-6403.conf cp redis.conf redis-6404.conf # 以此启动 redis cd /usr/local/redis/redis-2.8.19 nohup src/redis-server redis-6401.conf & nohup src/redis-server redis-6402.conf & nohup src/redis-server redis-6403.conf & nohup src/redis-server redis-6404.conf & # 查看 redis 进程 [root@eshop-detail01 redis-2.8.19]# ps -ef | grep redis root 14987 21476 0 06:38 pts/4 00:00:00 src/redis-server *:6401 root 14992 21476 0 06:39 pts/4 00:00:00 src/redis-server *:6402 root 14999 21476 0 06:40 pts/4 00:00:00 src/redis-server *:6403 root 15002 21476 0 06:40 pts/4 00:00:00 src/redis-server *:6404 root 15006 21476 0 06:40 pts/4 00:00:00 grep redis 配置树形主从，下面是其中一台从节点的配置方式，按照上面讲解的树形结构配置自己的主节点 cd /usr/local/redis/redis-2.8.19 [root@eshop-detail01 redis-2.8.19]# src/redis-cli -p 6403 127.0.0.1:6403> slaveof 192.168.99.11 6401 OK 127.0.0.1:6403> info replication # Replication role:slave master_host:192.168.99.11 master_port:6401 master_link_status:up master_last_io_seconds_ago:1 master_sync_in_progress:0 slave_repl_offset:29 slave_priority:100 slave_read_only:1 connected_slaves:0 master_repl_offset:0 repl_backlog_active:0 repl_backlog_size:1048576 repl_backlog_first_byte_offset:0 repl_backlog_histlen:0 另外一台也如法炮制启动 4 个 redis 实例，另外一台机器上的 6401 信息 [root@eshop-detail02 redis-2.8.19]# src/redis-cli -p 6401 127.0.0.1:6401> info replication # Replication role:slave master_host:192.168.99.11 # 机房 1 的 6403 是 master master_port:6403 master_link_status:up master_last_io_seconds_ago:1 master_sync_in_progress:0 slave_repl_offset:85 slave_priority:100 slave_read_only:1 connected_slaves:1 # 自己下面又有一个从 节点 6403 slave0:ip=192.168.99.12,port=6403,state=online,offset=15,lag=0 master_repl_offset:15 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:14 验证主从是否生效 # 在 192.168.99.11 上操作 cd /usr/local/redis/redis-2.8.19 src/redis-cli -p 6401 set k1 v1 # 去第二台机器上查看是否能获取到值 set k1 v2 # 修改 k1 为 v2 ，再去验证 # 在 192.168.99.12 上操作 cd /usr/local/redis/redis-2.8.19 src/redis-cli -p 6401 get k1 # 观察是否能获取到值 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/140.html":{"url":"dr/140.html","title":"140. 商品详情页动态渲染系统：给每个机房部署一个 redis 从集群","keywords":"","body":"140. 商品详情页动态渲染系统：给每个机房部署一个 redis 从集群 之前的架构中是双机房，一个主集群，一个从集群。在架构链路上，从集群作为缓存中的第二级缓存， 这个在之前的架构中有说明 由于我们把机器压缩了，两个机房的主部署到了一台机器上。这一点不要搞混淆了。 接下来，再部署两个 redis 从，来看下现在 redis 实例的分布 ← 192.168.99.12 slave: 6405 并列作为 6401 的从 192.168.99.11 master：6401 ← 192.168.99.11 slave: 6405 192.168.99.11 slave: 6403 ↑ 192.168.99.12 slave: 6401 ↑ 192.168.99.12 slave: 6403 ↑ ← 192.168.99.12 slave: 6406 并列作为 6402 的从 192.168.99.11 master：6402 ← 192.168.99.11 slave: 6406 192.168.99.11 slave: 6404 ↑ 192.168.99.12 slave: 6402 ↑ 192.168.99.12 slave: 6404 ↑ 来看下挂载的主从信息 [root@eshop-detail01 redis-2.8.19]# src/redis-cli -p 6401 127.0.0.1:6401> info replication # Replication role:master connected_slaves:3 # 3 个从，和上面示意的一模一样 slave0:ip=192.168.99.11,port=6403,state=online,offset=3007,lag=0 # 树形从 slave1:ip=192.168.99.11,port=6405,state=online,offset=3007,lag=0 # 单节点从 slave2:ip=192.168.99.12,port=6405,state=online,offset=3007,lag=1 # 机房2单节点从 master_repl_offset:3007 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:3006 搭建成功；注意这里部署的 redis 通过主从复制只实现了读写分离，但是并没有实现大数据量的存储， 所以后面会使用中间件来把两个主集群结合起来，组成一个分布式 redis 主集群 ::: tip 疑问 现在一个 master 有 3 个从，当 master 挂掉之后，会变成什么样子？ ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/141.html":{"url":"dr/141.html","title":"141. 商品详情页动态渲染系统：为 redis 主集群部署 twemproxy 中间件","keywords":"","body":"141. 商品详情页动态渲染系统：为 redis 主集群部署 twemproxy 中间件 最近加班有好几天都没有继续学习了，这里先来回顾下之前的机器 redis 布局 ← 192.168.99.12 slave: 6405 并列作为 6401 的从(从集群) 192.168.99.11 master：6401 ← 192.168.99.11 slave: 6405 192.168.99.11 slave: 6403 ↑ 192.168.99.12 slave: 6401 ↑ 192.168.99.12 slave: 6403 ↑ ← 192.168.99.12 slave: 6406 并列作为 6402 的从(从集群) 192.168.99.11 master：6402 ← 192.168.99.11 slave: 6406 192.168.99.11 slave: 6404 ↑ 192.168.99.12 slave: 6402 ↑ 192.168.99.12 slave: 6404 ↑ 本次要为主集群安装 twemproxy 中间件，来路由分发到两个集群上去 主要安装方式在前面章节已经讲过了 # 依赖安装请参考之前的章节 mkdir /usr/local/twemproxy cd /usr/local/twemproxy # 安装 twemproxy # 先备份一下原始配置文件 cd /usr/local/twemproxy-0.4.0/conf/ mv nutcracker.yml nutcracker.back.yml vi nutcracker.yml # 配置以下参数 redis-master: # 配置一个逻辑名称 listen: 127.0.0.1:1111 hash: fnv1a_64 distribution: ketama redis: true servers: - 192.168.99.11:6401:1 redis01 # 指向两个主集群 - 192.168.99.11:6402:1 redis02 # 启动 cd /usr/local/twemproxy/twemproxy-0.4.0/ ./src/nutcracker -d -c conf/nutcracker.yml # 查看是否已经启动 ps -ef | grep nutcracker 测试路由是否成功 cd /usr/local/redis/redis-2.8.19/src/ [root@eshop-detail01 src]# ./redis-cli -p 1111 set hello hello set k1 v1 set p1 p1 set h1 h1 set product 01 set hello1 1 [root@eshop-detail01 src]# ./redis-cli -p 6401 127.0.0.1:6401> keys * 1) \"h1\" 2) \"k1\" 3) \"product\" 4) \"p1\" [root@eshop-detail01 src]# ./redis-cli -p 6402 127.0.0.1:6402> keys * 1) \"hello1\" 2) \"hello\" 目前可以看出来测试还是成功的，就是不太清楚路由是按什么逻辑路由的。 我这里和视频中的路由表现不太一致 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/142.html":{"url":"dr/142.html","title":"142. 商品详情页动态渲染系统：为每个机房的 redis 从集群部署 twemproxy 中间件","keywords":"","body":"142. 商品详情页动态渲染系统：为每个机房的 redis 从集群部署 twemproxy 中间件 ← 192.168.99.12 slave: 6405 并列作为 6401 的从(从集群) 192.168.99.11 master：6401 ← 192.168.99.11 slave: 6405 192.168.99.11 slave: 6403 ↑ 192.168.99.12 slave: 6401 ↑ 192.168.99.12 slave: 6403 ↑ ← 192.168.99.12 slave: 6406 并列作为 6402 的从(从集群) 192.168.99.11 master：6402 ← 192.168.99.11 slave: 6406 192.168.99.11 slave: 6404 ↑ 192.168.99.12 slave: 6402 ↑ 192.168.99.12 slave: 6404 ↑ 这里说的从集群和上图理解的还不太一样，说实话这里讲课已经有点懵逼了， 现在已经不太确定上图示意的是否准确。 但是本节课内容讲解的从集群应该是每个主集群下的两个从 redis，如 [root@eshop-detail01 src]# ./redis-cli -p 6401 127.0.0.1:6401> info replication # Replication role:master connected_slaves:2 slave0:ip=192.168.99.11,port=6403,state=online,offset=3306,lag=1 slave1:ip=192.168.99.11,port=6405,state=online,offset=3306,lag=1 [root@eshop-detail01 src]# ./redis-cli -p 6402 127.0.0.1:6402> info replication # Replication role:master connected_slaves:2 slave0:ip=192.168.99.11,port=6404,state=online,offset=2660,lag=0 slave1:ip=192.168.99.12,port=6405,state=online,offset=2660,lag=1 本节课所讲的从集群是如上所示那样，对每个集群的从配置为一个读集群， 这里的规划是这样的： detail01：已经有一个 twemproxy 中间件了，只需要新增从集群的配置即可 detail02：需要先安装一个 twemproxy，再配置从集群 安装这里就不再重复了，贴出每台机器的最终配置 detail01：/usr/local/twemproxy-0.4.0/conf/nutcracker.yml redis-master: # 配置一个逻辑名称 listen: 127.0.0.1:1111 hash: fnv1a_64 distribution: ketama redis: true servers: - 192.168.99.11:6401:1 redis01 # 指向两个主集群 - 192.168.99.11:6402:1 redis02 redis-slave: # 从集群 listen: 127.0.0.1:1112 hash: fnv1a_64 distribution: ketama redis: true servers: - 192.168.99.11:6403:1 redis01 - 192.168.99.11:6405:1 redis02 detail02：/usr/local/twemproxy-0.4.0/conf/nutcracker.yml redis-slave: # 从集群 listen: 127.0.0.1:1112 hash: fnv1a_64 distribution: ketama redis: true servers: - 192.168.99.11:6404:1 redis01 - 192.168.99.12:6405:1 redis02 配置完成后，需要重启 nutcracker （kill 再启动） 测试路由是否正确 [root@eshop-detail01 src]# ./redis-cli -p 1112 127.0.0.1:1112> get hello (nil) 127.0.0.1:1112> get k1 \"v1\" 127.0.0.1:1112> get k2 (nil) 127.0.0.1:1112> get p1 \"p1\" 127.0.0.1:1112> get product \"01\" [root@eshop-detail02 src]# ./redis-cli -p 1112 127.0.0.1:1112> get hello \"hello\" 127.0.0.1:1112> get hell1 (nil) 127.0.0.1:1112> get hello1 \"1\" 经过测试，都能读取到各自主集群同步的数据 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/143.html":{"url":"dr/143.html","title":"143. 商品详情页动态渲染系统：部署 RabbitMQ 消息中间件","keywords":"","body":" 143. 商品详情页动态渲染系统：部署 RabbitMQ 消息中间件 安装编译工具 安装依赖 erlang 安装 rabbitmq 143. 商品详情页动态渲染系统：部署 RabbitMQ 消息中间件 安装编译工具 yum install -y ncurses ncurses-base ncurses-devel ncurses-libs ncurses-static ncurses-term ocaml-curses ocaml-curses-devel yum install -y openssl-devel zlib-devel yum install -y make ncurses-devel gcc gcc-c++ unixODBC unixODBC-devel openssl openssl-devel 安装依赖 erlang mkdir /usr/local/erlang wget http://erlang.org/download/otp_src_20.0.tar.gz tar -zxvf otp_src_20.0.tar.gz cd otp_src_20.0 ./configure --prefix=/usr/local/erlang --with-ssl -enable-threads -enable-smmp-support -enable-kernel-poll --enable-hipe --without-javac # 建立软连，软连有啥作用？ ln -s /usr/local/erlang/bin/erl /usr/local/bin/erl # 配置环境变量 vi ~/.bashrc ERLANG_HOME=/usr/local/erlang PATH=$ERLANG_HOME/bin:$PATH # 刷新环境变量 source ~/.bashrc # 尝试使用 erl 看是否正常安装 erl 安装 rabbitmq /usr/local wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.12/rabbitmq-server-generic-unix-3.6.12.tar.xz # 安装解压 xz 包的工具 yum install -y xz xz -d rabbitmq-server-generic-unix-3.6.12.tar.xz tar -xvf rabbitmq-server-generic-unix-3.6.12.tar mv rabbitmq_server-3.6.12 rabbitmq-3.6.12 # 开启 web 管理页面插件 cd rabbitmq-3.6.12/sbin/ ./rabbitmq-plugins enable rabbitmq_management # 后台启动 rabbitmq server ./rabbitmq-server -detached # 关闭 rabbitmq server ./rabbitmqctl stop # 添加管理员账号 ./rabbitmqctl add_user admin 123456 ./rabbitmqctl set_user_tags admin administrator 访问地址：http://192.168.99.11:15672/ 进入管理界面 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/144.html":{"url":"dr/144.html","title":"144. 商品详情页动态渲染系统：部署 MySQL 数据库","keywords":"","body":"144. 商品详情页动态渲染系统：部署 MySQL 数据库 这里使用用最简单的方式装一个 mysql 数据库，后面有数据库可以用来开发就可以了 yum install -y mysql-server # 跟随系统启动 chkconfig mysqld on service mysqld start mysql -u root # 修改 root 账户密码 set password for root@localhost=password('root'); # 允许远程连接 grant all privileges on root.* to 'root'@'%' identified by 'root'; mysql -uroot -proot # 用户名密码修改 UPDATE user SET password=PASSWORD('123456') WHERE user='root'; FLUSH PRIVILEGES; 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/145.html":{"url":"dr/145.html","title":"145. 商品详情页动态渲染系统：声音小问题&课程代码二次开发&商品服务需求","keywords":"","body":"145. 商品详情页动态渲染系统：声音小问题&课程代码二次开发&商品服务需求 ::: tip 声音小问题&课程代码二次开发& 不记录了，讲师小插曲，无伤大雅 ::: 第一阶段：已经将课程需要的一些基础性的环境都搭建好了 第二阶段：搭建了基础设施 目前可以正式进入业务系统的开发了 第一个版本的内容，实际上相对来说，没有出来一个完整的商品详情页架构，这次我们要做一个完整的架构，涉及到很多的服务，商品详情页的数据，都是从各个服务来的 商品服务、价格服务、库存服务、促销服务、广告服务、推荐服务等，可能多大十几个服务，甚至是二十多个服务 动手去开发几个正儿八经的依赖服务：商品服务、价格服务、库存服务 商品服务，管理商品的数据 分类管理：增删改查 品牌管理：增删改查 商品基本信息管理：增删改查 商品规格管理：增删改查 商品属性管理：增删改查 商品介绍管理：编辑 写出来，这块很定是要基于数据库去做，重点就是演示这种服务如何跟商品详情页系统的架构串接起来 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/146.html":{"url":"dr/146.html","title":"146. 商品详情页动态渲染系统：工程师的 why-how-what 思考方法&价格服务说明","keywords":"","body":" 146. 商品详情页动态渲染系统：工程师的 why-how-what 思考方法&价格服务说明 价格服务说明 why-how-what 价格服务思考 why 为什么要做真实的价格业务？ how 如何修改商品价格？ what 就是做一个简单的价格服务 146. 商品详情页动态渲染系统：工程师的 why-how-what 思考方法&价格服务说明 价格服务说明 复杂电商里面，商品的价格，是一个比较复杂的事情 如商家可以调整价格，但是这个时候可能需要引入很多的策略 价格是不是可以为负数？ 活动来临之际，是否允许商家先提价再降价销售？虚假打折 品牌加盟价格，不能比实体店卖的贵 等等的策略，这个很复杂 SKU 一个商品根据属性的不同会有不同的价格，如：土豪金、白色、深空灰，土豪金的价格就比其他颜色的手机要贵一点 还有多件套餐出售，价格也是多种多样 以上所述，本课程内容不打算做这些复杂的业务 why-how-what 大公司里面会给工程师培训一些软素质，比如如何正确的思考 why 为什么要做？ how 如何做？ what 具体做什么？ 错误的思考过程：what -> how -> why 先把事情做了之后，再来问自己为什么要做？貌似没有任何意义了 正确的思考过程：why -> how -> what 考虑一个事情，为什么要做？如何去做？具体做什么？ 价格服务思考 why 为什么要做真实的价格业务？ 对你有什么好处？对你没什么帮助 如果你是一个做电商行业系统的一个从业人员，这些业务对你来说是小儿科，产品经理有很完善的需求文档 如果你不是一个做电商行业系统的从业人员，你了解到了细枝末节的业务，对你也没什么用，你自己做了电商系统，你只有对自己所在的公司所处的行业，你工作了至少 2~3 年以后，才能说对一块业务是熟悉的。除非是那种博客、论坛、demo 级的项目你可以这样说 举个例子：培训机构，培训 j2ee 就业课程，里面的项目讲解 OA 系统、进销存系统，里面就挑选真实系统的 1% 的模块，花 10 天时间给你讲一讲，你出去敢说自己做了 OA 系统？进销存系统？ 对课程来说，有什么帮助？ 业务对我们商品详情页系统，我告诉你，对这个系统来说，是没有太大的意义，因为其实无论的价格怎么变，最终就是变化之后，反馈到商品详情页面里去，让用户可以尽快看到最新的价格，至于价格是怎么变化的，我们不关心 how 如何修改商品价格？ 给一个简单的接口，可以修改商品的价格，落地到数据库中，价格与后面的商品详情页系统架构，串接起来，时效性比较高的服务，去讲解 商品详情页上，部分时效性要求很高的数据，比如价格、库存、是通过 ajax 异步加载的 what 就是做一个简单的价格服务 提供一个接口：可以修改某个商品的价格，落地到数据库中，可以与商品详情页系统架构，串接起来 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/147.html":{"url":"dr/147.html","title":"147. 商品详情页动态渲染系统：库存服务的场景介绍以及课程需求说明","keywords":"","body":" 147. 商品详情页动态渲染系统：库存服务的场景介绍以及课程需求说明 要不要去做复杂的库存服务？ 库存业务的介绍 147. 商品详情页动态渲染系统：库存服务的场景介绍以及课程需求说明 要不要去做复杂的库存服务？ 对你个人的价值，对课程的价值，根本不需要 商品服务，之所以要做一些增删改查的操作，是因为那些东西跟商品详情页系统的影响较大，做那样的一些操作 库存服务 = 价格服务，不需要做复杂的业务，库存变化了，反应到数据库中，跟商品详情页系统架构串接起来，就 ok 了 库存业务的介绍 事务性关联很大：商品的购买，就要修改库存 保证库存的递减一定是事务的：不能失败、不能出错 最怕的就是系统里面库存不准确，比如一个商家都没有库存了，但是还是出了 bug，导致超售 那么这个时候就需要对用户道歉、退款等善后处理 电商退货、退款、库存增加（加回来） 进销存：物流等系统打通，进货、退货、增减库存 主要关注库存的增加和减少的最终结果就 ok 了，我们只要关注库存显示到商品详情页上去就 ok 了 那么我们也只提供一个服务接口：修改商品的商品库存，反应数据库中，跟商品详情页系统架构打通 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/148.html":{"url":"dr/148.html","title":"148. 商品详情页动态渲染系统：微服务与 Spring Cloud 基本介绍","keywords":"","body":" 148. 商品详情页动态渲染系统：微服务与 Spring Cloud 基本介绍 传统架构的问题 微服务架构的几大特征 足够单一的职责与功能 非常的微型 面向服务的思想 独立开发 微服务的强大作用 微服务的缺点 微服务的技术栈 领域驱动设计：微服务建模 Spring Cloud：基础技术架构 DevOps Docker 讲解重点 148. 商品详情页动态渲染系统：微服务与 Spring Cloud 基本介绍 微服务课程在龙果上，在本课程升级之前，就有 2 个课程了 dubbo 的课程 通用架构给搭建了起来，dubbo 去做微服务 + activemq + redis + mysql + 持续集成，把一个比较通用的完整的微服务的技术架构讲解了出来 spring cloud 课程：那个课程会主要讲解 spring cloud 的技术 本课程升级，也会讲解到这些东西：微服务实战、spring cloud、还有各种 DevOps、docker 容器、持续交付流水线，把这些结合在这个真实的电商详情页系统项目实战中去做微服务架构 本课程升级重点： 微服务的项目实战：spring cloud 微服务的项目实战 弥补第一个版本课程的遗憾，要将一个完整的电商详情页系统架构搭建出来、跑通、从前到后全部搞定； 本次的架构因为相对来说比较通用，所以可以套用在一些场景上做二次开发 传统架构的问题 单块应用：耦合严重 开发速度慢：响应新需求慢 不易于扩展和重构 不易于技术升级 一大段话简而言之：传统的一个 java web 项目中的内容太多了，各种业务模块都在一个大的工程中， 从开发到发布过程中，都需要考虑多人协作开发的问题，如：回归测试、其他模块依赖环境、代码合并冲突， 等问题。 微服务架构的几大特征 足够单一的职责与功能 非常的微型 面向服务的思想 独立开发：团队，技术选型，前后端分离，存储分离，独立部署 自动化开发流程：编码，自动化测试，持续集成，自动化部署 足够单一的职责与功能 用最简单的话来说，比如之前可能就一个单块应用，几十个兄弟在一个代码上开发，商品模块、价格模块、库存模块、促销模块、o2o 模块，全部放一起了，可能会有一部分公共的代码，内容多，那么改动的几率就大 那么做成微服务：把几十万行的单块应用拆分出多个服务，每个服务对应一个工程，每个工程就几百行到几千行代码，每个服务职责很单一，负责一块事情，如商品数据的管理做成一个商品服务; 对于价格这种复杂策略，单拉一个价格服务来管理复杂的价格变更的业务; 对于库存这种复杂的服务，也可以单拉一个管理复杂的库存变更的业务 非常的微型 几百行~几千行代码 面向服务的思想 每个服务暴露出来一堆接口，然后其他人都是依赖你的服务在开发 独立开发 工程上完全独立了， 那就可以给不同的服务配置不同的团队，或者工程师去开发。 比如商品服务是 3 个哥儿们在维护，价格服务是 1 个应届生在做，库存服务是 2 个哥儿们在做， 不同的人就做不同的工程，维护自己不同的代码 由于独立开发了，那么对于技术的选型就多样化了：spring mvc + spring + mybatis、php、go、c++， 对于存储 mysql、mongodb、memcached、redis、hbase，每个服务都是自己的存储，单独对接自己的前端工程师，独立的部署在自己的机器上 独立开发，跟其他人没关系 微服务的强大作用 迭代速度 你只要管好自己的服务就行了，跟别人没关系，随便你这么玩儿。 修改代码、测试、部署都是你自己的事情，不用考虑其他人，没有任何耦合 复用性 拆分成一个一个服务之后，就不需要写任何重复的代码了，有一个功能别人做好了，暴露了接口出来，直接调用不就 ok 了么 扩展性 独立、扩展、升级版本、重构、更换技术 完全克服了传统单块应用的缺点 微服务的缺点 服务太多，难以管理 微服务 = 分布式系统 你本来是一个系统，现在拆分成多块，部署在不同的服务器上，一个请求要经过不同的服务器上不同的代码逻辑处理，才能完成，这不就是分布式系统么 分布式一致性、分布式事务、故障+容错 微服务的技术栈 主要分为 4 块： 领域驱动设计：微服务建模 Spring Cloud：基础技术架构 DevOps：自动化+持续集成+持续交付+自动化流水线，将迭代速度提升到极致 Docker：容器管理大量服务 领域驱动设计：微服务建模 ::: tip 本课程内容不包括这个知识 ::: 你的任何业务系统都有自己独特的复杂的业务，但是这个时候就是有一个问题，怎么拆分服务？拆成哪些服务？拆成多大？每个服务负责哪些功能？ 微服务的建模，模型怎么设计，领域驱动的设计思想：可以去分析系统，完成建模的设计 这里不讲解了，一定是要拿超级复杂的业务来讲解，你才能听懂，业务采取的还是比较简单的，领域驱动 至少如果你真的很了解你的业务的话，你大概也知道应该如何去拆分这个服务 Spring Cloud：基础技术架构 各个服务之间怎么知道对方在哪里：服务的注册和发现 服务之间的调用怎么处理：rpc、负载均衡 服务故障的容错 服务调用链条的追踪怎么做 多个服务依赖的统一的配置如何管理DevOps 自动化 + 持续集成 + 持续交付 + 自动化流水线，将迭代速度提升到极致。 如果要将微服务的开发效率提升到最高，那么就需要玩 DevOps 全流程标准化、自动化，大幅度提升你的开发效率 Docker 微服务之后，一个大型的系统可以涉及到几十个，甚至是上百个服务， 那么难点就来了：怎么部署？机器怎么管理？怎么运维？ 使用 docker 可以天然的来解决这些问题 讲解重点 整个微服务技术架构，全部涉及到（可能不会很深入的讲解），全部结合我们的实际的项目，完成整套微服务架构的项目实战 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/149.html":{"url":"dr/149.html","title":"149. 商品详情页动态渲染系统：Spring Boot 与微服务的关系以及开发回顾","keywords":"","body":" 149. 商品详情页动态渲染系统：Spring Boot 与微服务的关系以及开发回顾 spring boot 的特点 spring boot 和微服务 spring boot 入门开发 149. 商品详情页动态渲染系统：Spring Boot 与微服务的关系以及开发回顾 第一版里面的内容 spring boot 也用了，但是几乎没有什么介绍， 本次课程会简单讲解下 spring boot 的特点 快速开发 spring 应用的框架 比如使用 sring mvc + spring + mybatis 开发一个系统 首先配置一大堆 xml 配置文件 其次部署和安装 tomcat、jetty 等容器 跟 java web 打交道（servlet、listener、filter） 手工部署到 tomcat 或者 jetty 等容器中，发布一个 web 应用 简单来说，spring boot 就是看中了这种 java web 应用繁琐而且重复的开发流程，采用了 spring 之上封装的一套框架（spring boot） 尽可能提升我们的开发效率，让我们专注于自己的业务逻辑即可 内嵌 tomcat 和 jetty 容器 不需要单独安装容器，jar 包直接发布一个 web 应用 简化 maven 配置 通过 parent 这种方式，一站式引入需要的各种依赖 基于注解的零配置思想 和各种流行框架 spring web mvc、mybatis、spring cloud 无缝整合 spring boot 和微服务 spring boot 不是微服务技术 spring boot 只是一个用于加速开发 spring 应用的基础框架，简化工作，开发单块应用很适合 如果要直接基于 spring boot 做微服务，相当于需要自己开发很多微服务的基础设施，比如基于 zookeeper 来实现服务注册和发现 spring cloud 才是微服务技术spring boot 入门开发 本小结内容没有什么新的，可以说是车祸现场了，最后的结论就是参考此； 对于 spring boot 基础环境请参考 (第一版：库存服务的开发框架整合与搭建：spring boot + mybatis + jedis)[../039.md] ::: tip 本次使用 spring cloud 相关说明 项目搭建，视频中使用 mavn，本笔记使用 gradle-4.8.1 + spring boot 2.1.6 gradle 版本的不同对于生成的 build.gradle 语法可能不太同 同样，对于 spring boot 来说，2.0.4 版本与 2.1.6 版本生成的 build.gradle 写法相差也有点大 ::: 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/150.html":{"url":"dr/150.html","title":"150. 商品详情页动态渲染系统：Spring Cloud 之 Eureka 注册中心","keywords":"","body":" 150. 商品详情页动态渲染系统：Spring Cloud 之 Eureka 注册中心 什么是注册中心？ eureka 基本原理 搭建项目说明 eureka-server eureka-client 150. 商品详情页动态渲染系统：Spring Cloud 之 Eureka 注册中心 ``` tip 温馨提示 本课程的 spring cloud 教程基本上是官网文档里面的教程， 所以如果觉得稍显吃力可以参考 慕课 Spring Cloud 微服务实战笔记 ## 什么是注册中心？ 1. 首先有一个 eureka server，服务的注册与发现的中心 2. 可以将你写好的服务注册到 eureka server 上去 3. 别人如果需要调用你的服务，就可以才能够 eureka server 上查找你的服务所在地址，然后调用 ## eureka 基本原理 1. 服务都会注册到 eureka 的注册表 2. eureka 有心跳机制，自动检测服务，故障时自动从注册表中摘除 3. 每个服务也会缓存 eureka 的注册表，及时 eureka server 挂掉，每个服务也可以基于本地注册表与其他服务进行通信 4. 如果 eureka server 如果挂掉了，就无法发布新的服务了 ## 搭建项目说明 本次练习项目放在 https://github.com/zq99299/cache-eshop.git 上，关于配置请参考该项目 使用工具或相关版本如下： - gradle v4.8.1 - spring boot v2.1.6.RELEASE - spring cloud vGreenwich.SR2\" - idea 开发 项目结构： // 由于是微服务，所以暂时把所有的项目放在该仓库下，并上传到 git // 如果后续有特殊情况，一定要分仓库才能实现的话，会特殊说明在本地仓库进行试验 |- cache-eshop : 仓库，只做最基础的通用配置 |- cache-eureka-server : 本课要搭建分服务注册中心 特别说明：关于 cloud 的使用，请首先[参考官网资料](https://cloud.spring.io/spring-cloud-static/Greenwich.SR2/single/spring-cloud.html)， 大部分的课程讲解都很基础，在官网上就已经有相关教程了 ## eureka-server eshop-eureka-server/build.gradle ```groovy plugins { id 'org.springframework.boot' version '2.1.6.RELEASE' id 'java' } apply plugin: 'io.spring.dependency-management' ext { set('springCloudVersion', \"Greenwich.SR2\") } dependencies { implementation 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-server' testImplementation 'org.springframework.boot:spring-boot-starter-test' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } application.yml server: port: 8761 eureka: instance: hostname: localhost client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/ EshopEurekaServerApplication package com.mrcode.cache.eshop.eshopeurekaserver; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer; @SpringBootApplication @EnableEurekaServer public class EshopEurekaServerApplication { public static void main(String[] args) { SpringApplication.run(EshopEurekaServerApplication.class, args); } } 启动之后，访问地址：http://localhost:8761/ 能看到管理界面就算可以了。 eureka-client eshop-eurela-client/build.gradle plugins { id 'org.springframework.boot' version '2.1.6.RELEASE' id 'java' } apply plugin: 'io.spring.dependency-management' ext { set('springCloudVersion', \"Greenwich.SR2\") } dependencies { implementation 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-client' implementation 'org.springframework.boot:spring-boot-starter-web' testImplementation 'org.springframework.boot:spring-boot-starter-test' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } application.yml server: port: 9000 eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ EshopEurelaClientApplication package cn.mrcode.cache.eshop.eshopeurelaclient; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; @SpringBootApplication @RestController public class EshopEurelaClientApplication { public static void main(String[] args) { SpringApplication.run(EshopEurelaClientApplication.class, args); } @Value(\"${server.port}\") private int port; @RequestMapping(\"/\") public String home() { return \"Hello world port \" + port; } } 启动之后访问地址：http://localhost:9000/ 能看到 Hello world port 9000 输出就成功了。 而这个时候在服务注册中的界面的 「Instances currently registered with Eureka」一栏中， 就出现了刚刚启动的 eshop-eurela-client 项目 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/151.html":{"url":"dr/151.html","title":"151. 商品详情页动态渲染系统：Spring Cloud 之 Ribbon+Rest 调用负载均衡","keywords":"","body":"151. 商品详情页动态渲染系统：Spring Cloud 之 Ribbon+Rest 调用负载均衡 上一章节我们学习了： 如何发布一个 eureka 注册中心 如何发布一个服务注册到 erueka server 这么服务之间怎么调用呢？本章讲解使用 ribbon 来通过 rest 方式调用服务接口 再创建一个 greeting-service 项目，注册到 eruak server，然后通过 ribbon 调用 eshop-eurela-client 的一个接口 greeting-service/build.gradle plugins { id 'org.springframework.boot' version '2.1.6.RELEASE' id 'java' } apply plugin: 'io.spring.dependency-management' ext { set('springCloudVersion', \"Greenwich.SR2\") } dependencies { implementation 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-client' implementation 'org.springframework.boot:spring-boot-starter-web' testImplementation 'org.springframework.boot:spring-boot-starter-test' // 在 boot2 中 spring-cloud-starter-ribbon 已过时 // https://cloud.spring.io/spring-cloud-netflix/multi/multi_spring-cloud-ribbon.html compile 'org.springframework.cloud:spring-cloud-starter-netflix-ribbon' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } application.yml server: port: 9005 spring: application: name: greeting-service eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ Application package cn.mrcode.cache.eshop.greetingservice; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.loadbalancer.LoadBalanced; import org.springframework.context.annotation.Bean; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.client.RestTemplate; @SpringBootApplication @RestController public class Application { @Autowired private RestTemplate restTemplate; public static void main(String[] args) { SpringApplication.run(Application.class, args); } @RequestMapping(\"/\") public String home() { String rest = restTemplate.getForObject(\"http://eshop-eurela-client\", String.class); return rest; } // 在spring容器中注入一个bean，RestTemplate，作为rest服务接口调用的客户端 // @LoadBalanced标注，代表对服务多个实例调用时开启负载均衡 @Bean @LoadBalanced public RestTemplate restTemplate() { return new RestTemplate(); } } 在项目中引用 spring-cloud-starter-netflix-ribbon ，会触发自动配置， 所以这里使用 @LoadBalanced 注解才会生效；如果不包含该项目也可以使用， 只是没有 @LoadBalanced 负载均衡的效果了 测试流程： 将 eshop-eurela-client 项目启动两个实例，端口为 9000、9001 将 greeting-service 启动 访问地址：http://localhost:9005/ 查看响应的内容 有 ribbon 均衡负载的情况下，访问一次就会变换一次端口。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/152.html":{"url":"dr/152.html","title":"152. 商品详情页动态渲染系统：Spring Cloud 之 Fegion 声明式服务调用","keywords":"","body":"152. 商品详情页动态渲染系统：Spring Cloud 之 Fegion 声明式服务调用 ribbon + rest 是比较底层的调用方式，其实一般不常用 fegion 声明式的服务调用，类似于 rpc 风格的服务调用，默认集成了 ribbon 做负载均衡，集成 eureka 做服务发现 使用如下： 添加依赖 // 注：spring-cloud-starter-netflix-eureka-client 中已经依赖了 ribbon、hystrix、openfeign compile('org.springframework.cloud:spring-cloud-starter-openfeign') 添加注解 @EnableFeignClients 编写接口映射 @FeignClient(name = \"eshop-eurela-client\") public interface EurelaClientService { // get 方法要传递参数的话，必须使用 @RequestParam 注解，并且必须声明参数名称 @GetMapping(\"/\") String home(@RequestParam(name = \"name\") String name); } 调用方式 @Autowired private EurelaClientService eurelaClientService; @RequestMapping(\"/\") public String home() { return eurelaClientService.home(\"xx\"); } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/153.html":{"url":"dr/153.html","title":"153. 商品详情页动态渲染系统：Spring Cloud 之 Hystrix 熔断降级","keywords":"","body":" 153. 商品详情页动态渲染系统：Spring Cloud 之 Hystrix 熔断降级 整合 hystrix hystrix dashboard 整合 /hystrix.stream 404 问题解决 改造被调用服务也支持 hystrix turbine （hystrix dashboard 集群聚合服务） 分集群聚合 153. 商品详情页动态渲染系统：Spring Cloud 之 Hystrix 熔断降级 微服务架构，很重要的就是多个服务之间互相调用，很可能某个服务就死了，然后依赖它的其他服务调用大量超时，最后耗尽资源，继续死，最终导致整个系统崩盘 使用 hystrix 去做资源隔离、限流、熔断、降级 整合 hystrix 添加依赖 // 注：spring-cloud-starter-netflix-eureka-client 中已经依赖了 ribbon、hystrix、openfeign compile 'org.springframework.cloud:spring-cloud-starter-netflix-hystrix' 开启 hystrix feign: hystrix: enabled: true 配置与降级逻辑实现 @FeignClient(name = \"eshop-eurela-client\", fallback = EurelaClientServiceFallback.class) public interface EurelaClientService { @GetMapping(\"/\") String home(@RequestParam(name = \"name\") String name); } @Component public class EurelaClientServiceFallback implements EurelaClientService { @Override public String home(String name) { return \"error \" + name; } } 测试步骤： 重启项目后访问地址 http://localhost:9005/ 查看是否能访问 关闭掉 eshop-eurela-client 实例 再次访问 http://localhost:9005/ 查看是否走了降级机制 在测试过程中发现如下特性： 当其中一个实例关闭时，第一次访问到挂掉的实例时会走降级机制，后续就不会再访问到该实例了 2 个实例都挂掉时，肯定每次访问都是走降级机制了 当实例恢复后，又可以正常提供服务了 hystrix dashboard 整合 添加依赖 // https://cloud.spring.io/spring-cloud-netflix/reference/html/#_how_to_include_hystrix // https://cloud.spring.io/spring-cloud-static/Greenwich.SR2/single/spring-cloud.html#_circuit_breaker_hystrix_dashboard compile 'org.springframework.cloud:spring-cloud-starter-netflix-hystrix-dashboard' compile 'org.springframework.boot:spring-boot-starter-actuator' 注解开启 @EnableHystrixDashboard @EnableCircuitBreaker hystrix dashboard ui 使用 访问地址：http://localhost:9005/hystrix 填入要监控的服务：http://localhost:9005/actuator/hystrix.stream 由于我们在 greeting-service 上开启了 CircuitBreaker 断路器，它就会提供一个 /hystrix.stream 服务, 通过这个服务，dashboard 就能拿到 greeting-service 上断路器状态数据并进行聚合展示了 /hystrix.stream 404 问题解决 但是在访问 /hystrix.stream ，这是因为 boot 2 使用了 endpoint 来管理这些扩展端点，这个配置就是 actuator 包来自动配置的。 暴露该端点 management: endpoints: web: exposure: # 暴露所有端点 include: '*' # 或者单独暴露 /hystrix.stream 断点 配置之后还是发现无法访问，最后在 dashboard ui 中看到了正确的地址 https://hystrix-app:port/actuator/hystrix.stream 改造被调用服务也支持 hystrix 添加依赖等配置与上面的一致，唯一不同的一点就是下面这个。 手动让自己提供服务的方法也支持 hystrix 的管控 eshop-eurela-client 项目 @RequestMapping(\"/\") @HystrixCommand(fallbackMethod = \"sayHello\") public String home(String name) { // 注意，前面在 greeting-service 中调用没有传递相关参数 // 这里为了模拟异常，已添加上 if (name != null && name.equals(\"error\")) { throw new RuntimeException(\"故意异常走降级机制\"); } return \"Hello world port \" + port; } public String sayHello(String name) { return \"降级机制\"; } 之前已经深入讲解过 hystrix 了，这里不再多说，这里只是使用了注解方式让我们自己的方法使用上 hystrix 测试：访问地址 http://localhost:9005/?name=error，可以看到输出了 降级机制 turbine （hystrix dashboard 集群聚合服务） 此方式在官网教程也有讲解 turbine 只是一个聚合 hystrix dashboard stream 的服务，前面讲解的是单实例的 hystrix dashboard stream 支持。 ::: tip 这里只要注意 turbine 不要部署在但实例 hystrix dashboard 上， 因为它是聚合多个流，一般建议建立一个空项目来开启 turbine ::: 新创建一个项目 eshop-turbine eshop-turbine/build.gradle plugins { id 'org.springframework.boot' version '2.1.6.RELEASE' id 'java' } apply plugin: 'io.spring.dependency-management' ext { set('springCloudVersion', \"Greenwich.SR2\") } dependencies { implementation 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-client' implementation 'org.springframework.boot:spring-boot-starter-web' testImplementation 'org.springframework.boot:spring-boot-starter-test' compile 'org.springframework.boot:spring-boot-starter-actuator' // 添加 turbine 自动配置 compile 'org.springframework.cloud:spring-cloud-starter-netflix-turbine' compile 'org.springframework.cloud:spring-cloud-netflix-turbine' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } 使用注解开启 @EnableTurbine application.yml server: port: 9007 spring: application: name: eshop-turbine eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ management: endpoints: web: exposure: include: '*' turbine: # 要聚合的服务 app-config: ESHOP-EURELA-CLIENT,GREETING-SERVICE # 聚合集群配置，其他客户端没有配置，则这里使用默认 # 注意下面的配置非常重要，否则访问 http://localhost:9007/clusters 不会出现任何 /urbine.stream 地址 aggregator: cluster-config: default cluster-name-expression: \"'default'\" 启动项目后可访问如下地址： turbine 支持聚合的服务地址： http://localhost:9007/clusters 聚合流：http://localhost:9007/turbine.stream 在任意一个支持 hystrix dashboard ui 上填入这里的聚合流地址，都能监控所有已配置的服务 可以从图上看到，通过这里的默认配置，把集群中的所有断路器名称都拿过来聚合了， 这里也看到有一丝乱，因为集群没有分开，所以导致看到的是所有服务的 分集群聚合 turbine: # 要聚合的服务 app-config: ESHOP-EURELA-CLIENT,GREETING-SERVICE aggregator: cluster-config: eshop-eurela-client clusterNameExpression: metadata['cluster'] 具体的客户端需要通过如下配置暴露自己的集群名称 eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ instance: metadata-map: # 这里的 cluster 对应了 clusterNameExpression 中的 metadata['cluster'] 表达式要获取的字段 # 而这里的 eshop-eurela-client 则对应了 cluster-config 中的值 cluster: eshop-eurela-client 现在就把 client 项目中的数据分离开来聚合统计显示了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/154.html":{"url":"dr/154.html","title":"154. 商品详情页动态渲染系统：Spring Cloud 之 Zuul 网关路由","keywords":"","body":" 154. 商品详情页动态渲染系统：Spring Cloud 之 Zuul 网关路由 整合 zuul 请求过滤 154. 商品详情页动态渲染系统：Spring Cloud 之 Zuul 网关路由 本章讲解的就是官网教程中的入门部分。 常规的 spring cloud 的微服务架构下，前端请求先通过 nginx 走到 zuul 网关服务， zuul 负责路由转发、请求过滤等网关接入层的功能，默认和 ribbon 整合实现了负载均衡 比如说你有 20 个服务暴露出去，你的调用方如果要跟 20 个服务打交道，是不是很麻烦？ 所以比较好的一个方式，就是开发一个通用的 zuul 路由转发的服务，根据请求 api 模式，动态将请求路由转发到对应的服务 你的前端，主要考虑跟一个服务打交道就可以了 整合 zuul 创建一个新项目 zuul-server/build.gradle plugins { id 'org.springframework.boot' version '2.1.6.RELEASE' id 'java' } apply plugin: 'io.spring.dependency-management' ext { set('springCloudVersion', \"Greenwich.SR2\") } dependencies { implementation 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-client' implementation 'org.springframework.cloud:spring-cloud-starter-netflix-zuul' // 引入 zuul 依赖 implementation 'org.springframework.boot:spring-boot-starter-web' testImplementation 'org.springframework.boot:spring-boot-starter-test' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } 使用注解开启 @EnableZuulProxy 配置路由的两个服务，application.yml server: port: 9010 spring: application: name: zuul-server eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ zuul: routes: greeting-service: path: /greeting-service/** serviceId: greeting-service eshop-eurela-client: path: /eshop-eurela-client/** serviceId: eshop-eurela-client 测试是否转发成功： 现在访问地址：http://localhost:9010/greeting-service/ 之前要访问：http://localhost:9001 现在访问地址：http://localhost:9010/eshop-eurela-client/ 之前要访问：http://localhost:9000 可以看到可以被正常转发 请求过滤 @Component public class MyZuulFilter extends ZuulFilter { private static final Logger logger = LoggerFactory.getLogger(MyZuulFilter.class); // 过滤器类型 pre，routing，post，error @Override public String filterType() { return \"pre\"; } // 顺序 @Override public int filterOrder() { return 0; } // 根据逻辑判断是否要过滤 @Override public boolean shouldFilter() { return true; } @Override public Object run() { RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); logger.info(String.format(\"%s >>> %s\", request.getMethod(), request.getRequestURL().toString())); Object userId = request.getParameter(\"userId\"); // 不携带 userId 这个参数就表示为未登陆 if (userId == null) { logger.warn(\"userId is empty\"); ctx.setSendZuulResponse(false); ctx.setResponseStatusCode(401); try { ctx.getResponse().getWriter().write(\"userId is empty\"); } catch (Exception e) { } return null; } logger.info(\"ok\"); return null; } } 再次访问地址：http://localhost:9010/eshop-eurela-client/ ，会发现返回了 401 的错误：userId is empty 只能加上 userId 访问：http://localhost:9010/eshop-eurela-client/?userId=12 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"dr/155.html":{"url":"dr/155.html","title":"155. 商品详情页动态渲染系统：Spring Cloud 之 Config 统一配置中心","keywords":"","body":" 155. 商品详情页动态渲染系统：Spring Cloud 之 Config 统一配置中心 创建 config-server 改造 greeting-service 使用配置中心 155. 商品详情页动态渲染系统：Spring Cloud 之 Config 统一配置中心 ::: 本章内容的深入使用请参考此笔记内容 spring cloud config ::: 多个服务共享相同的配置，举个例子、数据库连接、redis 连接，还有别的一些东西，包括一些降级开关，等等 创建 config-server 用 config 统一配置中心，config-server/build.gradle plugins { id 'org.springframework.boot' version '2.1.6.RELEASE' id 'java' } apply plugin: 'io.spring.dependency-management' ext { set('springCloudVersion', \"Greenwich.SR2\") } dependencies { implementation 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-client' implementation 'org.springframework.cloud:spring-cloud-config-server' implementation 'org.springframework.boot:spring-boot-starter-web' testImplementation 'org.springframework.boot:spring-boot-starter-test' } dependencyManagement { imports { mavenBom \"org.springframework.cloud:spring-cloud-dependencies:${springCloudVersion}\" } } 使用注解开启服务 @EnableConfigServer application.yml server: port: 9009 spring: application: name: config-server cloud: config: server: git: # uri: http://localhost/mrcode/config.git # username: xx # password: xx # 为了方便本地测试，使用本地目录方式，但是该目录需要是一个 git 仓库 # 使用 H:\\dev\\project\\mrcode\\cache-eshop-config-repo 目录 # test 作为一个配置仓库，在该目录下初始化为一个 git 仓库 uri: file:///H:\\dev\\project\\mrcode\\cache-eshop-config-repo\\test eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 在 test 目录下添加一个文件 application.yml name: zq 添加完成之后，并提交到 git（默认就是 master） 启动项目后，访问地址：http://localhost:9009/name/cc { \"name\": \"name\", \"profiles\": [ \"cc\" ], \"label\": null, \"version\": \"72284bf01842408e885bb3a8831e945bf47b5e76\", \"state\": null, \"propertySources\": [ { \"name\": \"file:///H:\\\\dev\\\\project\\\\mrcode\\\\cache-eshop-config-repo\\\\test/application.yml\", \"source\": { \"name\": \"zq\" } } ] } 可以看到，找到了该属性名称，并且还有版本号，与所在配置文件与对应的值 改造 greeting-service 使用配置中心 增加依赖 implementation 'org.springframework.cloud:spring-cloud-starter-config' 增加引导配置文件 bootstrap.yml spring: cloud: config: uri: http://localhost:9009 # 指向刚才创建的配置中心项目 profile: dev # 默认为 default 使用刚刚配置中心的 name 属性 @Value(\"${name}\") // 该属性在 greeting-service 中的配置文件并没有配置 private String name; @RequestMapping(\"/name\") public String name() { return name; } 重启项目后，访问地址：http://localhost:9005/name 如果返回 zq ，那么表示使用配置中心成功了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/156.html":{"url":"dr/156.html","title":"156. 商品详情页动态渲染系统：Spring Cloud 之 Sleuth 调用链路追踪","keywords":"","body":"156. 商品详情页动态渲染系统：Spring Cloud 之 Sleuth 调用链路追踪 在一个微服务系统中，一个请求过来，可能会经过一个很复杂的调用链路，经过多个服务的依次处理，才能完成。 在这个调用链路过程中，可能任何一个环节都会出问题，所以如果要进行一些问题的定位，那么就要对每个调用链路进行追踪 课程中使用以下依赖与 @EnableZipkinServer 注解实现 compile 'io.zipkin.java:zipkin-server' compile 'io.zipkin.java:zipkin-autoconfigure-ui' 在 spring boot 2 中此方式不再支持了。由于本人项目中简单的用到过，这里就不再使用此方式， 有关使用请参考 zipkin 官网文档 cloud 的 sleuth 与 sleuth-zipkin 的用法，请自行百度 org.springframework.cloud spring-cloud-starter-sleuth org.springframework.cloud spring-cloud-sleuth-zipkin 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/157.html":{"url":"dr/157.html","title":"157. 商品详情页动态渲染系统：Spring Cloud 之 Eureka Server 安全认证","keywords":"","body":"157. 商品详情页动态渲染系统：Spring Cloud 之 Eureka Server 安全认证 课程中使用如下方案实现安全认证 org.springframework.boot spring-boot-starter-security 与 security: basic: enabled: true user: name: admin password: 123456 但是此方式不适用于 boot 2+，过时信息为： Deprecated The security auto-configuration is no longer customizable. Provide your own WebSecurityConfigurer bean instead. 此课程本人忽略，而且也没有讲解注册中心使用了安全认证之后，客户端怎么注册？ 关于 security 的使用，请参考本人另外一个 笔记系列 Spring Security 下面就基于 spring cloud 提供的一整套技术：服务注册与发现、声明式的服务调用、熔断降级、网关路由、统一配置、链路追踪等技术， 来开发我们的商品详情页系统 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/158.html":{"url":"dr/158.html","title":"158. 商品详情页动态渲染系统：完成 Spring Boot+Spring Cloud+MyBatis 整合","keywords":"","body":" 158. 商品详情页动态渲染系统：完成 Spring Boot+Spring Cloud+MyBatis 整合 开发说明 整合 MyBatis 158. 商品详情页动态渲染系统：完成 Spring Boot+Spring Cloud+MyBatis 整合 开发说明 第一版没有做一个比较大而全的系统架构，因为涉及到的东西太多，只是针对几个点去深入讲解的。 第二版使用 cloud 来实战一下。 由于是个人电脑，开发策略调整下：mysql、rabbitmq、redis 等中间件能在 windows 中装的就装一个， 满足开发即可，最终完全课程讲完之后，全部使用微服务的方式部署各种服务到虚拟机模拟的生产环境中去，跑通整个流程 整合 MyBatis 请参考第一版的整合 再次强调一下，本课程只是简单的给你讲解下入门的使用，所以做过的本笔记不再费时间做， 因为在实际业务中绝大部分配置你都不会使用 唯一的配置讲解下，这里开发怎么简单怎么来，只是使用了注册中心，其他的都没有加 server: port: 9014 logging: level: root: info # 启动显示 controller 中的路径映射也就是 mapping org.springframework.web: TRACE # 可以打印 sql cn.mrcode.cache.eshop.userserver: debug spring: application: name: user-server datasource: driver-class-name: com.mysql.jdbc.Driver # driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/eshop?useUnicode=yes&characterEncoding=UTF-8&useSSL=false username: root password: 123456 jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 mybatis: # type-aliases-package: cn.mrcode.cachepdp.eshop.product.ha.model mapper-locations: classpath*:mapper/*.xml eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 启动项目后访问：http://localhost:9014/user 能查询出来 user 表中的信息表示成功 这种注解使用 sql 方式是本人第一次接触，记录下， 不用额外的什么配置，直接使用注解写 sql 即可 public interface UserMapper { List findUserInfo(); // org.apache.ibatis.annotations.Select; @Select(\"select * from user;\") List selectAll(); } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/159.html":{"url":"dr/159.html","title":"159. 商品详情页动态渲染系统：基于 Spring Cloud 开发商品服务（一）","keywords":"","body":" 159. 商品详情页动态渲染系统：基于 Spring Cloud 开发商品服务（一） 搭建商品服务 进行业务代码开发说明 159. 商品详情页动态渲染系统：基于 Spring Cloud 开发商品服务（一） ::: tip 本笔记练习项目，都在该 仓库 中 ::: 搭建商品服务 基于 spring cloud 搭建一个商品服务，跑通基本架构 spring boot + spring cloud + spring mvc + spring + mybatis 创建新项目 eshop-product-service （基础搭建请参考上一章节的 mybatis 整合），端口号是 9100 进行业务代码开发说明 这里还是简化，不会带着你去做复查业务建模，上百张表那种，只会简化再简化，能驱动这里的场景即可。 要做的 crud 功能如下 分类管理：增删改查 品牌管理：增删改查 商品基本信息管理：增删改查 商品规格管理：增删改查 商品属性管理：增删改查 商品介绍管理：编辑 大概会在 170 讲之前都不会使用 cloud 中的其他生态服务，就是很简单的使用一个服务注册 + boot 来开发业务代码， 业务代码写完之后再来搞那一套的生态 创建 3 张表 public class Brand { private Long id; private String name; private String description; } public class Category { private Long id; private String name; private String description; } public class Product { private Long id; private String name; private Long categoryId; private Long brandId; } 实现上面的的功能，这里记录下全注解开发的（本人第一次接触），这里粘贴出来其中一个 mapper 的写法 import org.apache.ibatis.annotations.Delete; import org.apache.ibatis.annotations.Insert; import org.apache.ibatis.annotations.Mapper; import org.apache.ibatis.annotations.Result; import org.apache.ibatis.annotations.Results; import org.apache.ibatis.annotations.Select; import org.apache.ibatis.annotations.Update; import cn.mrcode.cache.eshop.productserver.model.Product; @Mapper public interface ProductMapper { @Insert(\"INSERT INTO product(name,category_id,brand_id) VALUES(#{name},#{categoryId},#{brandId})\") void add(Product product); @Update(\"UPDATE product SET name=#{name},category_id=#{categoryId},brand_id=#{brandId} WHERE id=#{id}\") void update(Product product); @Delete(\"DELETE FROM product WHERE id=#{id}\") void delete(Long id); @Select(\"SELECT * FROM product WHERE id=#{id}\") @Results({ @Result(column = \"category_id\", property = \"categoryId\"), @Result(column = \"brand_id\", property = \"brandId\") }) Product findById(Long id); } service 就不记录了，特别简单的转调 controller 展示其中一个 @RestController @RequestMapping(\"/product\") public class ProductController { @Autowired private ProductService productService; @RequestMapping(\"/add\") public String add(Product product) { try { productService.add(product); } catch (Exception e) { e.printStackTrace(); return \"error\"; } return \"success\"; } @RequestMapping(\"/update\") public String update(Product product) { try { productService.update(product); } catch (Exception e) { e.printStackTrace(); return \"error\"; } return \"success\"; } @RequestMapping(\"/delete\") public String delete(Long id) { try { productService.delete(id); } catch (Exception e) { e.printStackTrace(); return \"error\"; } return \"success\"; } @RequestMapping(\"/findById\") public Product findById(Long id) { try { return productService.findById(id); } catch (Exception e) { e.printStackTrace(); } return new Product(); } } 写完来测试几个功能： 插入分类：http://localhost:9100/category/add?name=手机&description=电子类 查询分类：http://localhost:9100/category/findById?id=1 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/160.html":{"url":"dr/160.html","title":"160. 商品详情页动态渲染系统：基于 Spring Cloud 开发商品服务（二）","keywords":"","body":"160. 商品详情页动态渲染系统：基于 Spring Cloud 开发商品服务（二） 本章实现以下几个 CRUD 功能： 商品属性管理：增删改查 商品规格管理：增删改查 商品介绍管理：编辑 这里和后面要讲解的商品介绍，分段存储和分段 ajax 加载，也不讲解图片，存储的都是一些图片的名字， 因为有专门的图片服务器去做这个，这里就不再深入了 实际上，对于工程师而言，在一个大的系统中，可能就是架构师设计架构，玩儿的是技术含量最高的，你在里面写业务代码，CRUD 创建三张表： /** * 商品内容 */ public class ProductIntro { private Long id; // 里面存储 1.jpg,2.jpg,3.jpg 图片名称 private String content; private Long productId; } /** * 商品属性 */ public class ProductProperty { private Long id; // 如 机身颜色 iPhoneX【5.8寸黑色】 、 iPhoneX【5.8寸银色】 private String name; private String value; private Long productId; } /** * 商品规格 */ public class ProductSpecification { private Long id; // 如：分辨率: 2436x1125像素 private String name; private String value; private Long productId; } 添加以下数据，以下数据表示了本次一个商品的信息数据 插入分类：http://localhost:9100/category/add?name=手机&description=电子类 插入品牌：http://localhost:9100/brand/add?name=Apple/苹果&description=苹果产品描述 插入产品：http://localhost:9100/product/add?name=Apple/苹果 iPhone X 5.8寸 国行 iphonex三网通4G 全新苹果x手机&categoryId=1&brandId=1 插入属性：http://localhost:9100/product-property/add?name=机身颜色&value=iPhoneX【5.8寸黑色】,iPhoneX【5.8寸银色】&productId=1 插入属性：http://localhost:9100/product-property/add?name=版本类型&value=中国大陆,港澳台&productId=1 插入规格：http://localhost:9100/product-specification/add?name=分辨率&value=2436x1125像素&productId=1 插入规格：http://localhost:9100/product-specification/add?name=网络类型&value=4G全网通&productId=1 插入介绍：http://localhost:9100/product-intro/add?content=1.jpg,2.jpg,3.jpg,4.jpg,5.jpg&productId=1 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/161.html":{"url":"dr/161.html","title":"161. 商品详情页动态渲染系统：基于 Spring Cloud 开发价格服务","keywords":"","body":"161. 商品详情页动态渲染系统：基于 Spring Cloud 开发价格服务 单开一个服务 eshop-price-service，端口 9102，修改商品价格。 在真实的业务中这个是很复杂的，在这里只是修改价格 创建一个价格表 public class ProductPrice { private Long id; private Double value; private Long productId; } 访问地址，添加价格：http://localhost:9102/product-price/add?value=7480.00&productId=1 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/162.html":{"url":"dr/162.html","title":"162. 商品详情页动态渲染系统：基于 Spring Cloud 开发库存服务","keywords":"","body":"162. 商品详情页动态渲染系统：基于 Spring Cloud 开发库存服务 要实现的内容和上节的价格服务一致，只是名称变成了库存；新建一个服务 eshop-inventory-service，端口 9104 创建一张表 public class ProductInventory { private Long id; private Double value; private Long productId; } 访问地址，添加库存：http://localhost:9104/product-inventory/add?value=12&productId=1 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/163.html":{"url":"dr/163.html","title":"163. 商品详情页动态渲染系统：windows 部署 rabbitmq 作为开发测试环境","keywords":"","body":" 163. 商品详情页动态渲染系统：windows 部署 rabbitmq 作为开发测试环境 开启管理界面 添加管理员账户 163. 商品详情页动态渲染系统：windows 部署 rabbitmq 作为开发测试环境 官网安装指导 安装依赖：esl-erlang_22.0_windows_amd64.exe 官网下载页面 https://www.erlang-solutions.com/resources/download.html 安装：rabbitmq-server-3.7.17.exe 开启管理界面 # 进入安装目录 cd rabbitmq_server-3.7.17\\sbin # 开启管理插件 rabbitmq-plugins.bat enable rabbitmq_management 添加管理员账户 # 进入安装目录 cd rabbitmq_server-3.7.17\\sbin ./rabbitmqctl.bat add_user admin 123456 ./rabbitmqctl.bat set_user_tags admin administrator # 给权限，或者在 ui 的 admin 界面 ui 操作 # 如果此命令不行，那么久在 ui 中操作吧 ./rabbitmqctl.bat set_permissions -p / admin '.*' '.*' '.*' 访问管理界面 ：http://localhost:15672 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/164.html":{"url":"dr/164.html","title":"164. 商品详情页动态渲染系统：windows 部署 redis 作为开发测试环境","keywords":"","body":"164. 商品详情页动态渲染系统：windows 部署 redis 作为开发测试环境 下载 msi 程序 https://github.com/microsoftarchive/redis/releases 安装之后，就作为 windows 服务启动了 配置文件在安装目录下 D:\\Program Files\\Redis; 直接运行 redis-cli.exe 就进入了客户端模式（或者使用 win 自带 cmd 执行 redis-cli.exe 命令也可以） 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/165.html":{"url":"dr/165.html","title":"165. 商品详情页动态渲染系统：依赖服务将数据变更消息写入 rabbitmq 或双写 redis","keywords":"","body":" 165. 商品详情页动态渲染系统：依赖服务将数据变更消息写入 rabbitmq 或双写 redis 整合 rabbitmq 的发送与消费 生产者与消费者的 helloWord 代码如下 业务实现 整合 redis 165. 商品详情页动态渲染系统：依赖服务将数据变更消息写入 rabbitmq 或双写 redis 下 本次整合是在 eshop-product-service 项目中 整合 rabbitmq 的发送与消费 添加依赖 compile 'org.springframework.boot:spring-boot-starter-amqp' 自动配置 spring: rabbitmq: host: localhost port: 5672 username: admin password: 123456 此自动配置在 org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration.RabbitTemplateConfiguration#rabbitTemplate 中被应用， 生产者与消费者的 helloWord 代码如下 // 生产者 @Component public class RabbitMQSender { @Autowired private AmqpTemplate rabbitTemplate; public void send(String message) { this.rabbitTemplate.convertAndSend(\"my-queue\", message); } } // 消费者 @Component @RabbitListener(queues = \"my-queue\") public class RabbitMQReceiver { @RabbitHandler public void process(String message) { System.out.println(\"从my-queue队列接收到一条消息：\" + message); } } 业务实现 商品服务数据变更，将消息写入 rabbitmq，时效性比较低的数据，走 rabbitmq，然后后面就接着整套动态渲染系统去玩儿 封装一个生产者 package cn.mrcode.cache.eshop.productserver.rabbitmq; @Component public class RabbitMQSender { @Autowired // private AmqpTemplate amqpTemplate; private RabbitTemplate rabbitTemplate; public void send(String queue, String message) { this.rabbitTemplate.convertAndSend(queue, message); } } 在以下服务中添加事件发送 BrandService CategoryService ProductIntroService ProductPropertyService ProductService ProductSpecificationService 先封装一个事件实体，服务中以该实体作为载体，比自己拼接 json 串方便 public class ProductEvent { private final String eventType; private final String dataType; private final Long id; public ProductEvent(String eventType, String dataType, Long id) { this.eventType = eventType; this.dataType = dataType; this.id = id; } } 这里以品牌为例： @Service public class BrandServiceImpl implements BrandService { @Autowired private BrandMapper brandMapper; @Autowired private RabbitMQSender rabbitMQSender; public void add(Brand brand) { brandMapper.add(brand); // data_change_queue 队列名称，不会自动创建，所以需要在 ui 中手动创建 rabbitMQSender.send(RabbitMQName.DATA_CHANGE_QUEUE, JSON.toJSONString(new ProductEvent(\"add\", \"brand\", brand.getId()))); } public void update(Brand brand) { brandMapper.update(brand); rabbitMQSender.send(RabbitMQName.DATA_CHANGE_QUEUE, JSON.toJSONString(new ProductEvent(\"update\", \"brand\", brand.getId()))); } public void delete(Long id) { brandMapper.delete(id); rabbitMQSender.send(RabbitMQName.DATA_CHANGE_QUEUE, JSON.toJSONString(new ProductEvent(\"delete\", \"brand\", id))); } } 整合 redis 价格服务和库存服务数据变更，直接将数据双写到 redis 中， 时效性比较高的数据，直接 mysql+redis 双写，不走动态渲染系统，写到 redis 之后，后面走 OneService 服务提供页面的 ajax 调用 添加依赖 compile 'org.springframework.boot:spring-boot-starter-data-redis' 添加自动配置（有关 spring-boot-starter-data-redis 的详细深入配置请去官网查询） spring: redis: port: 6379 host: localhost 使用处直接使用 RedisTemplate 即可 @Autowired private RedisTemplate redisTemplate; 这里以商品价格服务来示例 @Autowired private RedisTemplate redisTemplate; public void add(ProductPrice productPrice) { productPriceMapper.add(productPrice); redisTemplate.opsForValue().set(\"product_price_\" + productPrice.getId(), JSON.toJSONString(productPrice)); } public void update(ProductPrice productPrice) { productPriceMapper.update(productPrice); redisTemplate.opsForValue().set(\"product_price_\" + productPrice.getId(), JSON.toJSONString(productPrice)); } public void delete(Long id) { productPriceMapper.delete(id); redisTemplate.delete(\"product_price_\" + id); } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-24 12:33:26 "},"dr/166.html":{"url":"dr/166.html","title":"166. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据同步服务","keywords":"","body":"166. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据同步服务 创建一个新服务 eshop-datasync-service（端口 9106） 数据同步服务，就是获取各种原子数据的变更消息 通过 spring cloud fegion 调用 eshop-product-service 服务的各种接口，获取数据 将原子数据在 redis 中进行增删改 将维度数据变化消息写入 rabbitmq 中另外一个 queue，供数据聚合服务来消费 维度分类（这里维度分类是为了后面的聚合服务聚合用的）： brand category product_intro product 核心实现 package cn.mrcode.cache.eshop.datasyncserver.rabbitmq; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import org.springframework.amqp.rabbit.annotation.RabbitHandler; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.stereotype.Component; import cn.mrcode.cache.eshop.datasyncserver.service.EshopProductService; @Component @RabbitListener(queues = \"data-change-queue\") // 前面的队列也统一成这种横线写法 public class DataChangeQueueReceiver { @Autowired private EshopProductService eshopProductService; @Autowired private RedisTemplate redisTemplate; @Autowired private RabbitMQSender rabbitMQSender; /** * 数据聚合队列 */ final static String AGGR_DATA_CHANGE_QUEUE = \"aggr-data-change-queue\"; @RabbitHandler public void process(String message) { ProductEvent productEvent = JSON.parseObject(message, ProductEvent.class); // 先获取data_type String dataType = productEvent.getDataType(); switch (dataType) { case \"brand\": processBrandDataChangeMessage(productEvent); break; case \"category\": processCategoryDataChangeMessage(productEvent); break; case \"product\": processProductDataChangeMessage(productEvent); break; case \"product_intro\": processProductIntroDataChangeMessage(productEvent); break; case \"product_property\": processProductPropertyDataChangeMessage(productEvent); break; case \"product_specification\": processProductSpecificationDataChangeMessage(productEvent); break; } } private void processBrandDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { // 通过 fegion 写的 service，获取相关数据 JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findBrandById(id)); redisTemplate.opsForValue().set(\"brand_\" + dataJSONObject.getLong(\"id\"), dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"brand_\" + id); } DimEvent dimEvent = new DimEvent(\"brand\", id); rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); } private void processCategoryDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findCategoryById(id)); redisTemplate.opsForValue().set(\"category_\" + dataJSONObject.getLong(\"id\"), dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"category_\" + id); } DimEvent dimEvent = new DimEvent(\"category\", id); rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); } private void processProductDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findProductById(id)); redisTemplate.opsForValue().set(\"product_\" + dataJSONObject.getLong(\"id\"), dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"product_\" + id); } DimEvent dimEvent = new DimEvent(\"product\", id); rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); } private void processProductIntroDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); // 注意这里，产品有关联的几个维度数据都使用产品 id 进行放置，在数据聚合里面都是通过 productId 对产品完整数据聚合 // 那么与之对应发送事件的 eshop-product-service 服务中就要加上这个属性 Long productId = productEvent.getProductId(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findProductIntroById(id)); redisTemplate.opsForValue().set(\"product_intro_\" + productId, dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"product_intro_\" + id); } // 这里暂时还不知道为什么要用 product 事件，而不是具体的对象事件，只能后面再来补坑了 DimEvent dimEvent = new DimEvent(\"product_intro\", productId); rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); } private void processProductPropertyDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); Long productId = productEvent.getProductId(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findProductPropertyById(id)); redisTemplate.opsForValue().set(\"product_property_\" + productId, dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"product_property_\" + id); } // 这里暂时还不知道为什么要用 product 事件，而不是具体的对象事件，只能后面再来补坑了 DimEvent dimEvent = new DimEvent(\"product\", productId); rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); } private void processProductSpecificationDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); Long productId = productEvent.getProductId(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findProductSpecificationById(id)); redisTemplate.opsForValue().set(\"product_specification_\" + productId, dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"product_specification_\" + id); } // 这里暂时还不知道为什么要用 product 事件，而不是具体的对象事件，只能后面再来补坑了 DimEvent dimEvent = new DimEvent(\"product\", productId); rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); } } 测试 访问商品修改地址：http://localhost:9100/product/update?id=1&name=修改Apple/苹果 iPhone X 5.8寸 国行 iphonex三网通4G 全新苹果x手机&categoryId=1&brandId=1 观察 aggr-data-change-queue 队列中是否有消息 观察 redis 中是否存在 product_1 的 key 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/167.html":{"url":"dr/167.html","title":"167. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据聚合服务","keywords":"","body":"167. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据聚合服务 数据同步服务 接收各个依赖服务发送过来的某个原子数据的变更消息， 将原子数据通过 fegion 调用依赖服务的接口拉取过来存入 redis 中。 再将某个维度数据的变更消息发送到另外一个 queue 中 数据聚合服务 监听维护数据变更事件，从 redis 中将这个维度数据全部读取出来， 拼成一个大的聚合 json 串，再将这个维度数据存入 redis 中 维度分类： brand category product_intro product 业务实现，新建一个服务 eshop-dataaggr-service （端口 9108） 核心业务代码如下，这里唯一的一个多原子数据就是商品信息，看下就能明白数据聚合服务的工作原理了 package cn.mrcode.cache.eshop.dataaggrserver.rabbitmq; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import org.apache.commons.lang.StringUtils; import org.springframework.amqp.rabbit.annotation.RabbitHandler; import org.springframework.amqp.rabbit.annotation.RabbitListener; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.stereotype.Component; @Component @RabbitListener(queues = \"aggr-data-change-queue\") public class DataChangeQueueReceiver { @Autowired private RedisTemplate redisTemplate; @RabbitHandler public void process(String message) { DimEvent event = JSON.parseObject(message, DimEvent.class); String dimType = event.getDimType(); switch (dimType) { case \"brand\": processBrandDimDataChangeMessage(event); break; case \"category\": processCategoryDimDataChangeMessage(event); break; case \"product\": processProductDimDataChangeMessage(event); break; case \"product_intro\": processProductIntroDimDataChangeMessage(event); break; } } /** * * 只看这里觉得可能是多此一举，这里说明下： * 1. 业务数据简化了 * 2. 实际业务中，每个维度都不可能只有一个原子数据 * 3. 比如品牌：结构多变，复杂，有很多不同的表，不同的原子数据，这里需要将一个品牌对应的多个原子数据都从 redis 中读取出来，聚合后写入 redis * */ private void processBrandDimDataChangeMessage(DimEvent event) { String key = \"brand_\" + event.getId(); String jsonStr = redisTemplate.opsForValue().get(key); if (StringUtils.isBlank(jsonStr)) { redisTemplate.delete(key); } else { redisTemplate.opsForValue().set(\"dim_\" + key, jsonStr); } } private void processCategoryDimDataChangeMessage(DimEvent event) { String key = \"category_\" + event.getId(); String jsonStr = redisTemplate.opsForValue().get(key); if (StringUtils.isBlank(jsonStr)) { redisTemplate.delete(key); } else { redisTemplate.opsForValue().set(\"dim_\" + key, jsonStr); } } private void processProductDimDataChangeMessage(DimEvent event) { Long productId = event.getId(); String productKey = \"product_\" + productId; String productJsonStr = redisTemplate.opsForValue().get(productKey); if (StringUtils.isBlank(productJsonStr)) { // 主商品数据都没有的话，就直接删除这个聚合数据 redisTemplate.delete(productKey); } else { JSONObject product = JSON.parseObject(productJsonStr); String productPropertyJsonStr = redisTemplate.opsForValue().get(\"product_property_\" + productId); if (StringUtils.isNotBlank(productPropertyJsonStr)) { product.put(\"productProperty\", JSON.parseObject(productPropertyJsonStr)); } String productSpecificationJsonStr = redisTemplate.opsForValue().get(\"product_specification_\" + productId); if (StringUtils.isNotBlank(productSpecificationJsonStr)) { product.put(\"productSpecification\", JSON.parseObject(productSpecificationJsonStr)); } redisTemplate.opsForValue().set(\"dim_\" + productKey, product.toJSONString()); } } private void processProductIntroDimDataChangeMessage(DimEvent event) { String key = \"product_intro\" + event.getId(); String jsonStr = redisTemplate.opsForValue().get(key); if (StringUtils.isBlank(jsonStr)) { redisTemplate.delete(key); } else { redisTemplate.opsForValue().set(\"product_intro\" + key, jsonStr); } } } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/168.html":{"url":"dr/168.html","title":"168. 商品详情页动态渲染系统：完成数据同步服务与数据聚合服务的测试","keywords":"","body":"168. 商品详情页动态渲染系统：完成数据同步服务与数据聚合服务的测试 这里就是测试之前写的所有代码，由于之前基本上都测试过了，这里测试下商品维度聚合功能 修改商品：http://localhost:9100/product/update?id=1&name=修改Apple/苹果 iPhone X 5.8寸 国行 iphonex三网通4G 全新苹果x手机&categoryId=1&brandId=1 修改商品规格：http://localhost:9100/product-specification/update?id=1&name=网络类型&value=4G全网通-修改&productId=1 修改商品属性：http://localhost:9100/product-property/update?id=1&name=机身颜色&value=修改iPhoneX【5.8寸黑色】,iPhoneX【5.8寸银色】&productId=1 在 redis 中查看 dim_product_1 这个 key 的信息 { \"brandId\": 1, \"categoryId\": 1, \"id\": 1, \"name\": \"修改2-Apple/苹果 iPhone X 5.8寸 国行 iphonex三网通4G 全新苹果x手机\", \"productProperty\": { \"id\": 1, \"name\": \"机身颜色\", \"value\": \"修改iPhoneX【5.8寸黑色】,iPhoneX【5.8寸银色】\" }, \"productSpecification\": { \"id\": 2, \"name\": \"网络类型\", \"value\": \"4G全网通-修改\" } } 本次测试发现以下问题： 商品服务中往队列里面投递的商品原子数据更改事件的 dataType 与同步服务中不一致，导致测试过程中没有看到对应的聚合效果 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/169.html":{"url":"dr/169.html","title":"169. 商品详情页动态渲染系统：消息队列架构升级之去重队列","keywords":"","body":" 169. 商品详情页动态渲染系统：消息队列架构升级之去重队列 为什么要做去重队列？ 去重队列实现思路 业务实现 169. 商品详情页动态渲染系统：消息队列架构升级之去重队列 已经做好的有如下： 基础依赖服务 商品服务走动态渲染系统（本章也属于动态渲染系统） OneService 系统：价格服务和库存服务走 mysql + redis 双写 前面已经讲过的流程： 商品服务：增删改、并发送变更消息到 mq 数据同步服务：接收变更消息，获取原子数据更新到 redis 中，并发送维度数据变更事件 数据聚合服务： 将原子数据从 redis 中查询出来，按维度聚合后写入 redis 对这个里面的细节进行架构上的优化和升级：本次做去重队列。 为什么要做去重队列？ 从现有架构和场景来看，数据同步服务接收到每一个原子数据变更都会发送 dim 事件给聚合服务， 短时间内一个商品修改了多次，那么数据聚合服务，将会多次从 redis 中取出数据聚合再回写。 所以做去重队列是有必要的 去重队列实现思路 在内存中放置一个 set 集合，来一个原子操作就放入 set 中，在一个时间窗口内（如一分钟）， 清空一次 set 集合，这里就实现了多条相同维度数据合并去重的功能。 优点如下： 减少数据聚合服务的压力 减少数据聚合服务调用 redis 次数 业务实现 核心代码如下 private Set dimDataChangeEventSet = Collections.synchronizedSet(new HashSet<>()); // 拿商品数据来举例 private void processProductDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findProductById(id)); redisTemplate.opsForValue().set(\"product_\" + dataJSONObject.getLong(\"id\"), dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"product_\" + id); } DimEvent dimEvent = new DimEvent(\"product\", id); // 这里不直接发送，先放入 set 中去重 // rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); dimDataChangeEventSet.add(JSON.toJSONString(dimEvent)); System.out.println(\"Product: \" + id); } // 新开一个线程，定时去清空集合，投递消息 @PostConstruct public void start() { new Thread(() -> { while (true) { // 这种方式目前肯定在并发下会出现问题，这线程和上面的线程不同步，会导致某些数据没有被处理就清空了 // 我自己感觉会有这个问题，可能概率有点小 if (!dimDataChangeEventSet.isEmpty()) { for (String dimEvent : dimDataChangeEventSet) { rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, dimEvent); } dimDataChangeEventSet.clear(); } try { TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } } }).start(); } 测试：拿商品维度来举例，业务无论执行以下哪一个修改操作，都会触发一条商品维度数据的变更消息， 这里在 10 秒内执行以下三条修改操作，看最后执行了几次商品维度数据的聚合。 修改商品：http://localhost:9100/product/update?id=1&name=修改Apple/苹果 iPhone X 5.8寸 国行 iphonex三网通4G 全新苹果x手机&categoryId=1&brandId=1 修改商品规格：http://localhost:9100/product-specification/update?id=1&name=网络类型&value=4G全网通-修改&productId=1 修改商品属性：http://localhost:9100/product-property/update?id=1&name=机身颜色&value=修改iPhoneX【5.8寸黑色】,iPhoneX【5.8寸银色】&productId=1 日志输出如下，可以看到，去重成功 ProductProperty: 1 ProductSpecification: 1 Product: 1 商品聚合：cn.mrcode.cache.eshop.dataaggrserver.rabbitmq.DimEvent@4334d1d5 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/170.html":{"url":"dr/170.html","title":"170. 商品详情页动态渲染系统：消息队列架构升级之刷数据与高优先级队列","keywords":"","body":" 170. 商品详情页动态渲染系统：消息队列架构升级之刷数据与高优先级队列 什么是刷数据？ 高优先级队列 业务实现 170. 商品详情页动态渲染系统：消息队列架构升级之刷数据与高优先级队列 什么是刷数据？ 刷数据就是全量刷新 redis 中的数据，那么此操作一般在以下场景中需要： 代码 bug 上线新功能 某个对象字段改动，如 status 字段，以前存储的是数值，现在要改成 open、send 等字符串 这个时候就需要全量的刷新数据了，而且一般都是在晚上凌晨时，依赖服务会大量的更新数据，大量的请求会进入到消息队列中， 此时系统压力会非常大，可能会影响夜间一些正常用户的购买行为等 解决这个场景，就是单独开一个刷数据队列，选择在晚上凌晨时来消费该队列中的请求 高优先级队列 所谓高优先级，比如一些活动促销，希望快速的响应。那么这个时候就可以单独开一个高优先级队列， 来单独处理这种加急的场景。 业务实现 总之，以上两个队列基本上是在消息队列架构中比较重要和基础的队列， 一个是等待时机消息，一个是能让加急的消息能快速处理； 核心原理就是，合理拆分消息，不和普通消息混合在一起。 新增以下队列 refresh-data-change-queue refresh-aggr-data-change-queue hight-priority-data-change-queue hight-priority-aggr-data-change-queue 在商品服务的增删改中就要支持 3 中队列事件的操作 视频中是给了一个 operationType 标识，选择走哪个队列就走哪个队列。 数据同步中也是需要单独创建一个消费者来消费，然后投递到聚合服务队列中 数据聚合服务中也是需要单独的消费者来消费 总的来说，与普通消息处理流程类似，只是从源头事件上走了不同的队列 本章就不练习了，因为没有太理解这样做在实际业务场景中要怎么去解决什么业务？ 原理是明白，拆分不同的队列，支持不同的特性业务。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/171.html":{"url":"dr/171.html","title":"171. 商品详情页动态渲染系统：吞吐量优化之批量调用依赖服务接口","keywords":"","body":"171. 商品详情页动态渲染系统：吞吐量优化之批量调用依赖服务接口 找到优化的地方和场景，比如下面这个位置 private void processBrandDataChangeMessage(ProductEvent productEvent) { Long id = productEvent.getId(); String eventType = productEvent.getEventType(); if (\"add\".equals(eventType) || \"update\".equals(eventType)) { // 这里单个的调用，视频中是放在一个 list 中的，当大于 20 条的时候，就批量请求一次接口 // 当然对应的服务需要提供批量查询接口。 // 然后再单个放到 redis 中 JSONObject dataJSONObject = JSONObject.parseObject(eshopProductService.findBrandById(id)); redisTemplate.opsForValue().set(\"brand_\" + dataJSONObject.getLong(\"id\"), dataJSONObject.toJSONString()); } else if (\"delete\".equals(eventType)) { redisTemplate.delete(\"brand_\" + id); } DimEvent dimEvent = new DimEvent(\"brand\", id); // rabbitMQSender.send(AGGR_DATA_CHANGE_QUEUE, JSON.toJSONString(dimEvent)); dimDataChangeEventSet.add(JSON.toJSONString(dimEvent)); } 思路和场景都有了，这里就不在代码中练习了，因为练习价值不高（视频中无并发冲突考虑） 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/172.html":{"url":"dr/172.html","title":"172. 商品详情页动态渲染系统：吞吐量优化之 redis mget 批量查询数据","keywords":"","body":"172. 商品详情页动态渲染系统：吞吐量优化之 redis mget 批量查询数据 优化的点也很简单，当一个操作中多次从 redis 中获取数据的时候，可以使用 mget 语法批量获取。 如下面数据聚合中的商品维度信息聚合操作 private void processProductDimDataChangeMessage(DimEvent event) { System.out.println(\"商品聚合：\" + event); Long productId = event.getId(); String productKey = \"product_\" + productId; String productJsonStr = redisTemplate.opsForValue().get(productKey); if (StringUtils.isBlank(productJsonStr)) { // 主商品数据都没有的话，就直接删除这个聚合数据 redisTemplate.delete(productKey); } else { JSONObject product = JSON.parseObject(productJsonStr); String productPropertyJsonStr = redisTemplate.opsForValue().get(\"product_property_\" + productId); if (StringUtils.isNotBlank(productPropertyJsonStr)) { product.put(\"productProperty\", JSON.parseObject(productPropertyJsonStr)); } String productSpecificationJsonStr = redisTemplate.opsForValue().get(\"product_specification_\" + productId); if (StringUtils.isNotBlank(productSpecificationJsonStr)) { product.put(\"productSpecification\", JSON.parseObject(productSpecificationJsonStr)); } redisTemplate.opsForValue().set(\"dim_\" + productKey, product.toJSONString()); } } 上面是单个获取，如果商品存在的话，会调用 redis 3 次。使用 mget 语法优化后如下 private void processProductDimDataChangeMessageBatch(DimEvent event) { System.out.println(\"商品聚合：\" + event); Long productId = event.getId(); String productKey = \"product_\" + productId; String productPropertyKey = \"product_property_\" + productId; String productSpecificationKey = \"product_specification_\" + productId; List items = redisTemplate.opsForValue().multiGet(Arrays.asList(productKey, productPropertyKey, productSpecificationKey)); String productJsonStr = items.get(0); if (StringUtils.isBlank(productJsonStr)) { // 主商品数据都没有的话，就直接删除这个聚合数据 redisTemplate.delete(productKey); } else { JSONObject product = JSON.parseObject(productJsonStr); String productPropertyJsonStr = items.get(1); if (StringUtils.isNotBlank(productPropertyJsonStr)) { product.put(\"productProperty\", JSON.parseObject(productPropertyJsonStr)); } String productSpecificationJsonStr = items.get(2); if (StringUtils.isNotBlank(productSpecificationJsonStr)) { product.put(\"productSpecification\", JSON.parseObject(productSpecificationJsonStr)); } redisTemplate.opsForValue().set(\"dim_\" + productKey, product.toJSONString()); } } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/173.html":{"url":"dr/173.html","title":"173. 商品详情页动态渲染系统：在分发层 nginx 部署流量分发的 lua 脚本","keywords":"","body":"173. 商品详情页动态渲染系统：在分发层 nginx 部署流量分发的 lua 脚本 参考前面的流量分发脚本，思路和之前的一样，按照 productId 来哈希到后端某一台服务上去。 这里也不记录了，始终感觉基本上是前面讲过的东西，现在也还是没有特别的符合第 2 版的主题，很多地方都是入门级别的一一带过。其实很多地方都有数据竞争，要解决竞争，还要高并发，很多地方都没有讲到，不知道是不是我还迷失在某些点中，一直想不透，还是根本就没有这些数据竞争的地方？ 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/174.html":{"url":"dr/174.html","title":"174. 商品详情页动态渲染系统：完成应用层 nginx 的 lua 脚本的编写与部署","keywords":"","body":"174. 商品详情页动态渲染系统：完成应用层 nginx 的 lua 脚本的编写与部署 同样这里也不实战了，与之前讲过的 多级缓存 知识点来看，只是多了一个 resty.redis 包的使用， 使用 lua 直接连 redis 获取数据。 local cjson = require(\"cjson\") local http = require(\"resty.http\") local redis = require(\"resty.redis\") local function close_redis(red) if not red then return end local pool_max_idle_time = 10000 local pool_size = 100 local ok, err = red:set_keepalive(pool_max_idle_time, pool_size) if not ok then ngx.say(\"set keepalive error : \", err) end end local uri_args = ngx.req.get_uri_args() local productId = uri_args[\"productId\"] local cache_ngx = ngx.shared.my_cache local productCacheKey = \"product_\"..productId -- 从本地缓存获取商品信息 local productCache = cache_ngx:get(productCacheKey) -- 如果本地缓存没有，则从 redis 中获取 if productCache == \"\" or productCache == nil then local red = redis:new() red:set_timeout(1000) local ip = \"192.168.31.223\" local port = 1112 local ok, err = red:connect(ip, port) if not ok then ngx.say(\"connect to redis error : \", err) return close_redis(red) end local redisResp, redisErr = red:get(\"dim_product_\"..productId) -- 如果从 redis 中也获取不到数据，则直接从 数据直连服务获取（后面章节会讲解数据直连服务） if redisResp == ngx.null then local httpc = http.new() local resp, err = httpc:request_uri(\"http://192.168.31.179:8767\",{ method = \"GET\", path = \"/product?productId=\"..productId }) productCache = resp.body end productCache = redisResp math.randomseed(tostring(os.time()):reverse():sub(1, 7)) local expireTime = math.random(600, 1200) -- 再延长时间放回本地缓存 cache_ngx:set(productCacheKey, productCache, expireTime) end local context = { productInfo = productCache, } -- 通过模板渲染返回 local template = require(\"resty.template\") template.render(\"product.html\", context) 这里的核心思路就是：如果 nginx local cache 没有，则通过 twemproxy 读本机房的从集群， 如果还是没有，则发送 http 请求给数据直连服务 视频中也只是写了这一个商品的多级缓存实现，品牌、分类数据也是一样的思路（视频中没有实现）， 模板也是使用的最简单的信息展示； 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/175.html":{"url":"dr/175.html","title":"175. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据直连服务","keywords":"","body":"175. 商品详情页动态渲染系统：基于 Spring Cloud 开发数据直连服务 前面多级缓存的核心思路是：如果 nginx local cache 没有，则通过 twemproxy 读本机房的从集群， 如果还是没有，则发送 http 请求给数据直连服务 那么这里数据直连的核心思路是： 先找本地 ehcache（前面讲解过，这里忽略不讲解）， 如果没有，则走 redis 主集群， 如果还是没有，则通过 fegion 拉取依赖服务的接口 将数据写入主集群中，主集群会同步到各个机房的从集群 同时数据直连服务将获取到的数据返回给 nginx，nginx 会写入自己本地 local cache 服务实现，创建一个服务 eshop-datalink-service application.yml server: port: 9110 logging: level: root: info # 启动显示 controller 中的路径映射也就是 mapping org.springframework.web: TRACE spring: application: name: eshop-datalink-service jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 redis: # 这里连接的是 twemproxy 代理地址，主机群的 port: 1111 host: 192.168.99.11 eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ twemproxy 主集群搭建请参考 核心就在这一个请求方法中 package cn.mrcode.cache.eshop.datalinkrserver.web.controller; import com.alibaba.fastjson.JSON; import com.alibaba.fastjson.JSONObject; import org.apache.commons.lang.StringUtils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import cn.mrcode.cache.eshop.datalinkrserver.service.EshopProductService; @RestController @RequestMapping(\"/product\") public class ProductController { @Autowired private RedisTemplate redisTemplate; @Autowired private EshopProductService eshopProductService; @RequestMapping(\"/{id}\") public String get(@PathVariable Long id) { // 先从本地缓存获取，这里就不写了 // 再从 redis 主机群获取，获取逻辑参考数据聚合服务中的商品维度 Long productId = id; String dimProductKey = \"dim_product_\" + productId; String dimProductJsonStr = redisTemplate.opsForValue().get(dimProductKey); // 如果商品聚合服务放入的 商品聚合信息没有在主机群查询到，那么就说明要么没有这个商品，要么就是过期了 if (StringUtils.isBlank(dimProductJsonStr)) { // 需要访问原始服务之间获取数据，按照数据聚合服务的聚合逻辑聚合起来 String productJsonStr = eshopProductService.findProductById(productId); if (StringUtils.isBlank(productJsonStr)) { // 没有这个商品信息 return null; } // 否则继续获取其他维度数据 JSONObject product = JSON.parseObject(productJsonStr); // 这里需要在商品服务中增加按 productId 查询的接口 String productPropertyJsonStr = eshopProductService.findProductPropertyByProductId(productId); if (StringUtils.isNotBlank(productPropertyJsonStr)) { product.put(\"productProperty\", JSON.parseObject(productPropertyJsonStr)); } String productSpecificationJsonStr = eshopProductService.findProductSpecificationByProductId(productId); if (StringUtils.isNotBlank(productSpecificationJsonStr)) { product.put(\"productSpecification\", JSON.parseObject(productSpecificationJsonStr)); } dimProductJsonStr = product.toJSONString(); redisTemplate.opsForValue().set(dimProductKey, dimProductJsonStr); } return dimProductJsonStr; } } 下一章再测试，这里需要启动虚拟机中的 redis 集群，才能测试这个逻辑。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/176.html":{"url":"dr/176.html","title":"176. 商品详情页动态渲染系统：完成多级缓存全链路的测试多个 bug 修复","keywords":"","body":" 176. 商品详情页动态渲染系统：完成多级缓存全链路的测试多个 bug 修复 bug 1：无法连接 twemproxy bug 2：获取商品属性返回的应该是一个列表 bug 改完，测试 176. 商品详情页动态渲染系统：完成多级缓存全链路的测试多个 bug 修复 这里就是之前写好的逻辑进行测试，我这里也不测了，因为 nginx 没有实现，之前在每章写完都测试过。 bug 1：无法连接 twemproxy 这里唯一需要注意的就是，上一章节的数据直连服务这里测试下。 访问地址：http://localhost:9110/product/1，但是发现无法连接上 twemproxy。 原因如下，listen 中监听的是 127.0.0.1 ，所以不是在本机的时候是连接不上的，只需要修改成局域网 ip 即可 [root@eshop-detail01 twemproxy-0.4.0]# vi conf/nutcracker.yml redis-master: # 配置一个逻辑名称 # listen: 127.0.0.1:1111 listen: 192.168.99.11:1111 hash: fnv1a_64 distribution: ketama redis: true servers: - 192.168.99.11:6401:1 redis01 # 指向两个主集群 - 192.168.99.11:6402:1 redis02 redis-slave: # 从集群 listen: 127.0.0.1:1112 hash: fnv1a_64 distribution: ketama redis: true servers: - 192.168.99.11:6403:1 redis01 - 192.168.99.11:6405:1 redis02 重启 twemproxy cd /usr/local/twemproxy/twemproxy-0.4.0 [root@eshop-detail01 twemproxy-0.4.0]# ps -ef | grep nutcracker root 7606 1 0 Jul28 ? 00:00:00 ./src/nutcracker -d -c conf/nutcracker.yml root 31766 26460 0 02:17 pts/0 00:00:00 grep nutcracker [root@eshop-detail01 twemproxy-0.4.0]# kill 7606 [root@eshop-detail01 twemproxy-0.4.0]# ./src/nutcracker -d -c conf/nutcracker.yml bug 2：获取商品属性返回的应该是一个列表 String productPropertyJsonStr = eshopProductService.findProductPropertyByProductId(productId); if (StringUtils.isNotBlank(productPropertyJsonStr)) { product.put(\"productProperty\", JSON.parseArray(productPropertyJsonStr)); } String productSpecificationJsonStr = eshopProductService.findProductSpecificationByProductId(productId); if (StringUtils.isNotBlank(productSpecificationJsonStr)) { product.put(\"productSpecification\", JSON.parseArray(productSpecificationJsonStr)); } 获取 ProductProperty 与 ProductSpecification ，一个商品都有多个属性和规格，所以这里也需要修改成数组， 那么同样的，我觉得数据聚合里面，维度数据这里之前也是错误的，也应该修改成数组 bug 改完，测试 访问地址：http://localhost:9110/product/1 然后查看 redis 中是否有维度数据 [root@eshop-detail01 src]# ./redis-cli -h 192.168.99.11 -p 1111 192.168.99.11:1111> get dim_product_1 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/177.html":{"url":"dr/177.html","title":"177. 商品详情页动态渲染系统：商品介绍分段存储以及分段加载的介绍","keywords":"","body":"177. 商品详情页动态渲染系统：商品介绍分段存储以及分段加载的介绍 这里是提供一个思路，商品介绍 product_intro 里面可能包含大段的文字，还有大量的图片， 存储的时候，完全可以将大段的东西，分段来存储，因为一般最好不要将一个特别大的 value 存储到 redis 中 通过上图可以了解到一个 使用场景 ：商品详情页面的浏览，很多时候只会加载第一屏， 所以如果你的商品详情内容很多，那么都查询出来，不在第一屏的内容就是白加载了，浪费了流量带宽 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/178.html":{"url":"dr/178.html","title":"178. 商品详情页动态渲染系统：高可用架构优化之读链路多级降级思路介绍","keywords":"","body":"178. 商品详情页动态渲染系统：高可用架构优化之读链路多级降级思路介绍 本章节也只是思路介绍，脚本也会有示例。 读链路：nginx local cache -> 本机房 redis 从集群 -> 数据直连服务的 jvm 堆缓存（之前讲解，这次没做） -> 其他机房 redis 主集群 -> 依赖服务 这里最有可能出现问题的就是：redis 从集群、 数据直连服务、redis 主集群。 如果这三个都挂了，在读链路上来看，也是灾难性的了，依赖服务很有可能被干死嘛。 那么针对这三个出问题的地方进行降级处理，思路如下脚本，当访问失败时做标记，失败一定次数就标记为挂掉， 在一定时间内就不再提供服务或访问降级策略（降级策略都失败就不再提供服务）。 其实这里做的功能就是之前 hystrix 中的熔断器类似的功能。 这个思路是没有问题，唯一的问题就是对于计数处理等场景，如：下面先从缓存获取失败次数，然后加 1，再更新到缓存中， 这种操作方式不会有数据竞争问题导致脏数据吗？还是说在这种高并发的场景下，脏一点无关紧要？ local cjson = require(\"cjson\") local http = require(\"resty.http\") local redis = require(\"resty.redis\") local function close_redis(red) if not red then return end local pool_max_idle_time = 10000 local pool_size = 100 local ok, err = red:set_keepalive(pool_max_idle_time, pool_size) if not ok then ngx.say(\"set keepalive error : \", err) end end local uri_args = ngx.req.get_uri_args() local productId = uri_args[\"productId\"] local cache_ngx = ngx.shared.my_cache local productCacheKey = \"product_\"..productId local productCache = cache_ngx:get(productCacheKey) if productCache == \"\" or productCache == nil then local slaveRedisDegrade = cache_ngx:get(\"slaveRedisDegrade\") -- redis 从集群是否挂掉 if slaveRedisDegrade == \"true\" then local dataLinkDegrade = cache_ngx:get(\"dataLinkDegrade\") -- 数据直连服务是否挂掉 if dataLinkDegrade == true then local red = redis:new() red:set_timeout(1000) local ip = \"192.168.31.223\" local port = 1111 local ok, err = red:connect(ip, port) local redisResp, redisErr = red:get(\"dim_product_\"..productId) productCache = redisResp local curTime = os.time() local diffTime = os.difftime(curTime, cache_ngx:get(\"startdataLinkDegradeTime\")) // 当挂掉时间超过 60 秒的时候，再次尝试从数据直连服务获取数据 if diffTime > 60 then local httpc = http.new() local resp, err = httpc:request_uri(\"http://192.168.31.179:8767\",{ method = \"GET\", path = \"/product?productId=\"..productId }) if resp then cache_ngx:set(\"dataLinkDegrade\", \"false\") end end else local httpc = http.new() local resp, err = httpc:request_uri(\"http://192.168.31.179:8767\",{ method = \"GET\", path = \"/product?productId=\"..productId }) -- 当从数据直连服务访问不到数据时，就计数 if not resp then ngx.say(\"request error :\", err) local dataLinkFailureCnt = cache_ngx:get(\"dataLinkFailureCnt\") cache_ngx:set(\"dataLinkFailureCnt\", dataLinkFailureCnt + 1) -- 当数据直连服务访问失败到达 10 次时，就标记为数据直连服务已经挂掉了 if dataLinkFailureCnt > 10 then cache_ngx:set(\"dataLinkDegrade\", \"true\") cache_ngx:set(\"startDataLinkDegradeTime\", os.time()) end end productCache = resp.body local curTime = os.time() local diffTime = os.difftime(curTime, cache_ngx:get(\"startSlaveRedisDegradeTime\")) if diffTime > 60 then local red = redis:new() red:set_timeout(1000) local ip = \"192.168.31.223\" local port = 1112 local ok, err = red:connect(ip, port) if ok then cache_ngx:set(\"slaveRedisDegrade\", \"false\") end end end else local red = redis:new() red:set_timeout(1000) local ip = \"192.168.31.223\" local port = 1112 local ok, err = red:connect(ip, port) if not ok then ngx.say(\"connect to redis error : \", err) local slaveRedisFailureCnt = cache_ngx:get(\"slaveRedisFailureCnt\") cache_ngx:set(\"slaveRedisFailureCnt\", slaveRedisFailureCnt + 1) if slaveRedisFailureCnt > 10 then cache_ngx:set(\"slaveRedisDegrade\", \"true\") cache_ngx:set(\"startSlaveRedisDegradeTime\", os.time()) end return close_redis(red) end local redisResp, redisErr = red:get(\"dim_product_\"..productId) if redisResp == ngx.null or redisResp == \"\" or redisResp == nil then local dataLinkDegrade = cache_ngx:get(\"dataLinkDegrade\") if dataLinkDegrade == \"true\" then local red = redis:new() red:set_timeout(1000) local ip = \"192.168.31.223\" local port = 1111 local ok, err = red:connect(ip, port) local redisResp, redisErr = red:get(\"dim_product_\"..productId) productCache = redisResp local curTime = os.time() local diffTime = os.difftime(curTime, cache_ngx:get(\"startdataLinkDegradeTime\")) if diffTime > 60 then local httpc = http.new() local resp, err = httpc:request_uri(\"http://192.168.31.179:8767\",{ method = \"GET\", path = \"/product?productId=\"..productId }) if resp then cache_ngx:set(\"dataLinkDegrade\", \"false\") end end else local httpc = http.new() local resp, err = httpc:request_uri(\"http://192.168.31.179:8767\",{ method = \"GET\", path = \"/product?productId=\"..productId }) if not resp then ngx.say(\"request error :\", err) local dataLinkFailureCnt = cache_ngx:get(\"dataLinkFailureCnt\") cache_ngx:set(\"dataLinkFailureCnt\", dataLinkFailureCnt + 1) if dataLinkFailureCnt > 10 then cache_ngx:set(\"dataLinkDegrade\", \"true\") cache_ngx:set(\"startDataLinkDegradeTime\", os.time()) end end productCache = resp.body end else productCache = redisResp end end math.randomseed(tostring(os.time()):reverse():sub(1, 7)) local expireTime = math.random(600, 1200) cache_ngx:set(productCacheKey, productCache, expireTime) end local context = { productInfo = productCache, } local template = require(\"resty.template\") template.render(\"product.html\", context) 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/179.html":{"url":"dr/179.html","title":"179. 商品详情页动态渲染系统：高可用架构优化之 hystrix 隔离与降级","keywords":"","body":"179. 商品详情页动态渲染系统：高可用架构优化之 hystrix 隔离与降级 前面深入讲解过 hystrix 了，这里就不多说，在服务相互调用时也可以使用 hystrix 来做降级，入门用法如下 添加依赖 compile 'org.springframework.cloud:spring-cloud-starter-hystrix' 添加配置 # feign 调用开启 hystrix 与一些配置 feign: # httpclient: # enabled: true client: config: default: #服务名，填写default为所有服务 connectTimeout: 10000 readTimeout: 400000 # 3.3 * 2 分钟 hystrix: enabled: true # hystrix 的一些配置 hystrix: command: default: execution: isolation: thread: timeoutInMilliseconds: 400000 threadpool: default: allowMaximumSizeToDivergeFromCoreSize: true coreSize: 20 maximumSize: 1000 maxQueueSize: -1 queueSizeRejectionThreshold: -1 keepAliveTimeMinutes: 2 增加降级类，这里在数据直连服务中做示例，实现 feign 的接口，这里没有具体的去实现业务逻辑，只讲整合 hystrix 的方法 @Component public class EshopProductServiceFallback implements EshopProductService { @Override public String findBrandById(Long id) { return null; } @Override public String findCategoryById(Long id) { return null; } @Override public String findProductIntroById(Long id) { return null; } @Override public String findProductPropertyById(Long id) { return null; } @Override public String findProductById(Long id) { return null; } @Override public String findProductSpecificationById(Long id) { return null; } @Override public String findProductPropertyByProductId(Long productId) { return null; } @Override public String findProductSpecificationByProductId(Long productId) { return null; } } fegin 中指定这个降级接口 @FeignClient(value = \"eshop-product-service\",fallback = EshopProductServiceFallback.class) public interface EshopProductService { } 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/180.html":{"url":"dr/180.html","title":"180. 商品详情页动态渲染系统：部署 jenkins 持续集成服务器","keywords":"","body":" 180. 商品详情页动态渲染系统：部署 jenkins 持续集成服务器 jenkins 安装（版本太老插件安装失败） jenkins 官网最新版本安装 jenkins 启动停止 jenkins 错误解决 初始密码后，卡在 Jenkins正在启动,请稍后... 中 180. 商品详情页动态渲染系统：部署 jenkins 持续集成服务器 使用 jenkins 自动化的流程： jenkins 上执行构建 会自动从配置的 git 仓库拉取代码 用配置的脚本打包 创建 docker 镜像 通过 docker 容器来部署 spring cloud 微服务 简要流程：代码上传至 git、使用 jenkins 创建构建项目、写自动化部署脚本 jenkins 安装（版本太老插件安装失败） 下载地址：http://updates.jenkins-ci.org/download/war/ 本次使用 2.78 版本的 jenkins.war 上传 jenkins.war 到 /usr/local/jenkins 目录下，并执行以下命令 # 启动 jenkins 服务器 # 如果提示 Error: Unable to access jarfile Jenkins.war，等一定要注意 war 包名称是否正确 [root@eshop-detail01 jenkins]# java -jar jenkins.war --httpPort=8080 # 启动成功后，会在控制台看到如下的信息 ************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: 90a7f0739f4b4b06b55a43aa803481a4 # 注意这个，第一次访问的初始密码位置，上面的内容就是该文件中的内容 This may also be found at: /root/.jenkins/secrets/initialAdminPassword 访问地址：http://192.168.99.11:8080，填入初始密码 选择 install suggested plugins 安装推荐的插件，等待安装完成 创建管理员账户，就可以了；（但是本人在实践过程中插件安装一直都安装不成功，无奈后续的没有进行下去） jenkins 官网最新版本安装 上面的安装方式在安装插件的时候，总会以为是由于没有翻墙的原因导致失败，在推荐插件安装失败后，选择继续（不是重试）， 进入 jenkins 后，在 ui 中插件管理里面看到的报错信息大意是，当前的 jenkins 版本太低，插件不能再低版本上安装。 所以这里使用官网的最新安装方法来安装 官网 centos 安装 https://pkg.jenkins.io/redhat-stable/ sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo # 如果已经导入了 key ，报错的话，可以忽略 sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key yum install jenkins 其他详细的使用和配置可以 参考官网文章 jenkins 启动停止 # 启动、停止、重启 sudo service jenkins start/stop/restart sudo chkconfig jenkins on 一些常用的路径如下： 日志文件路径： /var/log/jenkins/jenkins.log 安装目录：/var/lib/jenkins/ 启动之后，输入初始密码（在日志中有打印，ui 中也有指引），选择安装推荐的插件（这一步已经实锤：不修改源也能装成功）， 插件安装完成之后可以选择创建一个管理员账户，会告诉你需要重启 jenkins（如果它自动刷新浏览器被卡主，这个时候你只需要重新刷新下浏览器就行了） jenkins 错误解决 初始密码后，卡在 Jenkins正在启动,请稍后... 中 如果输入初始密码后，一直卡在 Jenkins正在启动,请稍后... 的页面中。 解决方案：请找到 jenkins 工作目录（日志中有打印 war 包被解压到了哪个目录下） # 把 http://updates.jenkins-ci.org/update-center.json 改成 http://mirror.xmission.com/jenkins/updates/update-center.json vi /root/.jenkins/hudson.model.UpdateCenter.xml # 修改另外一个地方 # 把 \"connectionCheckUrl\":\"http://www.google.com/\" 改为 \"connectionCheckUrl\":\"http://www.baidu.com/\" vi /root/.jenkins/updates/default.json 记得需要关闭浏览器后再重新进入一次，输入初始密码 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/181.html":{"url":"dr/181.html","title":"181. 商品详情页动态渲染系统：在 CentOS 6 安装和部署 Docker","keywords":"","body":" 181. 商品详情页动态渲染系统：在 CentOS 6 安装和部署 Docker centOS 7 安装 配置网络 关闭防火墙 安装 java 8 初步安装和启动 docker 设置镜像 开放管理端口映射 测试 docker 是否正常安装和运行 错误解决 yum-config-manager --add-repo 异常 yum -y install docker-ce 异常 181. 商品详情页动态渲染系统：在 CentOS 6 安装和部署 Docker ::: tip 注意，docker 需要在 centOS 7 中安装，这里安装 CentOS-7-x86_64-Minimal-1708.iso ， 然后在上面安装 docker 部分配置可以参考：之前的 centos 安装 ::: centOS 7 安装 使用版本：CentOS-7-x86_64-Minimal-1708.iso， 账户/密码 root/hadoop123 配置网络 vi /etc/sysconfig/network-scripts/ifcfg-enp0s3 # 先让它动态分配一个ip地址 ONBOOT=yes # 重启服务 service network restart # 查看分配的地址 ip addr # 再设置静态 ip 地址 BOOTPROTO=static IPADDR=192.168.99.20 NETMASK=255.255.255.0 GATEWAY=192.168.99.1 # 重启网络服务 service network restart # 查看是否设置成功 ip addr 执行以下命令报错，是缺少了 dns 配置 [root@localhost ~]# ping www.baidu.com ping: www.baidu.com: Name or service not known 配置 dns # 检查 NetManager 的状态： systemctl status NetworkManager.service # 检查 NetManager 管理的网络接口： nmcli dev status # 检查 NetManager 管理的网络连接： nmcli connection show # 设置 dns： nmcli con mod enp0s3 ipv4.dns \"114.114.114.114 8.8.8.8\" # 让 dns 配置生效： nmcli con up enp0s3 关闭防火墙 systemctl stop firewalld.service systemctl disable firewalld.service 安装 java 8 cd /usr/local # 安装上传下载工具 yum install lrzsz rz 选择 jdk-8u202-linux-i586.rpm rpm -ivh jdk-8u202-linux-i586.rpm # 卸载可以使用 rpm -e jdk1.8-2000:1.8.0_202-fcs.i586 # 安装过程中就出现了以下类似的错误 # 运行 java -version 出现以下错误 -bash: /usr/bin/java: /lib/ld-linux.so.2: bad ELF interpreter: No such file or directory 解决方案如下： # 安装 glibc.i686 yum install glibc.i686 # 卸载安装不完整的 java rpm -e jdk1.8-2000:1.8.0_202-fcs.i586 # 重新安装 rpm -ivh jdk-8u202-linux-i586.rpm 通过此方式安装之后，不需要配置环境变量 初步安装和启动 docker 如果这里的安装步骤让你感到不解，那么可以参考此教程 ，里面参考了官网的安装教程 yum update -y yum install -y yum-utils # 添加 docker-ce 仓库地址 # yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum -y install docker-ce systemctl start docker 设置镜像 vi /etc/docker/daemon.json # 设置成 阿里云的 { \"registry-mirrors\": [\"https://aj2rgad5.mirror.aliyuncs.com\"] } 开放管理端口映射 vi /lib/systemd/system/docker.service # 将第 11 行的 ExecStart=/usr/bin/dockerd，替换为： # 2375 是管理端口，7654 是备用端口 ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock -H tcp://0.0.0.0:7654 # 在 ~/.bashrc 中写入 docker 管理端口 vi ~/.bashrc export DOCKER_HOST=tcp://0.0.0.0:2375 source ~/.bashrc 测试 docker 是否正常安装和运行 # 记得先停止 docker 再运行，因为之前配置了端口什么的 systemctl stop docker systemctl start docker # 运行 docker 的一个 hello-world 镜像 docker run hello-world ... # 如果现实有以下文字就说明可以了 Hello from Docker! This message shows that your installation appears to be working correctly. 错误解决 yum-config-manager --add-repo 异常 执行命令后出现以下异常，原因是国内无法访问外国的 docker 镜像，这里需要使用阿里云的来安装 [root@eshop-detail01 ~]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Loaded plugins: fastestmirror adding repo from: https://download.docker.com/linux/centos/docker-ce.repo grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo: [Errno 14] PYCURL ERROR 22 - \"The requested URL returned error: 416 Requested Range Not Satisfiable\" Trying other mirror. Could not fetch/save url https://download.docker.com/linux/centos/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo: [Errno 14] PYCURL ERROR 22 - \"The requested URL returned error: 416 Requested Range Not Satisfiable\" 解决方案：yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum -y install docker-ce 异常 报错的地址文件的确 404 了，这个就是在 centos 6 下安装 docker-ce 的缘故，使用 centos 7 安装就行了 [root@eshop-detail01 ~]# yum install -y docker Loaded plugins: fastestmirror Setting up Install Process Loading mirror speeds from cached hostfile * base: mirrors.huaweicloud.com * extras: mirrors.huaweicloud.com * updates: ap.stykers.moe https://download.docker.com/linux/centos/7/i386/stable/repodata/repomd.xml: [Errno 14] PYCURL ERROR 22 - \"The requested URL returned error: 404 Not Found\" Trying other mirror. To address this issue please refer to the below wiki article https://wiki.centos.org/yum-errors If above article doesn't help to resolve this issue please use https://bugs.centos.org/. Error: Cannot retrieve repository metadata (repomd.xml) for repository: docker-ce-stable. Please verify its path and try again 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/182.html":{"url":"dr/182.html","title":"182. 商品详情页动态渲染系统：在 CentOS 6 安装 maven、git 以及推送 github","keywords":"","body":" 182. 商品详情页动态渲染系统：在 CentOS 6 安装 maven、git 以及推送 github gradle 安装 git 安装 182. 商品详情页动态渲染系统：在 CentOS 6 安装 maven、git 以及推送 github 由于本人使用的是 gradle ，这里就安装 gradle。 对于 github 的代码推送，由于只有一个仓库：https://github.com/zq99299/cache-eshop， 所以就不记录了，到时候直接使用这一个仓库然后进入到服务注册中心这个项目中去构建打包 gradle 安装 cd /usr/local mkdir gradle cd gradle wget https://services.gradle.org/distributions/gradle-4.8.1-all.zip yum install unzip unzip gradle-4.8.1-all.zip # 配置环境变量 vi ~/.bashrc export GRADLE_HOME=/usr/local/gradle/gradle-4.8.1 export PATH=$PATH:$GRADLE_HOME/bin source ~/.bashrc # 测试是否装好 gradle --version git 安装 yum install -y git git --version 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/183.html":{"url":"dr/183.html","title":"183. 商品详情页动态渲染系统：通过 jenkins+docker 部署 eureka 服务","keywords":"","body":" 183. 商品详情页动态渲染系统：通过 jenkins+docker 部署 eureka 服务 jenkins shell command not found dial unix /var/run/docker.sock: connect: permission denied 让 jenkins 使用 root 用户 docker run 报错 docker0，重置 docker 网络 183. 商品详情页动态渲染系统：通过 jenkins+docker 部署 eureka 服务 开始创建一个新任务。 任务信息 任务名称：eshop-eureka-server 选择：构建一个自由风格的软件项目 General -> 源码管理 添加构建步骤 说明，由于我们这里的仓库只有一个，所以需要手动进入到服务注册中心这个目录，来构建。 就无法使用 jenkins 自带的，gradle 或者 maven 来构建了。 pwd: 显示当前在那个目录下，可以看到当前目录在 /var/lib/jenkins/workspace/eshop-eureka-server 下，注意这里的 eshop-eureka-server 是任务名称。而这个目录下就是我们的 git 源代码了。 echo $PATH：显示当前的环境变量，当遇到执行系统中的某些命令提示 command not found 时，对比本机上的环境变量信息（后面有解决方案） 上面是尝试和解决 command not found 的测试，下面是 gradle build 构建步骤的完整 shell 脚本 #!/bin/bash cd eshop-eureka-server gradle build 保存之后，可以点击立即构建， 构建成功之后，会在 /var/lib/jenkins/workspace/eshop-eureka-server/eshop-eureka-server/build/libs 中生成 eshop-eureka-server-1.0.0.jar 包。构建成功 给项目添加 DockerFile 在 H:\\dev\\project\\mrcode\\cache-eshop\\eshop-eureka-server 也就是服务注册中心的项目根目录下添加一个名为 Dockerfile 的文件， 内容如下，有关 docker 的使用这里没有深入讲解，可以参考这个入门教程 和这个第一个 docker jar # 从这个地址获取一个 doker 镜像 FROM frolvlad/alpine-oraclejdk8:slim VOLUME /tmp # 使用 gradle build 打包好那个 jar ADD build/libs/eshop-eureka-server-1.0.0.jar app.jar #RUN bash -c 'touch /app.jar' # 运行这个 jar 包 ENTRYPOINT [\"java\",\"-Djava.security.egd=file:/dev/./urandom\",\"-jar\",\"/app.jar\"] # 暴露端口 EXPOSE 8761 添加构建步骤 - docker #!/bin/bash # doker 地址 REGISTRY_URL=192.168.99.20:2375 # 选择一个目录进行打包，这里写 /var/lib/jenkins 是为了简化 linux 权限相关，执行命令是用 jenkins 用户执行的 # 所以这里直接写在 jenkins 目录下，跳过了目录权限等问题 WORK_DIR=/var/lib/jenkins/work_build PROJECT_NAME=eshop-eureka-server PROJECT_VERSION=1.0.0 # 如果这个目录不存在则创建它 if [ ! -e ${WORK_DIR}/${PROJECT_NAME} ] && [ ! -d ${WORK_DIR}/${PROJECT_NAME} ]; then mkdir -p ${WORK_DIR}/${PROJECT_NAME} echo \"Create Dir: ${WORK_DIR}/${PROJECT_NAME}\" fi # 如果 Dockerfile 这个文件存在，则删除它 if [ -e ${WORK_DIR}/${PROJECT_NAME}/Dockerfile ]; then rm -rf ${WORK_DIR}/${PROJECT_NAME}/Dockerfile echo \"Remove File: ${WORK_DIR}/${PROJECT_NAME}/Dockerfile\" fi # 把项目中的 Dockerfile 与 打好包的 boot jar 包 复制到目录下 cp /var/lib/jenkins/workspace/eshop-eureka-server/eshop-eureka-server/Dockerfile ${WORK_DIR}/${PROJECT_NAME}/ cp /var/lib/jenkins/workspace/eshop-eureka-server/eshop-eureka-server/build/libs/*.jar ${WORK_DIR}/${PROJECT_NAME}/ cd ${WORK_DIR}/${PROJECT_NAME}/ # docker 构建 docker build -t ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION} . # 仓库 192.168.99.20:2375/eshop-detail/eshop-eureka-server docker push ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION} # 查看是否有这个服务运行 # docker ps -a | grep eshop-eureka-server if docker ps -a | grep ${PROJECT_NAME}; then # 移除正在运行的镜像 # docker rm -f eshop-eureka-server docker rm -f ${PROJECT_NAME} echo \"Remove Docker Container: ${PROJECT_NAME}\" fi # 使用 docker 运行镜像 # 在运行时可能报错端口等问题，可参考后面的 重置 docker 网络 解决 # 把 docker 中的 8761 与宿主机的 8761 相关联， # 运行 192.168.99.20:2375/eshop-detail/eshop-eureka-server:1.0.0 并取名 eshop-eureka-server # docker run -d -p 8761:8761 --name eshop-eureka-server 192.168.99.20:2375/eshop-detail/eshop-eureka-server:1.0.0 docker run -d -p 8761:8761 --name ${PROJECT_NAME} ${REGISTRY_URL}/eshop-detail/${PROJECT_NAME}:${PROJECT_VERSION} 添加好之后，就可以尝试立即构建了，构建成功之后可以访问：http://192.168.99.20:8761/ 看到注册中心界面 后面的是这个实验过程中的各种错误解决。 jenkins shell command not found 在 jenkins 中执行 shell 脚本提示 command not found，比如上面我们用到了 gradle， 居然报错：gradle command not found； 解决方案： 在终端上查看当前 path 信息 [root@localhost eshop-eureka-server]# echo $PATH /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/usr/local/gradle/gradle-4.8.1/bin 覆盖 jenkins 中的 path 变量：jenkins -> 系统管理 -> 系统设置 如上图，使用终端上的 path 信息覆盖之后，就可以正常使用了 dial unix /var/run/docker.sock: connect: permission denied 使用 jenkins 中执行 shell 出现权限不足的提示，这里还是让 jenkins 用 root 身份执行执行把， 不然要修改的脚本太多了。 让 jenkins 使用 root 身份执行设置如下： 让 jenkins 使用 root 用户 # 将配置中的 JENKINS_USER=\"jenkins\" 修改为 root vi /etc/sysconfig/jenkins # 并将相关几个目录修改为 root 所有 chown -R root:root /var/lib/jenkins chown -R root:root /var/cache/jenkins chown -R root:root /var/log/jenkins # 重启Jenkins（若是其他方式安装的jenkins则重启方式略不同） service jenkins restart # 查看Jenkins进程所属用户 # 若显示为 root 用户，则表示修改完成 ps -ef | grep jenkins docker run 报错 docker0，重置 docker 网络 [root@localhost eshop-eureka-server]# docker ps -a | grep eshop-eureka-server [root@localhost eshop-eureka-server]# docker run -d -p 8761:8761 --name eshop-eureka-server 192.168.99.20:2375/eshop-detail/eshop-eureka-server:1.0.0 0fbb1b660dede294593a540bb6c5d64ea55517572e5f6fa49deeae4077f65238 docker: Error response from daemon: driver failed programming external connectivity on endpoint eshop-eureka-server (4f4111c8f21f1d4c29c14aeb4e5775b065e9e955819861f2a34ef23652ea6238): (iptables failed: iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 8761 -j DNAT --to-destination 172.17.0.2:8761 ! -i docker0: iptables: No chain/target/match by that name. (exit status 1)). 重置 docker 网络 pkill docker iptables -t nat -F # yum install net-tools 安装后可用 ifconfig 命令 ifconfig docker0 down # yum install bridge-utils 安装后可用 brctl brctl delbr docker0 docker -d service docker restart 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"dr/184.html":{"url":"dr/184.html","title":"184. 商品详情页动态渲染系统：twemproxy hash tag+mget 优化思路介绍","keywords":"","body":"184. 商品详情页动态渲染系统：twemproxy hash tag + mget 优化思路介绍 用 twemproxy 其实就是将 Redis 集群做成了很多个分片，相当于是部署了很多个 redis 主实例， 通过 twemproxy 中间件，将数据散列存储到多个 redis 实例中去，每个 redis 实例中存储一部分的数据 之前在商品聚合服务中（eshop-dataaggr-service）使用了 mget 来优化一次获取多个 key 的操作，如下的 key product_1 product_property_1 product_specification_1 这里在 twemproxy 中来说就会存在一个问题，这三个 key 可能会散列到多个 redis 中去， 所以就会走多次网络才能拿到这三个数据。 那么这里可以使用 hash tag 这个功能来让这三个 key 落到同一个 redis 分片上去 vi conf/nutcracker.yml redis-master: # 配置一个逻辑名称 # listen: 127.0.0.1:1111 listen: 192.168.99.11:1111 hash: fnv1a_64 hash_tag: \"::\" # 增加此配置 distribution: ketama redis: true servers: - 192.168.99.11:6401:1 redis01 # 指向两个主集群 - 192.168.99.11:6402:1 redis02 那么在读写的时候就要使用如下的 key 形式来让 hash_tag 生效 product:1: product_property:1: product_specification:1: 这样一来，读写都会使用 :中间: 的字符串来计算 hash 值，就能落地到同一个分片上去了 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/186.html":{"url":"onservice/186.html","title":"186. 商品详情页 OneService 系统：整体架构设计","keywords":"","body":"186. 商品详情页 OneService 系统：整体架构设计 前面把动态渲染系统这一套做完了，处理的是时效性不高的数据，而 OneService 处理的是时效性较高的数据。 OneService 系统的功能如下： 商品详情页依赖的服务达到数十个，甚至是上百个，需要给一个统一的入口，打造服务闭环 请求预处理 合并接口调用，减少 ajax 异步加载次数 统一监控 统一降级 后面的课程针对以上几点进行讲解，不一定都做出来代码，但是会有经验 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/187.html":{"url":"onservice/187.html","title":"187. 商品详情页 OneService 系统：基于 Spring Cloud 构建 OneService 服务","keywords":"","body":"187. 商品详情页 OneService 系统：基于 Spring Cloud 构建 OneService 服务 创建新模块系统 eshop-one-service，端口号：9108，直接参考 eshop-datasync-service。 本章功能如下： 提供通过产品 id 查询库存接口，通过 feign 调用库存服务 提供通过产品 id 查询价格接口，通过 feign 调用价格服务 改造库存和价格服务中的逻辑 先从 redis 缓存获取，取不到再从数据库中查询返回，并写入数据库， 这里就是第一版中已经讲过的，缓存双写一致性，所以本次不具体考虑并发等问题，就简单的实现； 这里拿库存服务简单贴下实现代码，很简单。 OneService 中调用库存服务中的 feign cn.mrcode.cache.eshop.oneserver.service.InventoryService @FeignClient(value = \"eshop-inventory-service\") public interface InventoryService { @GetMapping(\"/product-inventory/findByProductId\") String findByProductId(@RequestParam(value = \"productId\") Long productId); } 库存服务中简单的实现，从缓存获取，取不到就从数据库中获取 public ProductInventory findByProductId(Long productId) { String key = \"product_inventory_productId_\" + productId; String json = redisTemplate.opsForValue().get(key); if (json != null) { return JSON.parseObject(json, ProductInventory.class); } ProductInventory productInventory = productInventoryMapper.findByProductId(productId); redisTemplate.opsForValue().set(key, JSON.toJSONString(productInventory)); return productInventory; } 简单测试两个调用是否有问题，cn.mrcode.cache.eshop.oneserver.web.controller.OneController /** * @author : zhuqiang * @date : 2019/9/26 21:39 */ @RestController @RequestMapping(\"/one\") public class OneController { @Autowired private InventoryService inventoryService; @Autowired private PriceService priceService; @GetMapping(\"/inventory\") public String findInventory(Long productId) { return inventoryService.findByProductId(productId); } @GetMapping(\"/price\") public String findPrice(Long productId) { return priceService.findByProductId(productId); } } 访问地址：http://localhost:9108/one/price?productId=1 访问地址：http://localhost:9108/one/inventory?productId=1 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/188.html":{"url":"onservice/188.html","title":"188. 商品详情页 OneService 系统：库存服务与价格服务的代理接口开发","keywords":"","body":"188. 商品详情页 OneService 系统：库存服务与价格服务的代理接口开发 就是前面测试那个 controller 的内容。。。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/189.html":{"url":"onservice/189.html","title":"189. 商品详情页 OneService 系统：请求预处理功能设计介绍","keywords":"","body":"189. 商品详情页 OneService 系统：请求预处理功能设计介绍 我这么跟大家说，我们是不可能完全真实的开发出一个完整业务的 OneService 系统的， 因为那样做的话，需要构建出来大量的各种各样的依赖服务，才能将这个业务场景模拟出来，没必要 我这边，把一个基本的 OenService 系统的架子，基于 spring cloud 做出来，大家就能体验到一个完整的架构是怎么做的， 至于里面一些具体的业务，就由我来给大家说明和介绍一下就可以了 什么是请求预处理？ 先做一些简单的，薄薄的一层的封装和代理，先做点业务逻辑的判断和处理 请求预处理可做的事情有： 比如库存状态（有货/无货）的转换，第三方运费的处理，第三方配送时效（多少天发货）的处理 处理主商品与配件关系，比如说 iphone 可能就搭载着耳机，充电器，等等 商家运费动态计算，等等 这样的话，就可以给后端的服务传递更多的参数，或者简化后端服务的计算逻辑 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/190.html":{"url":"onservice/190.html","title":"190. 商品详情页 OneService 系统：多服务接口合并设计介绍","keywords":"","body":"190. 商品详情页 OneService 系统：多服务接口合并设计介绍 之前我们都跟大家说过了，可能页面中需要相关联的几份数据，就不用一次又一次的发送 ajax 请求来获取多份数据， 直接就是一次请求发给一个 one service 系统的大接口，然后那个接口统一调用各个后端服务的接口就可以 目的： 减少浏览器和后端系统之间的交互次数 那么将哪些接口合并为一个接口呢？如何来设计接口的合并呢？ 这个策略就很多，一般常见的策略是根据相邻的位置（页面中 ui 展示相邻的合并为一个接口）， 比如第一屏有促销和部分广告，可以合并一下，在第一屏这两个一次就能拿回来需要展示的数据 还有比如以下的业务合并等 促销和广告，合并成一个接口，一次性发送请求过来，然后调用促销服务和广告服务，获取两份数据 库存服务，配送服务，合并成一个接口，一次性过来，获取当前有多少库存，如何配送 组合推荐服务+配件推荐服务+套装推荐服务，三个服务并发调用，合并结果 如上图的推荐和广告很近，往下拉的时候，基本上都需要加载出来。就适合合并起来 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/191.html":{"url":"onservice/191.html","title":"191. 商品详情页 OneService 系统：基于 hystrix 进行接口统一降级","keywords":"","body":"191. 商品详情页 OneService 系统：基于 hystrix 进行接口统一降级 先整合 hystrix，可参考 153. 商品详情页动态渲染系统：Spring Cloud 之 Hystrix 熔断降级 使用 hystrix ：限流，自动熔断，调用失败，都会走降级，这个需要根据业务对你代理的后端服务进行各种策略的降级 本章就是简单的整合 hystrix ，没有任何代码来实现和讲解业务。 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/192.html":{"url":"onservice/192.html","title":"192. 商品详情页 OneService 系统：基于 hystrix dashboard 进行统一监控","keywords":"","body":"192. 商品详情页 OneService 系统：基于 hystrix dashboard 进行统一监控 本章与之前的一样，不做记录。 前面做了统一降级，同样对接口调用也可以统一做监控 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/193.html":{"url":"onservice/193.html","title":"193. 商品详情页 OneService 系统：基于 jenkins+docker 部署 OneService 服务","keywords":"","body":"193. 商品详情页 OneService 系统：基于 jenkins+docker 部署 OneService 服务 本章与之前的一样，不做记录 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"onservice/194.html":{"url":"onservice/194.html","title":"194. 商品详情页 OneService 系统：基于 jenkins+docker 部署 hystrix terbine 服务","keywords":"","body":"194. 商品详情页 OneService 系统：基于 jenkins+docker 部署 hystrix terbine 服务 本章与之前的一样，不做记录 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "},"195.html":{"url":"195.html","title":"195. 商品详情页前端介绍&课程总结& Java 架构师展望","keywords":"","body":" 195. 商品详情页前端介绍&课程总结& Java 架构师展望 核心思路 总结 最后说明 195. 商品详情页前端介绍&课程总结& Java 架构师展望 本章是最后一讲，我们最后，相当于我们已经有了两套系统 第一套：商品服务 + 动态渲染系统 第二套：库存/价格服务+ OneService 系统 第三部分：前端页面 核心思路 时效性比较低的数据（服务端渲染） 更新的时候发送消息到 mq，专门有一套数据同步服务+数据聚合服务来进行数据的加工和处理 前端页面，请求商品详情页的时候，nginx 会走多级缓存策略（nginx local cache -> 本机房 redis 集群 -> 数据直连服务 -> 本地 jvm cache -> redis 主集群 -> 依赖服务），将时效性比较低的数据，全部加载到内存中，然后动态渲染到 html 中 前端 html 展示出来的时候，上来就有一些动态渲染出来的数据 时效性比较高的数据 依赖服务每次更新数据库的时候，直接就更新 redis 缓存了，mysql+redis 双写 前端 html 在展示出来以后，立即会对时效性要求比较高的数据，比如库存、价格、促销、推荐、广告，发送 ajax 请求到后盾 后端 nginx 接收到请求之后，就会将请求转发给 one service 系统，one service 系统代理了所有几十个服务的接口，统一代理、统一降级、预处理、合并接口、统一监控 由 one service 系统发送请求给后端的一些服务，那些服务优先读 redis，如果没有则读 mysql，然后再重新刷入 redis 商品介绍 写的时候，采取的是分段存储策略，之前介绍过了 读的时候，也是在用户滚屏的时候，动态的异步 ajax 加载，分段加载商品介绍，不要一次性将所有的商品介绍都加载出来 总结 你学到了哪些东西？ 第一版：深入 redis、缓存架构、hystrix 高可用 第二版：完整的亿级流量商品详情页的系统架构，spring cloud+jenkins+docker 的微服务项目实战 最后说明 单课，是不可能真的将所有东西讲的面面俱到的 如果要从职业生涯去考虑，比如说从月薪十几 k 的中级工程师到月薪二十多 k 的高级工程师，或者从高级工程师到月薪三十多 k，四十多 k 的架构师， 至少需要付出 1~2 年的时间，非常系统的去学习完整的架构师的知识，才能够达到 刘凯®喜欢花时间去鼓捣各种技术~Copyright © 刘凯（kaisesai） all right reservedmodified at 2020-05-06 15:39:48 "}}